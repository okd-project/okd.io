{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stemmer","stopWordFilter","trimmer"]},"docs":[{"location":"","title":"OKD.io","text":"<p>Latest</p> <p>Help us improve OKD by completing the 2023 OKD user survey</p> <p>Built around a core of OCI container packaging and Kubernetes container cluster management, OKD is also augmented by application lifecycle management functionality and DevOps tooling. OKD provides a complete open source container application platform.</p>"},{"location":"#okd-4","title":"OKD 4","text":"<p><code>$ openshift-install create cluster</code></p> <p>Tons of amazing new features</p> <p>Automatic updates not only for OKD but also for the host OS, k8s Operators are first class citizens, a fancy UI, and much much more</p> <p>CodeReady Containers for OKD: local OKD 4 cluster for development</p> <p>CodeReady Containers brings a minimal OpenShift 4 cluster to your local laptop or desktop computer! Download it here: CodeReady Containers for OKD Images</p>"},{"location":"#what-is-okd","title":"What is OKD?","text":"<p>OKD is a distribution of Kubernetes optimized for continuous application development and multi-tenant deployment</p> <p>OKD embeds Kubernetes and extends it with security and other integrated concepts</p> <p>OKD adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, easy deployment and scaling, and long-term lifecycle maintenance for small and large teams</p> <p>OKD is also referred to as Origin in GitHub and in the documentation</p> <p>OKD is a sibling Kubernetes distribution to Red Hat OpenShift | If you are looking for enterprise-level support, or information on partner certification, Red Hat also offers Red Hat OpenShift Container Platform</p> <p></p>"},{"location":"#okd-community","title":"OKD Community","text":"<p>We know you've got great ideas for improving OKD and its network of open source projects. So roll up your sleeves and come join us in the community!</p>"},{"location":"#get-started","title":"Get Started","text":"<p>All contributions are welcome! OKD uses the Apache 2 license and does not require any contributor agreement to submit patches. Please open issues for any bugs or problems you encounter, ask questions in the #openshift-users on Kubernetes Slack Channel, or get involved in the OKD-WG by joining the OKD-WG google group.</p> <ul> <li>Get started with the Contributors Guide</li> <li>Read the documentation</li> <li>Read our charter</li> <li>Help Resolve an Open Issue</li> </ul>"},{"location":"#connect-to-the-community","title":"Connect to the community","text":"<p>Join the OKD Working Group</p>"},{"location":"#talk-to-us","title":"Talk to Us","text":"<ul> <li>Follow the public mailing lists</li> <li>Chat with us on Matrix</li> <li>Chat with us on the #openshift-users channel on Slack</li> <li>See more options on the OKD Working Group Communications page</li> </ul>"},{"location":"#standardization-through-containerization","title":"Standardization through Containerization","text":"<p>Standards are powerful forces in the software industry. They can drive technology forward by bringing together the combined efforts of multiple developers, different communities, and even competing vendors.</p> <p></p> <p>Open source container orchestration and cluster management at scale</p> <p></p> <p>Standardized Linux container packaging for applications and their dependencies</p> <p></p> <p>A container-focused OS that's designed for painless management in large clusters</p> <p></p> <p>An open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator</p> <p></p> <p>A lightweight container runtime for Kubernetes</p> <p></p> <p>Prometheus is a systems and service monitoring toolkit that collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true</p>"},{"location":"#okd-end-user-community","title":"OKD End User Community","text":"<p>There is a large, vibrant end user community</p>"},{"location":"#become-a-part-of-something-bigger","title":"Become a part of something bigger","text":"<p>OpenShift Commons is open to all community participants: users, operators, enterprises, non-profits, educational institutions, partners, and service providers as well as other open source technology initiatives utilized under the hood or to extend the OpenShift platform</p> <ul> <li>If you are an OpenShift Online or an OpenShift Container Platform customer or have deployed OKD on premise or on a public cloud</li> <li>If you have contributed to the OKD project and want to connect with your peers and end users</li> <li>If you simply want to stay up-to-date on the roadmap and best practices for using, deploying and operating OpenShift</li> </ul> <p>... then OpenShift Commons is the right place for you</p> <p></p>"},{"location":"about/","title":"About OKD","text":"<p>OKD is the community distribution of Kubernetes optimized for continuous application development and multi-tenant deployment. OKD adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, easy deployment and scaling, and long-term lifecycle maintenance for small and large teams. OKD is also referred to as Origin in GitHub and in the documentation. OKD makes launching Kubernetes on any cloud or bare metal a snap, simplifies running and updating clusters, and provides all of the tools to make your containerized-applications succeed.</p>"},{"location":"about/#features","title":"Features","text":"<ul> <li>A fully automated distribution of Kubernetes on all major clouds and bare metal, OpenStack, and other virtualization providers</li> <li>Easily build applications with integrated service discovery and persistent storage.</li> <li>Quickly and easily scale applications to handle periods of increased demand.</li> <li>Support for automatic high availability, load balancing, health checking, and failover.</li> <li>Access to the Operator Hub for extending Kubernetes with new, automated lifecycle capabilities</li> <li>Developer centric tooling and console for building containerized applications on Kubernetes</li> <li>Push source code to your Git repository and automatically deploy containerized applications.</li> <li>Web console and command-line client for building and monitoring applications.</li> <li>Centralized administration and management of an entire stack, team, or organization.</li> <li>Create reusable templates for components of your system, and iteratively deploy them over time.</li> <li>Roll out modifications to software stacks to your entire organization in a controlled fashion.</li> <li>Integration with your existing authentication mechanisms, including LDAP, Active Directory, and public OAuth providers such as GitHub.</li> <li>Multi-tenancy support, including team and user isolation of containers, builds, and network communication.</li> <li>Allow developers to run containers securely with fine-grained controls in production.</li> <li>Limit, track, and manage the developers and teams on the platform.</li> <li>Integrated container image registry, automatic edge load balancing, and full spectrum monitoring with Prometheus.</li> </ul>"},{"location":"about/#what-can-i-run-on-okd","title":"What can I run on OKD?","text":"<p>OKD is designed to run any Kubernetes workload. It also assists in building and developing containerized applications through the developer console.</p> <p>For an easier experience running your source code, Source-to-Image (S2I) allows developers to simply provide an application source repository containing code to build and run.  It works by combining an existing S2I-enabled container image with application source to produce a new runnable image for your application.</p> <p>You can see the full list of Source-to-Image builder images and it's straightforward to create your own.  Some of our available images include:</p> <ul> <li>Ruby</li> <li>Python</li> <li>Node.js</li> <li>PHP</li> <li>Perl</li> <li>WildFly</li> <li>MySQL</li> <li>MongoDB</li> <li>PostgreSQL</li> <li>MariaDB</li> </ul>"},{"location":"about/#what-sorts-of-security-controls-does-openshift-provide-for-containers","title":"What sorts of security controls does OpenShift provide for containers?","text":"<p>OKD runs with the following security policy by default:</p> <ul> <li>Containers run as a non-root unique user that is separate from other system users<ul> <li>They cannot access host resources, run privileged, or become root</li> <li>They are given CPU and memory limits defined by the system administrator</li> <li>Any persistent storage they access will be under a unique SELinux label, which prevents others from seeing their content</li> <li>These settings are per project, so containers in different projects cannot see each other by default</li> </ul> </li> <li>Regular users can run Docker, source, and custom builds<ul> <li>By default, Docker builds can (and often do) run as root. You can control who can create Docker builds through the <code>builds/docker</code> and <code>builds/custom</code> policy resource.</li> </ul> </li> <li>Regular users and project admins cannot change their security quotas.</li> </ul> <p>Many containers expect to run as root (and therefore edit all the contents of the filesystem). The Image Author's guide gives recommendations on making your image more secure by default:</p> <ul> <li>Don't run as root</li> <li>Make directories you want to write to group-writable and owned by group id 0</li> <li>Set the net-bind capability on your executables if they need to bind to ports &lt; 1024</li> </ul> <p>If you are running your own cluster and want to run a container as root, you can grant that permission to the containers in your current project with the following command:</p> <pre><code># Gives the default service account in the current project access to run as UID 0 (root)\noc adm add-scc-to-user anyuid -z default\n</code></pre> <p>See the security documentation more on confining applications.</p>"},{"location":"charter/","title":"OKD Working Group Charter","text":"<p>v1.1</p> <p>2019-09-21</p>"},{"location":"charter/#introduction","title":"Introduction","text":"<p>The charter describes the operations of the OKD Working Group (OKD WG).</p> <p>OKD is the Origin Community Distribution of Kubernetes that is upstream to Red Hat\u2019s OpenShift Container Platform. It is built around a core of OCI containers and Kubernetes container cluster management. OKD is augmented by application lifecycle management functionality and DevOps tooling.</p> <p>The OKD Working Group's purpose is to discuss, give guidance to, and enable collaboration on current development efforts for OKD, Kubernetes, and related CNCF projects. The OKD Working Group will also include the discussion of shared community goals for OKD 4 and beyond. Additionally, the Working Group will produce supporting materials and best practices for end-users and will provide guidance and coordination for CNCF projects working within the SIG's scope.</p> <p>The OKD Working Group is independent of both Fedora and the Cloud Native Computing Foundation (CNCF). The OKD Working Group is a community sponsored by Red Hat.</p>"},{"location":"charter/#mission","title":"Mission","text":"<p>The mission of the OKD Working Group is:</p> <ul> <li>To collaborate on areas related to developing, deploying, managing, and operating OKD.</li> <li>To develop informational resources like guides, tutorials, and white papers to give the community an understanding of best practices, trade-offs, and value adds as it relates to developing, deploying, and managing applications in cloud native environments.</li> <li>To identify suitable projects and gaps in the ecosystem for future collaboration and coordination with those projects.</li> </ul>"},{"location":"charter/#areas-considered-in-scope","title":"Areas considered in Scope","text":"<p>The OKD Working Group focuses on the following end-user related topics of the lifecycle of cloud-native applications:</p> <ul> <li>Installation of OKD</li> <li>Integration with Fedora CoreOS. Other Linux distributions will be gated based on community interests and resource commitments and the establishment of Sub-Working Groups</li> <li>OKD delivery and release management</li> <li>OKD management and operations</li> </ul> <p>The Working Group will work on developing best practices, fostering collaboration between related projects, and working on improving tool interoperability. Additionally, the Working Group will propose new initiatives and projects when capability gaps in the current ecosystem are defined.</p> <p>The following, non-exhaustive, sample list of activities and deliverables are in-scope for the Working Group:</p> <ul> <li>Operator Centric Installer</li> <li>Definition and Documentation of requirements for running on Linux</li> <li>Identification of requirements</li> <li>Submission of code to upstream sources</li> <li>Development of Continuous Integration</li> <li>Education material to help provide guidance for the community</li> <li>Might include a summary of projects available in the community</li> <li>Definitions of implementations and patterns for best practices for delivering OKD</li> <li>Best practices for OKD management</li> </ul>"},{"location":"charter/#areas-considered-out-of-scope","title":"Areas considered out of Scope","text":"<p>Anything not explicitly considered in the scope above. Example include:</p> <ul> <li>Development of any Linux distribution or variant</li> <li>Initiation of new open source projects</li> <li>Definition of standards for components like container images other infrastructure-level building blocks of cloud-native applications</li> </ul>"},{"location":"charter/#governance","title":"Governance","text":""},{"location":"charter/#operations","title":"Operations","text":"<p>The OKD Working Group is run and managed by the following chairs:</p> <ul> <li>Red Hat Community Development Liaison - Diane Mueller (Red Hat)</li> <li>External to Red Hat - Jaime Magiera (UMich)</li> <li>Red Hat Engineering Liaison - Vadim Rutkovsky (Red Hat)</li> <li>Fedora CoreOS Engineering Liaison - Timoth\u00e9e Ravier (Red Hat)</li> </ul> <p>Note</p> <p>The referenced names and chair positions will be edited in-place as chairs are added, removed, or replaced. See the roles of chairs section for more information.</p> <p>A dedicated git repository will be the authoritative archive for membership list, code, documentation, and decisions made. The repository, along with this charter, will be hosted at github.com/openshift/community.</p> <p>The mailing list at groups.google.com/forum/#!forum/okd-wg will be used as a place to call for and publish group decisions, and to hold discussions in general.</p>"},{"location":"charter/#working-group-membership","title":"Working Group Membership","text":"<p>All active members of the Working Group are listed in the <code>MEMBERS.md</code> file with their name.</p> <p>New members can apply for membership by creating an Issue or Pull Request on the repository on GitHub indicating their desire to join.</p> <p>Membership can be surrendered by creating an Issue stating this desire, or by creating a Pull Request to remove the own name from the members list.</p>"},{"location":"charter/#decision-process","title":"Decision Process","text":"<p>This group will seek consensus decisions. After public discussion and consideration of different opinions, the Chair and/or Co-Chair will record a decision and summarize any objections.</p> <p>All WG members who have joined the GitHub group at least 21 days prior to the vote are eligible to vote. This is to prevent people from rallying outside supporters for their desired outcome.</p> <p>When the group comes to a decision in a meeting, the decision is tentative. Any group participant may object to a decision reached at a meeting within 7 days of publication of the decision on the GitHub Issue and/or mailing list. That decision must then be confirmed on the GitHub Issue via a Call for Agreement.</p> <p>The Call for Agreement, when a decision is required, will be posted as a GitHub Issue or Pull Request and must be announced on the mailing list. It is an instrument to reach a time-bounded lazy consensus approval and requires a voting period of no less than 7 days to be defined (including a specific date and time in UTC).</p> <p>Each Call for Agreement will be considered independently, except for elections of Chairs.</p> <p>The Chairs will submit all Calls for Agreement that are not vague, unprofessional, off-topic, or lacking sufficient detail to determine what is being agreed.</p> <p>In the event that a Call for Agreement falls under the delegated authority or within a chartered Sub-Working Group, the Call for Agreement must be passed through the Sub-Working Group before receiving Working Group consideration.</p> <p>A Call for Agreement may require quorum of Chairs under the circumstances outlined in the Charter and Governing Documents section.</p> <p>A Call for Agreement is mandatory when:</p> <ul> <li>A Chair determines that the topic requires a Call for Agreement.</li> <li>When petitioned by members of the Working Group and submitted to the Chairs to call a vote unless rejected for cause.</li> <li>Technical decisions that add, remove, or change dependencies and requirements.</li> <li>Revoke a previous decision made by the Call for Agreement process.</li> <li>Approval of a Sub-Working Group when such SWG has any delegated authority.</li> </ul> <p>Once the Call for Agreement voting period has elapsed, all votes are counted, with at least a 51% majority of votes needed for consensus. A Chair will then declare the agreement \u201caccepted\u201d or \u201cdeclined\u201d, ending the Call for Agreement.</p> <p>Once rejected, a Call for Agreement must be revised before re-submission for a subsequent vote. All rejected Calls for Agreement will be reported to the Working Group as rejected.</p>"},{"location":"charter/#charter-and-governing-documents","title":"Charter and Governing Documents","text":"<p>The Working Group may, from time to time, adopt or amend its Governing Documents and Charter, using a modified Call for Agreement process:</p> <ul> <li>A quorum of at least 51% of active Working Group Members must vote.</li> <li>A quorum of 3 Chairs is needed</li> <li>Two-thirds of the voting quorum must approve the proposal.</li> <li>A majority of Chairs must approve the proposal.</li> <li>A public notice period of no less than 14 days from the Call for Agreement proposing the change must elapse before voting begins.</li> <li>Any Call for Agreement that follows this process is considered a Governing Document.</li> </ul> <p>For initial approval of this Charter via Call for Agreement all members are eligible to vote, even those that have been a member for less than 21 days. This Charter will be approved if there is a majority of positive votes.</p>"},{"location":"charter/#organizational-roles","title":"Organizational Roles","text":""},{"location":"charter/#role-of-chairs","title":"Role of Chairs","text":"<p>The primary role of Chairs is to run operations and the governance of the group. The Chairs are responsible for:</p> <ul> <li>Setting the agenda for meetings.</li> <li>Extending discussion via asynchronous communication to be inclusive of members who cannot attend a specific meeting time.</li> <li>Scheduling discussing of proposals that have been submitted.</li> <li>Asking for new proposals to be made to address an identified need.</li> <li>Oversee disciplinary action for members. The Chairs have the authority to declare a member inactive and expel members for cause.</li> <li>Chairs will serve for one-year revolving terms and will be approved using the Condorcet Method. Upon the expiration of a Chair\u2019s term, it will continue for another year, unless challenged.</li> </ul> <p>The terms for founding Chairs start on the approval of this charter.</p> <p>When no candidate has submitted their name for consideration, the current Chairs may appoint an acting Chair until a candidate comes forward.</p> <p>Chairs must be active members. Any inactivity, disability, or ineligibility results in immediate removal.</p> <p>Chairs may be removed by petition to the Working Group through the Call for Agreement process outlined above.</p> <p>Additional Chairs may be added so long as the existing number of Chairs is odd. These Chairs are added using a Call for Agreement. Extra Chairs enjoy the same rights, responsibilities, and obligations of a Chartered Chair. Upon vacancy of an Extra Chair, it may be filled by appointment by the remaining Chairs, or a majority vote of the Working Group until the term naturally expires.</p> <p>In the event that an even number of Chairs exist and vote situation arises, the Chairs will randomly select one chair to abstain.</p>"},{"location":"charter/#role-of-sub-working-groups","title":"Role of Sub-Working Groups","text":"<p>Each Sub-Working Group (SWG) must have a Chair working as an active sponsor. Under the mandate of the Working Group, each SWG will have the autonomy to establish their own charter, membership rules, meeting times, and management processes. Each SWG will also have the authority to make in-scope decisions as delegated by the Working Group.</p> <p>SWGs are required to submit their agreed Charter to the Working Group for information and archival. The Chairs can petition for dissolution of an inactive or hostile SWG by Call for Agreement. Once dissolved the SWG\u2019s delegated Charter and outstanding authority to make decisions is immediately revoked. The Chairs may then take any required action to restrict access to Working Group Resources.</p> <p>No SWG will have authority with regards to this Charter or other OKD Working Group Governing Documents.</p>"},{"location":"communications/","title":"OKD Working Group Communications","text":"<p>The working group issues regular communications through several different methods. There are also a few ways to contact the working group depending on the type of communication needed. This page will help you navigate the various communication channels that the working group utilizes.</p>"},{"location":"communications/#e-mail","title":"E-Mail","text":"<p>The working group maintains a mailing list as well as several email addresses.</p> <p>Mailing List</p> <p>okd-wg mailing list</p> <p>The purpose of this list is to discuss, give guidance &amp; enable collaboration on current development efforts for OKD4, Fedora CoreOS (FCOS) &amp; Kubernetes. Please note that the focus of this list is the active development of OKD, and the processes of this community, its is not intended as a forum for reporting bugs or requesting help with operating OKD.</p> <p>Reporting Addresses</p> <p>The working group uses several e-mail addresses to receive communications from the community based on the intent of the message.</p> <p>chairs@okd.io</p> <p>The chairs address is for messages that are related to the working group and its processes. It is intended for communications that will go directly to the working group chairs and not the wider community.</p> <p>security@okd.io</p> <p>The security address is intended for any reporting of sensitive or confidential security related bugs and findings about OKD.</p> <p>info@okd.io</p> <p>The info address is for requesting general information about the working group and its processes.</p>"},{"location":"communications/#social-media","title":"Social Media","text":"<p>The working group uses social media to broadcast updates about new releases, working group meetings, and community events.</p> <p>Twitter</p> <p>@okd_io</p>"},{"location":"communications/#slack","title":"Slack","text":"<p>The working group maintains a presence on the Kubernetes community Slack instance in the <code>#openshift-users</code> channel. This channel is a good place to come for OKD-specific help with operations and usage. If you are not yet a member of the Kubernetes workspace, you can request your invitation on slack.kubernetes.io.</p>"},{"location":"communications/#github","title":"GitHub","text":"<p>The working group maintains several repositories on GitHub in the OKD-Project organization. These repositories contain information and discussions about OKD and the working group's future plans.</p> <p>okd-project/okd discussions</p> <p>The okd repository discussions board is a good place to visit for researching or raising specific operational issues with OKD.</p> <p>okd-project/planning project board</p> <p>The planning repository contains a kanban board which records the current state of the working group and its related projects.</p>"},{"location":"community/","title":"End User Community","text":"<p>OKD has an active community of end-users, with many different use-cases.  From enterprises, academic institutions or home hobbyists.  In addition to the end-user community there is a smaller community of volunteers that contribute to the OKD project by helping other users resolve issues or by participating in one of the OKD working groups to enhance the OKD project.</p>"},{"location":"community/#code-of-conduct","title":"Code of Conduct","text":"<p>We want the OKD community to be a welcoming community, where everyone is treat with respect, so the link to the code of conduct should be made visible at all events</p> <p>Red Hat supports the Inclusive Naming Initiative and the OKD project follows the guidelines and recommendations from that project.  All contributions to OKD must also follow their guidelines</p>"},{"location":"community/#end-user-community_1","title":"End-User community","text":"<p>The community of OKD users is a self-supporting community.  There is no official support for OKD, all help is community provided.</p> <p>The Help section provides details on how to get help for any issues you may be experiencing.</p> <p>We encourage all users to participate in discussions and to help fellow users where they can.</p>"},{"location":"community/#contributing-to-okd","title":"Contributing to OKD","text":"<p>The OKD project has a charter, setting out how the project is run.</p> <p>If you want to join the team of volunteers working on the OKD project then details of how to become a contributor are set out here.</p>"},{"location":"conduct/","title":"OKD Community Code of Conduct","text":"<p>Every community can be strengthened by a diverse variety of viewpoints, insights, opinions, skill sets, and skill levels. However, with diversity comes the potential for disagreement and miscommunication. The purpose of this Code of Conduct is to ensure that disagreements and differences of opinion are conducted respectfully and on their own merits, without personal attacks or other behavior that might create an unsafe or unwelcoming environment.</p> <p>These policies are not designed to be a comprehensive set of things you cannot do. We ask that you treat your fellow community members with respect and courtesy. This Code of Conduct should be followed in spirit as much as in letter and is not exhaustive.</p> <p>All okd events and community members are governed by this Code of Conduct and anti-harassment policy. We expect working group chairs and organizers to enforce these guidelines throughout all events, and we expect attendees, speakers, sponsors, and volunteers to help ensure a safe environment for our whole community.</p> <p>For the purposes of this Code of Conduct:</p> <ul> <li> <p>An event is any in-person or on-line gathering, e-mail thread, chat room,   code or documentation contribution, or any situation where okd community   members are invited to gather and communicate.</p> </li> <li> <p>A community member is anyone who attends okd events and/or participates in   contributing or maintaining the okd project. okd community members are:</p> <ul> <li> <p>Considerate:     Contributions of every kind have far-ranging consequences. Just as your work depends on the work of others, decisions you make surrounding your contributions to the okd community affect your fellow community members. You are strongly encouraged to take those consequences into account while making decisions. Additionally, pinging a specific person with general questions is inconsiderate to that person. Always pose general question to the community as a whole.</p> </li> <li> <p>Patient:     Asynchronous communication can come with its own frustrations, even in the most responsive of communities. Please remember that our community is largely built on volunteered time. It might take some time to receive a response to questions, contributions, and requests for support. Repeated bumps or reminders in rapid succession are not good displays of patience. Always wait patiently for a response.</p> </li> <li> <p>Respectful:     Every community inevitably has disagreements, but disagreements are never an excuse for rudeness, hostility, threatening behavior, abuse (verbal or physical), or personal attacks. Always remember that it is possible to disagree respectfully and courteously.</p> </li> <li> <p>Kind:     Everyone should feel welcome in the okd community, regardless of background. Please be courteous, respectful and polite to fellow community members. Additionally, you are encouraged not to make assumptions about the background or identity of your fellow community members. Always follow the xref:okd-code-conduct-harassment[Anti-harassment policy].</p> </li> <li> <p>Inquisitive:     We encourage okd community members to ask early and ask often. Rather than asking whether you can ask a question (the answer is always yes!), simply ask your question. You are encouraged to provide as many specifics as possible. Code snippets in the form of Gists or other paste site links are almost always needed in order to get the most helpful answers. Refrain from pasting multiple lines of code directly into the Google Group channel or emails. Instead use gist.github.com or another paste site to provide code snippets. Always remember that the only stupid question is the one that does not get asked.</p> </li> <li> <p>Helpful:     The okd community is committed to being a welcoming environment for all users, regardless of skill level. Our community cannot grow without an environment where new users feel safe and comfortable asking questions. It can become frustrating to answer the same questions repeatedly; however, community members are expected to remain courteous and helpful to all users equally, regardless of skill or knowledge level. At the same time, everyone is expected to read the provided documentation thoroughly. We are happy to answer questions, provide strategic guidance, and suggest effective workflows, but we are not here to do your job for you. Always remember we were all beginners once upon a time,</p> </li> </ul> </li> </ul>"},{"location":"conduct/#anti-harassment-policy","title":"Anti-harassment policy","text":"<p>Harassment includes (but is not limited to) the following behaviors:</p> <ul> <li>Offensive comments related to gender (including gender expression and   identity), age, sexual orientation, disability, physical appearance, body   size, race, and religion</li> <li>Derogatory terminology including words commonly known to be slurs</li> <li>Posting sexual images or imagery in public spaces</li> <li>Deliberate intimidation</li> <li>Stalking</li> <li>Posting others\u2019 personal information without explicit permission</li> <li>Sustained disruption of talks or other events</li> <li>Inappropriate physical contact</li> <li>Unwelcome sexual attention</li> </ul> <p>Community members asked to stop any harassing behavior are expected to comply immediately. In particular, community members should not use sexual images, activities, or other material. Community members should not use sexual attire or otherwise create a sexualized environment at community events.</p> <p>In addition to the behaviors outlined above, continuing to behave in a certain way after you have been asked to stop also constitutes harassment, even if that behavior is not specifically outlined in this policy. It is considerate and respectful to stop doing something after you have been asked to stop, and all community members are expected to comply with such requests immediately.</p>"},{"location":"conduct/#policy-violations","title":"Policy violations","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting codeofconduct@okd.io.</p> <p>If a community member engages in harassing behavior, organizers or working group chairs may take any action deemed appropriate. These actions may include but are not limited to warning the offender and expelling the offender from an event. The OKD working group leaders might determine that the offender should be barred from participating in the community.</p> <p>Event organizers and working group leaders will be happy to help community members contact security or local law enforcement, provide escorts to an alternate location, or otherwise assist those experiencing harassment to feel safe for the duration of an event. We value the safety and well-being of our community members and want everyone to feel welcome at our events, both online and in-person.</p> <p>We expect all community members to follow these policies during all of our events.</p> <p>The okd Community Code of Conduct is licensed under the Creative Commons Attribution-Share Alike 3.0 license. Our Code of Conduct was adapted from Codes of Conduct of other open source projects, including:</p> <ul> <li>Ansible</li> <li>Contributor Covenant</li> <li>Elastic</li> <li>The Fedora Project</li> <li>OpenStack</li> <li>Puppet Labs</li> <li>Ubuntu</li> </ul>"},{"location":"contributor/","title":"Contributor Community","text":"<p>OKD is built from many different open source projects - Fedora CoreOS, the CentOS Stream and UBI RPM ecosystems, cri-o, Kubernetes, and many different extensions to Kubernetes. The <code>openshift</code> organization on GitHub holds active development of components on top of Kubernetes and references projects built elsewhere. Generally, you'll want to find the component that interests you and review their README.md for the processes for contributing.</p> <p>Community process and questions can be raised in our community repo and issues opened in this repository (Bugzilla locations coming soon).</p> <p>Our unified continuous integration system tests pull requests to the ecosystem and core images, then builds and promotes them after merge. To see the latest development releases of OKD visit our continuous release page. These releases are built continuously and expire after a few days. Long lived versions are pinned and then listed on our stable release page.</p> <p>All contributions are welcome - OKD uses the Apache 2 license and does not require any contributor agreement to submit patches.  Please open issues for any bugs or problems you encounter, ask questions in the OKD discussion forum, or get involved in the Kubernetes project at the container runtime layer.</p>"},{"location":"contributor/#becoming-a-contributor","title":"Becoming a contributor","text":"<p>The easiest way to get involved in the community is to:</p> <ul> <li>watch replays of previous working group meetings</li> <li>attend one of the working group meetings (no invite needed, just use the link in the calendar to join the Zoom video conference).</li> <li>join the OKD Working Group Google Group</li> <li>join the Matrix room</li> </ul> <p>The OKD project has a charter, setting out how the project is run.</p>"},{"location":"contributor/#working-groups","title":"Working Groups","text":"<p>The project is managed by a bi-weekly working group video call:</p> <p>The main working group is where are the major project decisions are made, but when a specific work item needs to be completed a sub-group may be formed, so a focussed set of volunteers can work on a specific area.</p>"},{"location":"crc/","title":"CodeReady Containers for OKD","text":"<p>CodeReady Containers brings a minimal, single node OKD 4 cluster to your local computer. This cluster provides a minimal environment for development and testing purposes. CodeReady Containers is mainly targeted at running on developers' laptops and desktops.  Note that arm64 OKD payload is not yet available.</p>"},{"location":"crc/#download-codeready-containers-for-okd","title":"Download CodeReady Containers for OKD","text":"<p>Run a developer instance of OKD4 on your local workstation with CodeReady Containers built for OKD - &gt;No Pull Secret Required! The Getting Started Guide explains how to install and use CodeReady Containers.</p> <p>You can fetch crc binaries without Red Hat subscription here</p> <pre><code>$ crc config set preset okd\nChanges to configuration property 'preset' are only applied when the CRC instance is created.\nIf you already have a running CRC instance with different preset, then for this configuration change to take effect, delete the CRC instance with 'crc delete', setup it with `crc setup` and start it with 'crc start'.\n\n$ crc config view\n- consent-telemetry                     : yes\n- preset                                : okd\n</code></pre> <p>If you encounter any problems, please open a discussion item in the OKD GitHub Community!</p>"},{"location":"crc/#crc-working-group","title":"CRC Working group","text":"<p>There is a working group looking at automating the OKD CRC build process.  If you want technical details on how to build OKD CRC see the working group section of this site</p>"},{"location":"docs/","title":"Documentation","text":"<p>There are 2 primary sources of information for OKD:  </p> <ul> <li>community documentation - https://okd.io (this site)</li> <li>product documentation - https://docs.okd.io</li> </ul>"},{"location":"docs/#updates-and-issues","title":"Updates and Issues","text":"<p>If you encounter an issue with the documentation or have an idea to improve the content or add new content then please follow the directions below to learn how you can get changes made.</p> <p>The source for the documentation is managed in GitHub. There are different processes for requesting changes in the community and product documentation:</p>"},{"location":"docs/#community-documentation","title":"Community documentation","text":"<p>The OKD Documentation subgroup is responsible for the community documentation.  The process for making changes is set out in the working group section of the documentation</p>"},{"location":"docs/#product-documentation","title":"Product documentation","text":"<p>The OKD docs are built off the openshift/openshift-docs repo. If you notice any problems in the OKD docs that need to be addressed, you can either create a pull request with those changes against the openshift/openshift-docs repo or create an issue to suggest the changes.</p> <p>Among the changes you could suggest are:</p> <ul> <li>errors</li> <li>typos</li> <li>missing information</li> <li>incorrect product name (OpenShift Container Platform instead of OKD)</li> <li>Incorrect operating system (RHEL or RHCOS instead of FCOS)</li> <li>incorrect code examples</li> </ul> <p>If you create an issue, please do the following:</p> <ul> <li>Add [OKD] to the title of the issue.</li> <li>Provide as much information as possible, including the problem, the exact location in the file, the versions of OKD that the error affects (if known), and the correction you would like to see. A link to the file with the problem is extremely helpful.</li> <li> <p>If you have the appropriate permissions, assign the issue to Michael Burke (mburke5678) so that the issue gets our direct attention.  You can assign an issue by including the following in the issue description:</p> <pre><code>/assign @mburke5678\n</code></pre> <p>If not, you can @ mention mburke5678 in a comment. - If you have the permissions, add a <code>kind/documentation</code> label.</p> </li> </ul>"},{"location":"docs/#testing-changes-locally","title":"Testing changes locally","text":"<p>Before opening a pull request, you might want to inspect your changes locally. To do this you will need to use podman or docker. After cloning the openshift-docs repository, run the following command inside the root of the repository:</p> <pre><code>podman run --rm -it -v `pwd`:/docs:Z quay.io/openshift-cs/asciibinder asciibinder build --distro openshift-origin\n</code></pre> <p>Note: substitute <code>docker</code> for <code>podman</code> if you are using docker instead</p> <p>If this process succeeds, you will find the rendered output files in <code>./_preview/openshift-origin/latest/</code>. To view the landing page for the documentation version built, open <code>./_preview/openshift-origin/latest/welcome/index.html</code> in your browser.</p> <p>There are 2 caveats to be aware of when building the site locally:</p> <ol> <li>you will most likely see some error output that looks like this:</li> </ol> <pre><code>WARN: The following branches do not exist in your local git repo:\n- enterprise-3.10\n- enterprise-3.11\n- enterprise-3.6\n- enterprise-3.7\n- enterprise-3.9\n- enterprise-4.11\n- enterprise-4.6\n- enterprise-4.7\nThe build will proceed but these branches will not be generated.\nWARN: The /docs/_topic_map.yml20230919-1-628e1m file on branch 'OSDOCS-7251-Azure-CPMS-mutli-subnet' references 215 nonexistent topics. Set logging to 'debug' for details.\nWARN: Branch OSDOCS-7251-Azure-CPMS-mutli-subnet includes 6073 files that are not referenced in the /docs/_topic_map.yml20230919-1-bhydnn file. Set logging to 'debug' for details.\n</code></pre> <p>This is a normal type of warning produced by the build process and doesn't necessarily need investigating. 2. The root index file will be <code>./index-community.html</code>, but this file does not link properly to the content in <code>./_preview</code> directory. if you want to see the main page for the version, the file is <code>./_preview/openshift-origin/latest/welcome/index.html</code>.</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Below are answers to common questions regarding OKD installation and administration. If you have a suggested question or a suggested improvement to an answer, please feel free to reach out.</p>"},{"location":"faq/#what-are-the-relations-with-ocp-project-is-okd4-an-upstream-of-ocp","title":"What are the relations with OCP project? Is OKD4 an upstream of OCP?","text":"<p>In 3.x release time OKD was used as an upstream project for Openshift Container Platform. OKD could be installed on Fedora/CentOS/RHEL and used CentOS based images to install the cluster. OCP, however, could be installed only on RHEL and its images were rebuilt to be RHEL-based.</p> <p>Universal Base Image project has enabled us to run RHEL-based images on any platform, so the full image rebuild is no longer necessary, allowing OKD4 project to reuse most images from OCP4. There is another critical part of OCP - Red Hat Enterprise Linux CoreOS. Although RHCOS is an open source project (much like RHEL8) it's not a community-driven project. As a result, OKD workgroup has made a decision to use Fedora CoreOS - open source and community-driven project - as a base for OKD4. This decision allows end-users to modify all parts of the cluster using prepared instructions.</p> <p>It should be noted that OKD4 is being automatically built from OCP4 ci stream, so most of the tests are happening in OCP CI and being mirrored to OKD. As a result, OKD4 CI doesn't have to run a lot of tests to ensure the release is valid.</p> <p>These relationships are more complex than \"upstream/downstream\", so we use \"sibling distributions\" to describe its state.</p>"},{"location":"faq/#how-stable-is-okd4","title":"How stable is OKD4?","text":"<p>OKD4 builds are being automatically tested by release-controller. Release is rejected if either installation, upgrade from previous version or conformance test fails. Test results determine the upgrade graph, so for instance, if upgrade tests passed for beta5-&gt;rc edge, clusters on beta5 can be directly updated to rc release, bypassing beta6.</p> <p>The OKD stable version is released bi-weekly, following Fedora CoreOS schedule, client tools are uploaded to Github and images are mirrored to Quay.</p>"},{"location":"faq/#can-i-run-a-single-node-cluster","title":"Can I run a single node cluster?","text":"<p>Currently, single-node cluster installations cannot be deployed directly by the 4.7 installer. This is a known issue. Single-node cluster installations do work with the 4.8 nightly installer builds.</p> <p>As an alternative, if OKD version 4.7 is needed, you may have luck with Charro Gruver's OKD 4 Single Node Cluster instructions. You can also use Code Ready Containers (CRC) to run a single-node cluster on your desktop.</p>"},{"location":"faq/#what-to-do-in-case-of-errors","title":"What to do in case of errors?","text":"<p>If you experience problems during installation you must collect the bootstrap log bundle, see instructions</p> <p>If you experience problems post installation, collect data of your cluster with:</p> <pre><code>oc adm must-gather\n</code></pre> <p>See documentation for more information.</p> <p>Upload it to a file hoster and send the link to the developers (Slack channel, ...)</p> <p>During installation the SSH key is required. It can be used to SSH onto the nodes later on - <code>ssh core@&lt;node ip&gt;</code></p>"},{"location":"faq/#where-do-i-seek-support","title":"Where do I seek support?","text":"<p>OKD is a community-supported distribution, Red Hat does not provide commercial support of OKD installations.</p> <p>Contact us on Slack:</p> <ul> <li>Workspace: Kubernetes, Channel: #openshift-users (for installation &amp; usage questions/issues)</li> </ul> <p>For OKD-specific help, visit the community-led discussion https://github.com/okd-project/okd/discussions forum</p> <p>See https://openshift.tips/ for useful Openshift tips</p>"},{"location":"faq/#where-can-i-find-upgrades","title":"Where can I find upgrades?","text":"<p>https://amd64.origin.releases.ci.openshift.org/</p> <p>Warning</p> <p>Nightly builds (from <code>4.x.0-0.okd</code>) are pruned every 72 hours.</p> <p>If your cluster uses these images, consider mirroring these files to a local registry.</p> <p>Builds from the stable-4 stream are not removed.</p>"},{"location":"faq/#how-can-i-upgrade-my-cluster-to-a-new-version","title":"How can I upgrade my cluster to a new version?","text":"<p>Find a version where a tested upgrade path is available from your version for on</p> <p>https://amd64.origin.releases.ci.openshift.org/</p> <p>Upgrade options:</p> <p>Preferred ways:</p> <ul> <li>Web Console: Home -&gt; Overview -&gt; Tab: Cluster, Card: Overview -&gt; View settings -&gt; Update Status</li> <li>Shell:   Upgrades to latest available version</li> </ul> <pre><code>oc adm upgrade\n</code></pre> <p>Last resort:</p> <p>Upgrade to a certain version (will ignore the update graph!)</p> <pre><code>oc adm upgrade --force --allow-explicit-upgrade=true --to-image=registry.ci.openshift.org/origin/release:4.4.0-0.okd-2020-03-16-105308\n</code></pre> <p>This will take a while; the upgrade may take several hours. Throughout the upgrade, kubernetes API would still be accessible and user workloads would be evicted and rescheduled as nodes are updated.</p>"},{"location":"faq/#interesting-commands-while-an-upgrade-runs","title":"Interesting commands while an upgrade runs","text":"<p>Check overall upgrade status:</p> <pre><code>oc get clusterversion\n</code></pre> <p>Check the status of your cluster operators:</p> <pre><code>oc get co\n</code></pre> <p>Check the status of your nodes (cluster upgrades may include base OS updates):</p> <pre><code>oc get nodes\n</code></pre>"},{"location":"faq/#how-can-i-find-out-whats-inside-of-a-ci-release-and-which-commit-id-each-component-has","title":"How can I find out what's inside of a (CI) release and which commit id each component has?","text":"<p>This one is very helpful if you want to know if a certain commit has landed in your current version:</p> <pre><code>oc adm release info registry.ci.openshift.org/origin/release:4.4  --commit-urls\n</code></pre> <pre><code>Name:      4.4.0-0.okd-2020-04-10-020541\nDigest:    sha256:79b82f237aad0c38b5cdaf386ce893ff86060a476a39a067b5178bb6451e713c\nCreated:   2020-04-10T02:14:15Z\nOS/Arch:   linux/amd64\nManifests: 413\n\nPull From: registry.ci.openshift.org/origin/release@sha256:79b82f237aad0c38b5cdaf386ce893ff86060a476a39a067b5178bb6451e713c\n\nRelease Metadata:\n  Version:  4.4.0-0.okd-2020-04-10-020541\n  Upgrades: &lt;none&gt;\n\nComponent Versions:\n  kubernetes 1.17.1\n  machine-os 31.20200407.20 Fedora CoreOS\n\nImages:\n  NAME                                           URL\n  aws-machine-controllers                        https://github.com/openshift/cluster-api-provider-aws/commit/5fa82204468e71b44f65a5f24e2675dbfa0f5c29\n  azure-machine-controllers                      https://github.com/openshift/cluster-api-provider-azure/commit/832a43a30d7f00cd6774c1f5cd117aeebbe1b730\n  baremetal-installer                            https://github.com/openshift/installer/commit/a58f24b0df7e3699b39d4ae1d23c45672706934d\n  baremetal-machine-controllers\n  baremetal-operator\n  baremetal-runtimecfg                           https://github.com/openshift/baremetal-runtimecfg/commit/09850a724d9290ffb05db3dd7f4f4c748b982759\n  branding                                       https://github.com/openshift/origin-branding/commit/068fa1eac9f31ffe13089dd3de2ec49c153b2a14\n  cli                                            https://github.com/openshift/oc/commit/2576e482bf003e34e67ba3d69edcf5d411cfd6f3\n  cli-artifacts                                  https://github.com/openshift/oc/commit/2576e482bf003e34e67ba3d69edcf5d411cfd6f3\n  cloud-credential-operator                      https://github.com/openshift/cloud-credential-operator/commit/446680ed10ac938e11626409acb0c076edd3fd52\n  ...\n</code></pre>"},{"location":"faq/#how-can-i-find-out-the-version-of-a-particular-package-within-an-okd-release","title":"How can I find out the version of a particular package within an OKD release?","text":"<pre><code># Download and enter the machine-os-content container.\npodman run --rm -ti `oc adm release info quay.io/openshift/okd:4.13.0-0.okd-2023-06-24-145750 --image-for=machine-os-content`\n\n# Query the particular rpm. For example, to get the version of the cri-o package in the release, use the following:\nrpm -qa cri-o\n</code></pre>"},{"location":"faq/#how-to-use-the-official-installation-container","title":"How to use the official installation container?","text":"<p>The official installer container is part of every release.</p> <pre><code># Find out the installer image.\noc adm release info quay.io/openshift/okd:4.7.0-0.okd-2021-04-24-103438 --image-for=installer\n\n# Example output\n# quay.io/openshift/okd-content@sha256:521cd3ac7d826749a085418f753f1f909579e1aedfda704dca939c5ea7e5b105\n\n# Run the container via Podman or Docker to perform tasks. e.g. create ignition configurations\ndocker run -v $(pwd):/output -ti quay.io/openshift/okd-content@sha256:521cd3ac7d826749a085418f753f1f909579e1aedfda704dca939c5ea7e5b105 create ignition-configs\n</code></pre>"},{"location":"help/","title":"Help","text":"<p>There is no official product support for OKD as it is a community project. All assistance is provided by volunteers from the user community.</p>"},{"location":"help/#how-to-ask-for-help","title":"How to ask for help","text":"<p>For questions or feedback, start a discussion on the discussion forum or reach us on Kubernetes Slack on #openshift-users. If you are not a member of the Kubernetes Slack workspace, you can request your invitation on slack.kubernetes.io.</p>"},{"location":"help/#community-etiquette","title":"Community Etiquette","text":"<p>As all assistance is provided by the community, you are reminded of the code-of-conduct when asking a question or replying to a question.</p> <p>Before starting a new discussion topic, do a search on the discussion forum to see if anyone else has already raised the same issue - then contribute to the existing discussion topic rather than starting a new topic.</p> <p>When seeking help you should provide all the information a community volunteer may need to assist you.  The easier it is for a volunteer to understand your issue, the more likely they are to provide assistance.</p> <p>This information should include:</p> <ul> <li>the version of OKD you are using</li> <li>the platform you are running on (and type of install - IPI / UPI)</li> <li>if you are following instructions and are having issues, a link to the instructions</li> <li>the must gather logs - these should be uploaded to a sharing site, such as Google Drive then a public link added to the discussion topic</li> <li>a description of the steps that can be used to recreate your issue, if applicable</li> </ul> <p>Please do not tag people you see answering other questions to try to get a faster answer as it is anti-social.  We have an active community and it is up to individuals which questions they feel they want to respond to.</p>"},{"location":"help/#raising-bugs","title":"Raising bugs","text":"<p>We are trying to do all the diagnostic work in the discussion forum rather than using issues for the OKD project.  If you are certain you have discovered a bug, then please raise an issue, but if you are not sure if you have found a bug then use the discussion forum to discuss it.  If it turns out to be a bug, then the discussion topic can be converted to an issue.</p>"},{"location":"installation/","title":"Install OKD","text":""},{"location":"installation/#plan-your-installation","title":"Plan your installation","text":"<p>OKD supports 2 types of cluster install options:</p> <ul> <li>Installer-provisioned infrastructure (IPI)</li> <li>User-provisioned infrastructure (UPI)</li> </ul> <p>IPI is a largely automated install process, where the installer is responsible for setting up the infrastructure, where UPI requires you to set up the base infrastructure.  You can find further details in the documentation</p> <p>OKD support installation on bare metal hardware, a number of virtualization platforms and a number of cloud platforms, so you need to decide where you want to install OKD and that your environment has sufficient resources for the cluster to operate.  The documentation has more information to help you plan your installation.</p> <p>If you want to install on a typical developer workstation, then Code-Ready Containers may be a better options, as that is a cut-down installation designed to run on limited compute and memory resources.</p>"},{"location":"installation/#getting-started","title":"Getting Started","text":"<p>To obtain the openshift installer and client, visit releases for stable versions or https://amd64.origin.releases.ci.openshift.org/ for nightlies.</p> <p>You can verify the downloads using:</p> <pre><code>curl https://www.okd.io/vrutkovs.pub | gpg --import\n</code></pre> <p>Output</p> <pre><code>    gpg: key 3D54B6723B20C69F: public key \"Vadim Rutkovsky &lt;vadim@vrutkovs.eu&gt;\" imported\n    gpg: Total number processed: 1\n    gpg:               imported: 1\n</code></pre> <pre><code>gpg --verify sha256sum.txt.asc sha256sum.txt\n</code></pre> <p>Output</p> <pre><code>gpg: Signature made Mon May 25 18:48:22 2020 CEST\ngpg:                using RSA key DB861D01D4D1138A993ADC1A3D54B6723B20C69F\ngpg: Good signature from \"Vadim Rutkovsky &lt;vadim@vrutkovs.eu&gt;\" [ultimate]\ngpg:                 aka \"Vadim Rutkovsky &lt;vrutkovs@redhat.com&gt;\" [ultimate]\ngpg: WARNING: This key is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: DB86 1D01 D4D1 138A 993A  DC1A 3D54 B672 3B20 C69F\n</code></pre> <pre><code>sha256sum -c sha256sum.txt\n</code></pre> <p>Output</p> <pre><code>release.txt: OK\nopenshift-client-linux-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK\nopenshift-client-mac-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK\nopenshift-client-windows-4.4.0-0.okd-2020-05-23-055148-beta5.zip: OK\nopenshift-install-linux-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK\nopenshift-install-mac-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK\n</code></pre> <p>Please note that each nightly release is pruned after 72 hours. If the nightly that you installed was pruned, the cluster may be unable to pull necessary images and may show errors for various functionality (including updates).</p> <p>Alternatively, if you have the openshift client <code>oc</code> already installed, you can use it to download and extract the openshift installer and client from our container image:</p> <pre><code>oc adm release extract --tools quay.io/openshift/okd:4.5.0-0.okd-2020-07-14-153706-ga\n</code></pre> <p>Note</p> <p>You need a 4.x version of <code>oc</code> to extract the installer and the latest client. You can initially use the official Openshift client (mirror)</p> <p>There are full instructions in the OKD documentation for each supported platform, but the main steps for an IPI install are:</p> <ol> <li>extract the downloaded tarballs and copy the binaries into your PATH.</li> <li>run the following from an empty directory:     <pre><code>openshift-install create cluster\n</code></pre></li> <li>follow the prompts to create the install config<ul> <li>you will need to have cloud credentials set in your shell properly before installation.</li> <li>you must have permission to configure the appropriate cloud resources from that account (such as VPCs, instances, and DNS records).</li> <li>you must have already configured a public DNS zone on your chosen cloud before the install starts.</li> <li>you will also be prompted for a pull-secret that will be made available to all of of your machines - for OKD4 you should either paste the pull-secret you use for your registry, or paste <code>{\"auths\":{\"fake\":{\"auth\":\"aWQ6cGFzcwo=\"}}}</code> to bypass the required value check (see bug #182).</li> </ul> </li> </ol> <p>Once the install completes successfully the console URL and an admin username and password will be printed. If your DNS records were correct, you should be able to log in to your new OKD4 cluster!</p> <p>To undo the installation and delete any cloud resources created by the installer, run</p> <pre><code>openshift-install destroy cluster\n</code></pre> <p>Note</p> <p>The OpenShift client tools for your cluster can be downloaded from the help drop down menu at the top of the web console.</p>"},{"location":"working-groups/","title":"Working Groups","text":"<p>OKD is governed by working groups as set out in the OKD Working Group Charter</p> <p>There is a primary working group, where all the main decisions are made regarding the project.</p> <p>Where an area of the project needs more time or is of interest to a subset of the working group membership, then a sub-group will be formed for that specific area,</p> <p>The current sub groups are:</p> <ul> <li>Documentation working group</li> <li>Code-Ready Containers on OKD working group</li> <li>OKD virtualization working group</li> </ul>"},{"location":"working-groups/#okd-primary-working-group","title":"OKD Primary Working Group","text":"<p>The OKD group meets virtually every other week.</p> <ul> <li>Calendar link : OKD working group calendar</li> <li>Connection details : OKD working group meetings</li> <li>Agenda link : OKD working group agenda and meeting nodes</li> <li>Previous meeting playlist : YouTube playlist</li> </ul> <p>You don't need an invitation to join a working group -- simply join the video call. You may also want to join other online discussions as set out in the contributor section</p>"},{"location":"blog/","title":"Posts","text":""},{"location":"blog/2021/05/06/okd_working_group_office_hours_at_kubeconeu_on_openshifttv/","title":"OKD Working Group Office Hours at KubeconEU on OpenShift.tv","text":"<p>On May 6th 2020, OKD-Working Group members hosted an hour long community led Office Hour with a brief introduction to the latest release by Red Hat's Charro Gruver then live Q/A!</p>"},{"location":"blog/2021/05/06/okd_working_group_office_hours_at_kubeconeu_on_openshifttv/#video-from-okd-working-group-office-hours-at-kubeconeu-on-openshifttv","title":"Video from OKD Working Group Office Hours at KubeconEU on OpenShift.tv","text":"<p>Join the OKD-Working Group and add your voice to the conversation!</p>"},{"location":"blog/2022/10/24/okd_streams_-_building_the_next_generation_of_okd_together/","title":"OKD Streams - Building the Next Generation of OKD together","text":"<p>OKD is the community distribution of Kubernetes that powers Red Hat OpenShift. The OKD community has created reusable Tekton build pipelines on a shared Kubernetes cluster for the OKD build pipelines so that they could manage the build &amp; release processes for OKD in the open.</p> <p>With the operate-first.cloud hosted at the massopen.cloud, the OKD community has launched a fully open source release pipeline that the community can participate in to help support and manage the release cycle ourselves. The OKD Community is now able to build and release stable builds of OKD 4.12 on both Fedora CoreOS and the newly introduced CentOS Stream CoreOS. We are calling it OKD Streams.</p>"},{"location":"blog/2022/10/24/okd_streams_-_building_the_next_generation_of_okd_together/#new-patterns-new-cicd-pipelines-and-a-new-coreos","title":"New Patterns, New CI/CD Pipelines and a new CoreOS","text":"<p>Today we invite you into our OKD Streams initiative. An OKD Stream refers to a build, test, and release pipeline for any configuration of OKD, the open source kubernetes distribution that powers OpenShift. The OKD working group is pleased to announce the availability of tooling and processes that will enable building and testing many configurations, or \"streams\".  The OKD Working Group and Red Hat Engineering are now testing one such stream that runs an upstream version of RHEL9 via CentOS Streams CoreOS (\u2018SCOS\u2019 for short) to improve our RHEL9 readiness signal for Red Hat OpenShift. It is the first of many OKD Streams that will enable developers inside and outside of Red Hat to easily experiment with and explore Cloud Native technologies. You can check out our MVP OKD on SCOS release here.</p> <p>With this initiative, the OKD working group has embraced new patterns and built new partnerships. We have leveraged the concepts in the open source managed service \u2018Operate First\u2019 pattern, worked with the CentOS and CoreOS communities to build a pipeline for building SCOS and applied new CI/CD technologies (Tekton) to build a new OKD release build pipeline service. The MVP of OKD Streams, for example, is an SCOS backed version of OKD built with a Tekton pipeline managed by the OKD working group that runs on AWS infrastructure managed by Operate First. Together we are unlocking some of the innovations to get better (and earlier) release signals for Kubernetes , OCP and RHEL and to enable the OKD community to get more deeply involved with the OKD build processes.</p> <p>The OKD Working group wanted to make participation in all of these activities easier for all Cloud Native developers and this has been the motivating force behind the OKD Streams initiative.</p>"},{"location":"blog/2022/10/24/okd_streams_-_building_the_next_generation_of_okd_together/#from-the-one-size-fits-all-to-built-to-order","title":"From the \u2018One Size Fits All\u2019 to \u2018Built to Order\u2019","text":"<p>There are main three problems that both the OKD working group and Red Hat Engineering teams spend a lot of time thinking about:</p> <ol> <li>how do we improve our release signals for OpenShift, RHEL, CoreOS</li> <li>how do we get features into the hands of our customer and partners faster</li> <li>how do we enable engineers to experiment and innovate</li> </ol> <p>Previously, what we referred to as an \u2018OKD\u2019 release, was built on the most recent release of OKD running on the latest stable release of Fedora CoreOS (FCOS for short).  In actuality, we had a singular release pipeline that built a release of OKD with a bespoke version of FCOS. These releases of OKD gave us early signals for the impact of new operating system features that would eventually be landing in RHEL, where they will surface in RHEL CoreOS (RHCOS). It was (and still is) a very good way for developers to experiment with OKD and explore its functionality.</p> <p>The OKD community wanted to empower wider use of OKD for experimentation in more use cases that required layering on additional resources in some cases, and in others use cases, reducing the footprints for edge and local deployments. OKD has been stable enough for some to run production deployments. CERN\u2019s OKD deployment on OpenStack, for example, is assembled with custom OKD build pipelines. The feedback from these OKD builds has been a source of inspiration for this OKD Streams initiative to enable more such use cases.</p> <p>The OKD Streams initiative invites more community input and feedback quickly into the project without interrupting the productized builds for OpenShift and OpenShift customers. We can experiment with new features that can then get pushed upstream into Kubernetes or downstream into the OpenShift product. We can reuse the Tekton build pipelines for building streams specific to HPC or Openstack or Bare Metal or whatever the payload customization needs to be for their organizations.</p> <p>Our goal is to make it simple for others to experiment.</p> <p>We are experimenting too. The first OKD Streams \u2018experiment\u2019 built with the new Tekton build pipeline running on an Operate First AWS Cluster is OKD running on SCOS, which is a future version of OpenShift running on a near-future version of RHEL that's leveraging CentOS Streams CoreOS. This will improve our RHEL9 readiness signal for OCP. Improved RHEL9 readiness signals with input from the community will showcase our work as we explore what the new OKD build service is going to mean for all of us.</p>"},{"location":"blog/2022/10/24/okd_streams_-_building_the_next_generation_of_okd_together/#tekton-pipelines-as-the-building-blocks","title":"Tekton Pipelines as the Building Blocks","text":"<p>Our new OKD Streams are built using Tekton pipelines, which makes it easier for us to explore building many different kinds of pipelines.</p> <p>Tekton is a Continuous Deployment (CD) system that enables us to run tasks and pipelines in a composable and flexible manner. This fits in nicely with our OKD Streams initiative where the focus is less on the artifacts that are produced than the pipeline that builds it.</p> <p>While OKD as a payload remains the core focus of the OKD Working Group, we are also collaborating with the Operate First Community to ensure that anyone is able to take the work we have done and lift and shift it to any cloud enabling OKD to run in any Kubernetes-based infrastructure anywhere. Now anybody can experiment and build their own \u2018stream\u2019 of OKD with the Tekton pipeline.</p> <p>This new pipeline approach enables builds that can be customized via parameters, even the tasks within the pipeline can be exchanged or moved around. Add your own tasks. They are reusable templates for creating your own testable stream of OKD. Run the pipelines on any infrastructure, including locally in Kubernetes using podman, for example, or you can run them on a vanilla Kubernetes cluster. We are enabling access to the Operate First managed OKD Build Service to deploy more of these builds and pipelines to get some ideas that we have at Red Hat out into the community for early feedback AND to let other community members test their ideas.</p> <p>As an open source community, we\u2019re always evolving and learning together. Our goal is to make OKD the goto place to experiment and innovate for the entire OpenShift ecosystem and beyond, to showcase new features and functionalities, and to fail fast and often without impacting product releases or incurring more technical debt.</p>"},{"location":"blog/2022/10/24/okd_streams_-_building_the_next_generation_of_okd_together/#the-ask","title":"THE ASK","text":"<p>Help drive faster innovation into OCP, OKD, Kubernetes and RHEL along with the multitude of other Cloud Native open source projects that are part of the OpenShift and the cloud native ecosystem.</p> <ul> <li>Download the MVP OKD/SCOS build and deploy it!</li> <li>Review our Tekton OKD Build pipelines. Try running them on your own Kubernetes cluster with Tekton - help us make our pipelines more efficient and easier to re-use.</li> <li>Review our pipeline documentation and help us make it better.</li> <li>Fork our pipelines and add your own tasks and resources and let us know how it goes.</li> <li>Come to an OKD Working Group meeting and share your OKD use cases with the rest of the community. We\u2019ll help you connect with like minded collaborators!</li> </ul> <p>This project is a game changer for lots of open source communities internally and externally. We know there are folks out there in the OKD working group and in the periphery that haven't spoken up and we'd love to hear from you, especially if you are currently doing bespoke OKD builds. Will this unblock your innovation the way we think it will?</p>"},{"location":"blog/2022/10/24/okd_streams_-_building_the_next_generation_of_okd_together/#additional-resources","title":"Additional Resources","text":"<ul> <li>OKD Github</li> <li>OKD.io</li> <li>MVP Release page</li> <li>Introducing OKD Streams</li> <li>Full OKD Streams playlist</li> <li>Operate First Principles</li> </ul>"},{"location":"blog/2022/10/24/okd_streams_-_building_the_next_generation_of_okd_together/#kudos-and-thank-you","title":"Kudos and Thank you","text":"<p>Operate First\u2019s Infrastructure Team: Thorsten Schwesig, Humair Khan, Tom Coufal, Marcel Hild Red Hat\u2019s CFE Team: Luigi Zuccarelli, Sherine Khoury OKD Working Group: Vadim Rutkovsky, Alessandro Di Stefano, Jaime Magiera, Brian Innes CentOS Cloud and HPC SIGs: Amy Marrich, Christian Glombek, Neal Gompa</p>"},{"location":"blog/2022/10/20/okd_at_kubecon__cloudnativecon_north_america_2022/","title":"OKD at KubeCon + CloudNativeCon North America 2022","text":"<p>Are you heading to Kubecon/NA October 24, 2022 - October 28, 2022 in Detroit at KubeCon + CloudNativeCon North America 2022?</p> <p>If so, here's where you'll find members of the OKD Working Group and Red Hat engineers that working on delivering the latest releases of OKD at Kubecon!</p>"},{"location":"blog/2022/10/20/okd_at_kubecon__cloudnativecon_north_america_2022/#october-25th","title":"October 25th","text":"<p>At the OpenShift Commons Gathering on Tuesday, October 25, 2022 | 9:00 a.m. - 6:00 p.m. EDT, we're hosting an in-person OKD Working Group Lunch &amp; Learn Meet up from 12 noon to 3 pm lead by co-chairs Jaime Magiera (ICPSR at University of Michigan Institute for Social Research), Diane Mueller(Red Hat) and special guests including Michael McCune(Red Hat) in Break-out room D at the Westin Book Cadillac a 10 minute walk from the conference venue. followed by a Lightning Talk: OKD Working Group Update &amp; Road Map on the OpenShift Common main stage at 3:45 pm. The main stage event will be live streamed via Hopin so if you are NOT attending in person, you'll be able to join us online.</p> <p>Registration for OpenShift Commons Gathering is FREE and OPEN to ALL for both in-person and virtual attendance - https://commons.openshift.org/gatherings/kubecon-22-oct-25/</p>"},{"location":"blog/2022/10/20/okd_at_kubecon__cloudnativecon_north_america_2022/#october-27th","title":"October 27th","text":"<p>At 11:30 am EDT, the OKD Working Group will hold a Kubecon Virtual Office Hour that on OKD Streams initiatives and the latest release lead by OKD Working Group members: Vadim Rutkovsky, Luigi Mario Zuccarelli, Christian Glombek and Michelle Krejci!</p> <p>Registration for the virtual Kubecon/NA event is required to join the Kubecon Virtual Office Hour</p> <p>If you're attending in person and just want to grab a cuppa coffee and have a chat with us, please reach ping either of the OKD working group co-chairs Jaime Magiera (ICPSR at University of Michigan Institute for Social Research), or Diane Mueller(Red Hat)</p> <p>Come connect with us to discuss the OKD Road Map, OKD Streams initiative, MVP Release of OKD on CentOS Streams and the latest use cases for OKD, and talk all things open with our team.</p> <p></p>"},{"location":"blog/2022/09/09/an_introduction_to_debugging_okd_release_artifacts/","title":"An introduction to debugging okd release artifacts","text":""},{"location":"blog/2022/09/09/an_introduction_to_debugging_okd_release_artifacts/#an-introduction-to-debugging-okd-release-artifacts","title":"An Introduction to debugging OKD release artifacts","text":"<p>by Denis Moiseev and Michael McCune</p> <p>During the course of installing, operating, and maintaining an OKD cluster it is natural for users to come across strange behaviors and failures that are difficult to understand. As Red Hat engineers working on OpenShift, we have many tools at our disposal to research cluster failures and to report our findings to our colleagues. We would like to share some of our experiences, techniques, and tools with the wider OKD community in the hopes of inspiring others to investigate these areas.</p> <p>As part of our daily activities we spend a significant amount of time investigating bugs, and also failures in our release images and testing systems. As you might imagine, to accomplish this task we use many tools and pieces of tribal knowledge to understand not only the failures themselves, but the complexity of the build and testing infrastructures. As Kubernetes and OpenShift have grown, there has always been an organic growth of tooling and testing that helps to support and drive the development process forward. To fully understand the depths of these processes is to be actively following what is happening with the development cycle. This is not always easy for users who are also focused on delivering high quality service through their clusters.</p> <p>On 2 September, 2022, we had the opportunity to record a video of ourselves diving into the OKD release artifacts to show how we investigate failures in the continuous integration release pipeline. In this video we walk through the process of finding a failing release test, examining the Prow console, and then exploring the results that we find. We explain what these artifacts mean, how to further research failures that are found, and share some other web-based tools that you can use to find similar failures, understand the testing workflow, and ultimately share your findings through a bug report.</p> <p>To accompany the video, here are some of the links that we explore and related content:</p> <ul> <li>www.okd.io/installation/ - the main OKD installation page, where our journey begins</li> <li>amd64.origin.releases.ci.openshift.org/ - OKD releases for AMD64, the place to go for release images and continuous integration reporting</li> <li>docs.prow.k8s.io/docs/overview/architecture/ - an overview of Prow architecture, this is useful to understand how Prow operates</li> <li>github.com/openshift/release - OpenShift and OKD\u2019s Prow configuration, go here to find how the jobs are setup</li> <li>github.com/openshift/origin/ - conformance tests for OpenShift and OKD, this is where many of the Kubernetes tests are located</li> <li>steps.ci.openshift.org/ - CI step registry, useful for discovering how test jobs flow together</li> <li>search.ci.openshift.org/ - CI log search, useful for finding similar test failures</li> <li>docs.ci.openshift.org/docs/ - home of the OpenShift CI docs</li> <li>docs.ci.openshift.org/docs/getting-started/useful-links/ - useful links to various services, also links to talks and presentations</li> <li>docs.ci.openshift.org/docs/how-tos/artifacts/ - explanations of CI artifacts</li> <li>issues.redhat.com - OpenShift/OKD bug reporting system</li> </ul> <p>Finally, if you do find bugs or would like report strange behavior in your clusters, remember to visit issues.redhat.com and use the project OCPBUGS.</p>"},{"location":"blog/2022/12/12/building_the_okd_payload/","title":"Building the OKD payload","text":"<p>Over the last couple of months, we've been busy building a new OKD release on CentOS Stream CoreOS (SCOS), and were able to present it for the OpenShift Commons Detroit 2022.  </p> <p>While some of us created a Tekton pipeline that could build SCOS on a Kind cluster, others were tediously building the OKD payload with Prow, but also creating a Tekton pipeline for building that payload on any OpenShift or OKD cluster.</p> <p>The goal of this effort is to enable and facilitate community collaboration and contributions, giving anybody the ability to do their own payload builds and run tests themselves.</p> <p>This process has been difficult because OpenShift's Prow CI instance is not open to the public, and changes could thus not easily be tested before PR submission. Even after opening a PR, a non-Red Hatter will require a Red Hat engineer to add the <code>/ok-to-test</code> label in order to start Prow testing.</p> <p>With the new Tekton pipelines, we are now providing a straight forward way for anybody to build and test their own changes first (or even create their own Stream entirely), and then present the results to the OKD Working Group, which will then expedite the review process on the PR.</p> <p>In this article, I will shed some light on the building blocks of the OKD on SCOS payload, how it is built, both the Prow way, and the Tekton way:  </p>"},{"location":"blog/2022/12/12/building_the_okd_payload/#whats-the-payload","title":"What's the payload?","text":"<p>Until now, the OKD payload, like the OpenShift payload, was built by the ReleaseController in Prow.</p> <p>The release-controller automatically builds OpenShift release images when new images are created for a given OpenShift release. It detects changes to an image stream, launches a job to build and push the release payload image using <code>oc adm release new</code>, and then runs zero or more ProwJobs against the artifacts generated by the payload.</p> <p>A release image is nothing more than a ClusterVersionOperator image (CVO), with an extra layer containing the <code>release-manifests</code> folder. This folder contains :</p> <ul> <li><code>image-references</code>: a list of all known images with their SHA digest,</li> <li>yaml manifest files for each operator controlled by the CVO.</li> </ul> <p>The list of images that is included in the <code>release-manifests</code> is calculated from the <code>release</code> image stream, taking :</p> <ul> <li>all images with label <code>io.openshift.release.operator=true</code> in that image stream</li> <li>plus any images referenced in the <code>/manifests/image-references</code> file within each of the images with this label.</li> </ul> <p>As you can imagine, the list of images in a release can change from one release to the next, depending on:</p> <ul> <li>new operators being delivered within the OpenShift release</li> <li>existing operators adding or removing an operand image</li> <li>operators previously included that are removed from the payload to be delivered independently, through OLM instead.</li> </ul> <p>In order to list the images contained in a release payload, run this command:</p> <pre><code>oc adm release info ${RELEASE_IMAGE_URL}\n</code></pre> <p>For example:</p> <pre><code>oc adm release info quay.io/okd/scos-release:4.12.0-0.okd-scos-2022-12-02-083740 \n</code></pre> <p>Now that we've established what needs to be built, let's take a deeper look at how the OKD on SCOS payload is built.</p>"},{"location":"blog/2022/12/12/building_the_okd_payload/#building-okdscos-the-prow-way","title":"Building OKD/SCOS the Prow way","text":"<p>The obvious way to build OKD on SCOS is to use Prow - THE Kubernetes-based CI/CD system, which is what builds OCP and OKD on FCOS already today. This is what Kubernetes uses upstream as well. </p> <p>For a new OKD release to land in the releases page, there's a whole bunch of Prow jobs that run. Hang on! It's a long story...</p>"},{"location":"blog/2022/12/12/building_the_okd_payload/#imagestreams","title":"ImageStreams","text":"<p>Let's start by the end , and prepare a new image stream for OKD on SCOS images. This ImageStream (IS) is a placeholder for all images that form the OKD/SCOS payload.</p> <p>For OKD on Fedora CoreOS (OKD/FCOS) it's named <code>okd</code>.For OKD/SCOS, this ImageStream is named <code>okd-scos</code>.  </p> <p>This ImageStream includes all payload images contained in the specific <code>OKD</code> release based on CentOS Stream CoreOS (SCOS)  </p> <p>Among these payload images, we distinguish:</p> <ul> <li>Images that can be shared between OCP and OKD. These are built in Prow and mirrored into the <code>okd-scos</code> ImageStream.  </li> <li>Images that have to be specifically built for OKD/SCOS, which are directly tagged into the <code>okd-scos</code> ImageStream. This is the case for images that are specific to the underlying operating system, or contain RHEL packages. These are: the <code>installer</code> images, the <code>machine-config-operator</code> image, the <code>machine-os-content</code> that includes the base operating system OSTree, as well as the <code>ironic</code> image for provisioning bare-metal nodes, and a few other images.</li> </ul>"},{"location":"blog/2022/12/12/building_the_okd_payload/#triggers-for-building-most-payload-images","title":"Triggers for building most payload images","text":"<p>Now that we've got the recipient Image Stream for the OKD payload images, let's start building some payloads!  </p> <p>Take the Cluster Network Operator for example: For this operator, the same image can be used on OCP CI and OKD releases. Most payload images fit into this case.  </p> <p>For such an image, the build is pretty straight forward. When a PR is filed for a GitHub repository that is part of a release payload:</p> <ul> <li>The Pre-submit jobs run. It essentially builds the image and stores it in an ImageStream in an ephemeral namespace to run tests against several platforms (AWS, GCP, BareMetal, Azure, etc)</li> <li> <p>Once the tests are green and the PR is approved and merges, the Post-submit jobs run. It essentially promotes the built image to the appropriate release-specific ImageStream:</p> <ul> <li>if the PR is for master, images are pushed to the <code>${next-release}</code> ImageStream</li> <li>If the PR is for <code>release-${MAJOR}.${MINOR}</code>, images are pushed to the <code>${MAJOR}.${MINOR}</code> ImageStream</li> </ul> </li> </ul> <p>Next, the OCP release controller which runs at every change to the ImageStream, will mirror all images from the <code>${MAJOR}.${MINOR}</code> ImageStream to the <code>scos-${MAJOR}.${MINOR}</code> ImageStream.</p> <pre><code>graph TD;\n    PR[fa:fa-github PullRequest]-- submitted --&gt;PreSub{{fa:fa-ship PreSubmitJob}};\n    PreSub-- builds --&gt;tis[(temporary IS)];\n    PreSub-- runs --&gt;test{{Platform tests}};\n    test -. against .- tis\n    PR-- merged --&gt;PostSub{{fa:fa-ship PostSubmitJob}};\n    PostSub-- promotes --&gt;isci[(OCP CI IS 4.x)];\n    PostSub -. from .- tis\n    RCtrl{{fa:fa-ship ReleaseController}}-- mirrors --&gt;isokd[(OKD IS scos-4.x)];\n    RCtrl -. from .- isci\n</code></pre> <p>As mentioned before, some of the images are not mirrored, and that brings us to the next section, on building those images that have content (whether code or manifests) specific to OKD.</p>"},{"location":"blog/2022/12/12/building_the_okd_payload/#trigger-for-building-the-okd-specific-payload-images","title":"Trigger for building the OKD-specific payload images","text":"<p>For the OKD-specific images, the CI process is a bit different, as the image is built in the PostSubmit job and then directly promoted to the <code>okd-scos</code> IS, without going through the OCP CI to OKD mirroring step. This is called a variant configuration. You can see this for MachineConfigOperator for example.  </p> <pre><code>graph TD;\n    PR[PullRequest]-- submitted --&gt;PreSubRHCOS{{PreSubmitJob / OCP CI}};\n    PreSubRHCOS:::RHCOS-- builds --&gt;tis[(temporary IS)];\n    PreSubRHCOS-- runs --&gt;testRHCOS{{Platform tests}};\n    testRHCOS:::RHCOS -. against .- tis\n\n    PR[PullRequest]-- submitted --&gt;PreSubSCOS{{PreSubmitJob / OKD}};\n    PreSubSCOS:::SCOS-- builds --&gt;tis[(temporary IS)];\n    PreSubSCOS-- runs --&gt;testSCOS{{Platform tests}};\n    testSCOS:::SCOS -. against .- tis\n\n    PR-- merged --&gt;PostSubRHCOS{{PostSubmitJob / OCP CI}};\n    PostSubRHCOS:::RHCOS-- promotes --&gt;isocp[(OCP CI IS 4.x)];\n    PostSubRHCOS -. from .- tis\n\n    PR-- merged --&gt;PostSubSCOS{{PostSubmitJob / OKD}};\n    PostSubSCOS:::SCOS-- promotes --&gt;isokd[(OKD IS scos-4.x)]:::SCOS;\n    PostSubSCOS -. from .- tis\n\n     classDef RHCOS fill:;\n     classDef SCOS fill:#1e77aa;</code></pre> <p>The built images land directly in the <code>scos-${MAJOR}-${MINOR}</code> ImageStream.</p> <p>That is why there's no need for OCP's CI release controller to mirror these images from the CI ImageStream: During the PostSubmit phase, images are already getting built in parallel for OCP, OKD/FCOS and OKD/SCOS and pushed, respectively to <code>ocp/$MAJOR.$MINOR</code>, <code>origin/$MAJOR.$MINOR</code>, <code>origin/scos-$MAJOR.$MINOR</code></p>"},{"location":"blog/2022/12/12/building_the_okd_payload/#okd-release-builds","title":"OKD release builds","text":"<p>Now the ImageStream <code>scos-$MAJOR.$MINOR</code> is getting populated by payload images. With every new image tag, the release controller for OKD/SCOS will  build a release image.</p> <p>The ReleaseController ensures that OpenShift update payload images (aka release images) are created whenever an ImageStream representing the images in a release is updated.  </p> <p>Thanks to the annotation <code>release.openshift.io/config</code> on the <code>scos-${MAJOR}-{MINOR}</code> ImageStream, the controller will:</p> <ol> <li>Create a tag in the <code>scos-${MAJOR}-{MINOR}</code> ImageStream that uses the release name + current timestamp.</li> <li>Mirror all of the tags in the input ImageStream so that they can't be pruned.</li> <li>Launch a job in the job namespace to invoke <code>oc adm release new</code> from the mirror pointing to the release tag we created in step 1.</li> <li>If the job succeeds in pushing the tag, it sets an annotation on that tag <code>release.openshift.io/phase = \"Ready\"</code>, indicating that the release can be used by other steps. And that's how a new release appears in `https://origin-release.ci.openshift.org/#4.13.0-0.okd-scos</li> <li>The release state switches to \"Verified\" when the verification end-to-end test job  succeeds.</li> </ol>"},{"location":"blog/2022/12/12/building_the_okd_payload/#building-the-tekton-way","title":"Building the Tekton way","text":"<p>Building with Prow has the advantage of being driven by new code being pushed to payload components, thus building fresh releases as the code of github.com/openshift evolves.</p> <p>The problem is that Prow, along with all the clusters involved with it, the ImageStreams, etc. are not accessible to the OKD community outside of RedHat. Also, users might be interested in building custom OKD payload, in their own environment, to experiment exchanging components for example.</p> <p>To remove this impediment, the OKD team has been working on the OKD Payload pipeline based on Tekton.</p> <p>Building OKD payloads with Tekton can be done by cloning the okd-payload-pipeline repository. One extra advantage of this repository is the ability to see the list of components that form the OKD payload: In fact, the list under buildconfigs corresponds to the images in the OKD final payload. This list is currently manually synced with the list of OCP images on each release.  </p> <p>The pipeline is fairly simple. Take the build-from-scratch.yaml for example. It has 3 main tasks:</p> <ul> <li>Build the base image and the builder image, with which all the payload images will be built<ul> <li>The builder image is a CentOS Stream 9 container image that includes all the dependencies needed to build payload components and is used as the build environment for them</li> <li>The built binaries are then layered onto a CentOS Stream 9 base image, creating a payload component image.</li> <li>The base image is shared across all the images in the release payload</li> </ul> </li> <li>Build payload images in batches (starting with the ones that don't have any dependencies)</li> <li>Finally, as all OKD payload component images are in the image stream, the OKD release image is in turn built, using the <code>oc adm release new</code> command.</li> </ul> <pre><code>graph LR\n    subgraph pipeline\n    direction TB\n\n    pipelineRun{{fa:fa-gears PipelineRun}}-- starts --&gt; taskRunBuilderBase{{fa:fa-gears TaskRun - BatchBuild}}\n\n    taskRunBuilderBase -- runBefore --&gt; taskRun1{{fa:fa-gears BatchBuild#1}};\n    taskRun1 -- runBefore --&gt; taskRun2{{fa:fa-gears BatchBuild#2}};\n    taskRun2 -- runBefore --&gt; payloadRelease{{fa:fa-gears BuildRelease}}\n\n    taskRunBuilderBase -- builds --&gt; base(fa:fa-cube base)\n    taskRunBuilderBase -- builds --&gt; builder(fa:fa-cube builder)\n\n    taskRun1 -- builds --&gt; payloadImage101(fa:fa-cube Payload-101)\n    taskRun1 -- builds --&gt; payloadImage102(fa:fa-cube Payload-102) \n\n    taskRun2 -- builds --&gt; payloadImage201(fa:fa-cube Payload-201) \n    taskRun2 -- builds --&gt; payloadImage202(fa:fa-cube Payload-201)\n\n    payloadRelease -- builds --&gt; payloadReleaseImage(fa:fa-cube Payload-Release)\n\n    base                 -- pushed --&gt; IS[(IS origin)]\n    builder              -- pushed --&gt; IS\n    payloadImage101      -- pushed --&gt; IS\n    payloadImage102      -- pushed --&gt; IS\n    payloadImage201      -- pushed --&gt; IS\n    payloadImage202      -- pushed --&gt; IS\n    payloadReleaseImage  -- pushed --&gt;quay[(quay.io)];\n    quay &lt;-.- IS\n\n    end\n</code></pre>"},{"location":"blog/2022/12/12/building_the_okd_payload/#triggers","title":"Triggers","text":"<p>For the moment, this pipeline has no triggers. It can be executed manually when needed. We are planning to automatically trigger the pipeline on a daily cadence.</p>"},{"location":"blog/2022/12/12/building_the_okd_payload/#batch-build-task","title":"Batch Build Task","text":"<p>With a set of buildConfigs passed in the parameters, this task relies on an openshift <code>oc</code> image containing the client binary and loops on the list of build configs with a <code>oc start-build</code>, and waits for all the builds to complete.  </p>"},{"location":"blog/2022/12/12/building_the_okd_payload/#new-release-task","title":"New Release Task","text":"<p>This task simply uses an OpenShift client image to call <code>oc adm release new</code> which creates the release image from the image stream <code>release</code> (on the OKD/OpenShift cluster where this Tekton pipeline is running), and mirroring the release image, and all the payload component images to a registry configured in its parameters.  </p>"},{"location":"blog/2022/12/12/building_the_okd_payload/#buildconfigs","title":"BuildConfigs","text":"<p>As explained above, the OKD payload Tekton pipeline heavily relies on the buildconfigs. This folder contains one buildconfig yaml file for each image included in the release payload.  </p> <p>Each build config simply uses a builder image to build the operator binary, invoking the correct Dockerfile in the operator repository. Then, the binary is copied as a layer on top of an OKD base image, which is built in the preparatory task of the pipeline.  </p> <p>This process currently uses the OpenShift Builds API. We are planning to move these builds to the Shipwright Builds API in order to enable builds outside of OCP or OKD clusters.</p>"},{"location":"blog/2022/12/12/building_the_okd_payload/#updating-build-configs","title":"Updating build configs","text":"<p>Upon deploying the Tekton OKD Payload pipeline on an OKD (or OpenShift) cluster, Kustomize is used in order to :</p> <ul> <li>patch the BuildConfig files, adding TAGS to the build arguments according to the type of payload we want to build (based on FCOS, SCOS or any other custom stream)</li> <li>patch the BuildConfig files, replacing the builder image references to the non-public <code>registry.ci.openshift.org/ocp/builder</code> in the payload component's Dockerfiles with the builder image reference from the local image stream</li> <li>setting resource requests and limits if needed</li> </ul>"},{"location":"blog/2022/12/12/building_the_okd_payload/#preparing-for-a-new-release","title":"Preparing for a new release","text":"<p>The procedure to prepare a new release is still a work in progress at the time of writing.</p> <p>To build a new release, each BuildConfig file should be updated with the git branch corresponding to that release. In the future, the branch can be passed along as a kustomization, or in the parameters of the pipeline.  </p> <p>The list of images from a new OCP release (obtained through <code>oc adm release info</code>) must now be synced with the BuildConfigs present here:</p> <ul> <li>For any new image, a new BuildConfig file must be added</li> <li>For any image removed from the OCP release, the corresponding BuildConfig file must be removed.</li> </ul>"},{"location":"blog/2022/12/12/building_the_okd_payload/#take-away","title":"Take away","text":""},{"location":"blog/2022/12/12/building_the_okd_payload/#what-are-our-next-steps","title":"What are our next steps?","text":"<p>In the coming weeks and months, you can expect lots of changes, especially as the OKD community is picking up usage of OKD/SCOS, and doing their own Tekton Pipeline runs:</p> <ul> <li>Work to automate the OKD release procedure is progress by automatically verifying payload image signatures, signing the release, and tagging it on GitHub.</li> </ul> <p>The goal is to deliver a new OKD/SCOS on a sprint (3-weekly) basis, and to provide both the OCP teams and the OKD community with a fresh release to test much earlier than previously with the OCP release cadence.</p> <ul> <li>For the moment, OKD/SCOS releases are only verified on AWS. To gain more confidence in our release payloads, we will expand the test matrix to other platforms such as GCP, vSphere and Baremetal</li> <li>Enable GitOps on the Tekton pipeline repository, so that changes to the pipeline are automatically deployed on OperateFirst for the community to use the latest and greatest.</li> <li>The OKD Working Group will be collaborating with the Mass Open Cloud to allow for deployments of test clusters on their baremetal infrastructure.</li> <li>The OKD Working Group will be publishing the Tekton Tasks and Pipelines used to build the SCOS Operating System as well as the OKD payload to Tekton Hub and Artifact Hub</li> <li>The OKD operators Tekton pipeline will be used for community builds of optional OLM operators. A first OKD operator has already been built with it, and other operators are to follow, starting with the Pipelines operator, which has long been an ask by the community</li> <li>Additionally, we are working on multi-arch releases for both OKD/SCOS and OKD/FCOS</li> </ul>"},{"location":"blog/2022/12/12/building_the_okd_payload/#opened-perspectives","title":"Opened perspectives","text":"<p>Although in the near future the OKD team will still rely on Prow to build the payload images, the Tekton pipeline will start getting used to finalize the release.</p> <p>In addition, this Tekton pipeline has opened up new perspectives, even for OCP teams.</p> <p>One such example is for the Openshift API team who would like to use the Tekton pipeline to test API changes by building all components that are dependent of the OpenShift API from that PR, create an OKD release and test it thus getting extra quick feedback on impacts of the API changes on the OKD (and later OCP) releases.</p> <p>Another example is the possibility to build images on other platforms than Openshift or OKD platform, replacing build configs with Shipwright, or why not <code>docker build</code>...</p> <p>Whatever your favorite flavor is, we are looking forward to seeing the pipelines in action, increasing collaboration and improving our community feedback loop.</p>"},{"location":"blog/2021/05/04/from_okd_to_openshift_in_3_years_-_talk_by_josef_meier_rohde__schwarz_from_openshift_commons_gathering_at_kubecon/","title":"From OKD to OpenShift in 3 Years - talk by Josef Meier (Rohde &amp; Schwarz) from OpenShift Commons Gathering at Kubecon","text":"<p>On May 4th 2020, OKD-Working Group member Josef Meier gave a wonderful talk about Rohde &amp; Schwarz's Journey to OpenShift 4 from OKD to ARO (Azure Red Hat OpenShift) and discussed benefits of participating in the OKD Working Group!</p> <p>Join the OKD-Working Group and add your voice to the conversation!</p>"},{"location":"blog/2021/03/19/please_avoid_using_fcos_332021030131_for_new_okd_installs/","title":"Please avoid using FCOS 33.20210301.3.1 for new OKD installs","text":"<p>Due to several issues ([1] and [2]) fresh installations using FCOS 33.20210301.3.1 would fail. The fix is coming in Podman 3.1.0.</p> <p>Please use an older stable release - 33.20210217.3.0 - as a starting point instead. See download links at https://builds.coreos.fedoraproject.org/browser?stream=stable (might need some scrolling),</p> <p>Note, that only fresh installs are affected. Also, you won't be left with outdated packages, as OKD does update themselves to latest stable FCOS content during installation/update.</p> <ol> <li>https://bugzilla.redhat.com/show_bug.cgi?id=1936927</li> <li>https://github.com/openshift/okd/issues/566</li> </ol> <p>-- Cheers, Vadim</p>"},{"location":"blog/2021/03/22/recap_okd_testing_and_deployment_workshop_-_videos_and_additional_resources/","title":"Recap OKD Testing and Deployment Workshop - Videos and Additional Resources","text":"<p>On March 20th, OKD-Working Group hosted a day-long event to bring together people from the OKD and related Open Source project communities to collaborate on testing and documentation of the OKD 4 install and upgrade processes for the various platforms that people are deploying OKD 4 on as well to identify any issues with the current documentation for these processes and triage them together.</p>"},{"location":"blog/2021/03/22/recap_okd_testing_and_deployment_workshop_-_videos_and_additional_resources/#the-okd-working-group-held-a-virtual-community-hosted-workshop-on-testing-and-deploying-okd4-on-march-20th","title":"The OKD Working Group held a virtual community-hosted workshop on testing and deploying OKD4 on March 20th","text":"<p>The day started with all attendees together in the \u2018main stage\u2019 area for 2 hours where community members gave an short welcome along with the following four presentations:</p> <ul> <li>What is OKD4 (with a Release Update) - by Charro Gruver (Red Hat)</li> <li>Walk Thru of the OKD Release and Build Processes - Vadim Rutkovsky (Red Hat)</li> <li>Walk Thru of the OKD Deployment and Configuration Guides - Jamie Magiera (UMich)</li> <li>Best Practices such as DNS/DHCP server and Load Balancer Configuration) - Josef Meier (Rohde and Schwarz)</li> </ul> <p>Then attendees then broke into track sessions specific to the deployment target platforms for deep dive demos with live Q/A, answered as many questions as possible about that specific deployment target's configurations, attempted to identify any missing pieces in the documentation and triage the documentation as we went along.  </p> <p>The 4 track break-out rooms set-up for 2.5 hours of deployment walk throughs and Q/A with session leads:</p> <ul> <li>Automated Installation on vSphere UPI - lead by Jaime Magiera (UMich) and Josef Meier (Rohde &amp; Schwarz)</li> <li>Bare Metal/UPI - lead by Andrew Sullivan (Red Hat) and Jason Pittman (Red Hat)</li> <li>Single Node Cluster - lead by Charro Gruver (Red Hat) and Bruce Link (BCIT)</li> <li>Home Lab Setup - lead by Craig Robinson (Red Hat), Sri Ramanujam (Datto) and Vadim Rutkovsky(Red Hat)</li> </ul> <p>Our goal was to triage our existing community documentation, identify any short comings and encourage your participation in the OKD-Working Group's testing of the installation and upgrade processes for each OKD release.</p>"},{"location":"blog/2021/03/22/recap_okd_testing_and_deployment_workshop_-_videos_and_additional_resources/#resources","title":"Resources:","text":"<ul> <li>Link to Playlist</li> <li>OKD Workshop Slides - Charro Gruver</li> <li>DNS DHCP Load Balancer Diagram - Josef Meier</li> </ul>"},{"location":"blog/2023/07/18/state_of_affairs_in_okd_cicd/","title":"State of affairs in OKD CI/CD","text":"<p>OKD is a community distribution of Kubernetes which is built from Red Hat OpenShift components on top of Fedora CoreOS (FCOS) and recently also CentOS Stream CoreOS (SCOS). The OKD variant based on Fedora CoreOS is called OKD or OKD/FCOS. The SCOS variant is often referred to as OKD/SCOS.</p> <p>The previous blog posts introduced OKD Streams and its new Tekton pipelines for building OKD/FCOS and OKD/SCOS releases. This blog post gives an overview of the current build and release processes for FCOS, SCOS and OKD. It outlines OKD's dependency on OpenShift, an remnant from the past when its Origin predecessor was a downstream rebuild of OpenShift 3, and concludes with an outlook on how OKD Streams will help users, developers and partners to experiment with future OpenShift.</p>"},{"location":"blog/2023/07/18/state_of_affairs_in_okd_cicd/#fedora-coreos-and-centos-stream-coreos","title":"Fedora CoreOS and CentOS Stream CoreOS","text":"<p>Fedora CoreOS is built with a Jenkins pipeline running in Fedora's infrastructure and is being maintained by the Fedora CoreOS team.</p> <p>CentOS Stream CoreOS is built with a Tekton pipeline running in a OpenShift cluster on MOC's infrastructure and pushed to <code>quay.io/okd/centos-stream-coreos-9</code>. The SCOS build pipeline is owned and maintained by the OpenShift OKD Streams team and SCOS builds are being imported from <code>quay.io</code> into OpenShift CI as <code>ImageStream</code>s.</p>"},{"location":"blog/2023/07/18/state_of_affairs_in_okd_cicd/#openshift-payload-components","title":"OpenShift payload components","text":"<p>At the time of writing, most payload components for OKD/FCOS and OKD/SCOS get mirrored from OCP CI releases. OpenShift CI (Prow and ci-operator) periodically builds OCP images, e.g. for OVN-Kubernetes. OpenShift's <code>release-controller</code> detects changes to image streams, caused by recently built images, then builds and tests a OCP release image. When such an release image passes all non-optional tests (also see release gating docs), the release image and other payload components are mirrored to <code>origin</code> namespaces on <code>quay.io</code> (release gating is subject to change). For example, at most every 3 hours a OCP 4.14 release image will be deployed (and upgraded) on AWS and GCP and afterwards tested with OpenShift's conformance test suite. When it passes the non-optional tests the release image and its dependencies will be mirrored to <code>quay.io/origin</code> (except for <code>rhel-coreos*</code>, <code>*-installer</code> and some other images). These OCP CI releases are listed with a <code>ci</code> tag at amd64.ocp.releases.ci.openshift.org. Builds and promotions of <code>nightly</code> and <code>stable</code> OCP releases are handled differently (i.e. outside of Prow) by the Automated Release Tooling (ART) team.</p>"},{"location":"blog/2023/07/18/state_of_affairs_in_okd_cicd/#okd-payload-components","title":"OKD payload components","text":"<p>A few payload components are built specifically for OKD though, for example OKD/FCOS' okd-machine-os. Unlike RHCOS and SCOS, okd-machine-os, the operating system running on OKD/FCOS nodes, is layered on top of FCOS (also see CoreOS Layering, OpenShift Layered CoreOS).</p> <p>Note, some payload components have OKD specific configuration in OpenShift CI although the resulting images are not incorporated into OKD release images. For example, OVN-Kubernetes images are built and tested in OpenShift CI to ensure OVN changes do not break OKD.</p>"},{"location":"blog/2023/07/18/state_of_affairs_in_okd_cicd/#okd-releases","title":"OKD releases","text":"<p>When OpenShift's <code>release-controller</code> detects changes to OKD related image streams, either due to updates of FCOS/SCOS, an OKD payload component or due to OCP payload components being mirrored after an OCP CI release promotion, it builds and tests a new OKD release image. When such an OKD release image passes all non-optional tests, the image is tagged as <code>registry.ci.openshift.org/origin/release:4.14</code> etc. This CI release process is similar for OKD/FCOS and OKD/SCOS, e.g. compare these examples for OKD/FCOS 4.14 and with OKD/SCOS 4.14. OKD/FCOS's and OKD/SCOS's CI releases are listed at amd64.origin.releases.ci.openshift.org.</p> <p>Promotions for OKD/FCOS to <code>quay.io/openshift/okd</code> (published at github.com/okd-project/okd) and for OKD/SCOS to <code>quay.io/okd/scos-release</code> (published at github.com/okd-project/okd-scos) are done roughly every 2 to 3 weeks. For OKD/SCOS, OKD's release pipeline is triggered manually once a sprint to promote CI releases to <code>4-scos-{next,stable}</code>.</p>"},{"location":"blog/2023/07/18/state_of_affairs_in_okd_cicd/#okd-streams-and-customizable-tekton-pipelines","title":"OKD Streams and customizable Tekton pipelines","text":"<p>However, the OKD project is currently shifting its focus from doing downstream rebuilds of OCP to OKD Streams. As part of this strategic repositioning, OKD offers Argo CD workflows and Tekton pipelines to build CentOS Stream CoreOS (SCOS) (with okd-coreos-pipeline), to build OKD/SCOS (with okd-payload-pipeline) and to build operators (with okd-operator-pipeline). The OKD Streams pipelines have been created to improve the RHEL9 readiness signal for Red Hat OpenShift. It allows developers to build and compose different tasks and pipelines to easily experiment with OpenShift and related technologies. Both <code>okd-coreos-pipeline</code> and <code>okd-operator-pipeline</code> are already used in OKD's CI/CD and in the future <code>okd-payload-pipeline</code> might supersede OCP CI for building OKD payload components and mirroring OCP payload components.</p>"},{"location":"blog/2021/03/16/save_the_date_okd_testing_and_deployment_workshop_march_20_register_now/","title":"Save The Date! OKD Testing and Deployment Workshop (March 20) Register Now!","text":""},{"location":"blog/2021/03/16/save_the_date_okd_testing_and_deployment_workshop_march_20_register_now/#the-okd-working-group-is-hosting-a-virtual-workshop-on-testing-and-deploying-okd4","title":"The OKD Working Group is hosting a virtual workshop on testing and deploying OKD4","text":"<p>On March 20th, OKD-Working Group is hosting a one day event to bring together people from the OKD and related Open Source project communities to collaborate on testing and documentation of the OKD 4 install and upgrade processes for the various platforms that people are deploying OKD 4 on as well to identify any issues with the current documentation for these processes and triage them together.</p> <p>The day will start with all attendees together in the \u2018main stage\u2019 area for 2 hours where we will give an short welcome and describe the logistics for the day, give a brief introduction to OKD4 itself then walk thru a install deployment to vSphere using UPI approach along with a few other more universal best practices such as DNS/DHCP server configuration) that apply to all deployment targets.</p> <p>Then we will break into tracks specific to the deployment target platforms for deep dive demos with Q/A, try and answer any questions you have about your specific deployment target's configurations, identify any missing pieces in the documentation and triage the documentation as we go.  </p> <p>There will be 4 track break-out rooms set-up for 3 hours of deployment walk throughs and Q/A with session leads:</p> <ul> <li>vSphere/UPI - lead by Jaime Magiera (UMich) and Josef Meier (Rohde &amp; Schwarz)</li> <li>Bare Metal/UPI - lead by Andrew Sullivan (Red Hat) and Jason Pittman (Red Hat)</li> <li>Single Node Cluster - lead by Charro Gruver (Red Hat) and Bruce Link (BCIT)</li> <li>Home Lab Setup - lead by Craig Robinson (Red Hat) and Sri Ramanujam (Datto)</li> </ul> <p>Our goal is to triage our existing community documentation, identify any short comings and encourage your participation in the OKD-Working Group's testing of the installation and upgrade processes for each OKD release.</p> <p>This is community event NOT meant as a substitute for Red Hat technical support.</p> <p>There is no admission or ticket charge for OKD-Working Group events. However, you are required to complete a free hopin.to platform registration and watch the hopin site for updates about registration and schedule updates.</p> <p>We are committed to fostering an open and welcoming environment at our working group meetings and events. We set expectations for inclusive behavior through our code of conduct and media policies, and are prepared to enforce these.</p> <p>You can Register for the workshop here:</p> <p>https://hopin.com/events/okd-testing-and-deployment-workshop</p>"},{"location":"blog/2020/08/31/implementing_an_automated_installation_solution_for_okd_on_vsphere_with_user_provisioned_infrastructure_upi/","title":"Implementing an Automated Installation Solution for OKD on vSphere with User Provisioned Infrastructure (UPI)","text":"<p>It\u2019s possible to completely automate the process of installing OpenShift/OKD on vSphere with User Provisioned Infrastructure by chaining together the various functions of OCT via a wrapper script.</p>"},{"location":"blog/2020/08/31/implementing_an_automated_installation_solution_for_okd_on_vsphere_with_user_provisioned_infrastructure_upi/#steps","title":"Steps","text":"<ol> <li>Deploy the DNS, DHCP, and load balancer infrastructure outlined in the Prerequisites section.</li> <li>Create an install-config.yaml.template file based on the format outlined in the section Sample install-config.yaml file for VMware vSphere of the OKD docs. Do not add a pull secret. The script will query you for one or it will insert a default one if you use the \u2013auto-secret flag.</li> <li>Create a wrapper script that:<ul> <li>Installs the desired FCOS image</li> <li>Downloads the oc and openshift-installer binaries for your desired release version</li> <li>Generates and modifies the ignition files appropriately</li> <li>Builds the cluster nodes</li> <li>Triggers the installation process.</li> </ul> </li> </ol>"},{"location":"blog/2020/08/31/implementing_an_automated_installation_solution_for_okd_on_vsphere_with_user_provisioned_infrastructure_upi/#prerequisites","title":"Prerequisites","text":""},{"location":"blog/2020/08/31/implementing_an_automated_installation_solution_for_okd_on_vsphere_with_user_provisioned_infrastructure_upi/#dns","title":"DNS","text":"<p>1 entry for the bootstrap node of the format bootstrap.[cluster].domain.tld 3 entries for the master nodes of the form master-[n].[cluster].domain.tld An entry for each of the desired worker nodes in the form worker-[n].[cluster].domain.tld 1 entry for the API endpoint in the form api.[cluster].domain.tld 1 entry for the API internal endpoint in the form api-int.[cluster].domain.tld 1 wildcard entry for the Ingress endpoint in the form *.apps.[cluster].domain.tld</p>"},{"location":"blog/2020/08/31/implementing_an_automated_installation_solution_for_okd_on_vsphere_with_user_provisioned_infrastructure_upi/#dhcp","title":"DHCP","text":""},{"location":"blog/2020/08/31/implementing_an_automated_installation_solution_for_okd_on_vsphere_with_user_provisioned_infrastructure_upi/#load-balancer","title":"Load Balancer","text":"<p>vSphere UPI requires the use of a load balancer. There needs to be two pools.</p> <ul> <li>API: This pool should contain your master nodes.</li> <li>Ingress: This pool should contain your worker nodes.</li> </ul>"},{"location":"blog/2020/08/31/implementing_an_automated_installation_solution_for_okd_on_vsphere_with_user_provisioned_infrastructure_upi/#proxy-optional","title":"Proxy (Optional)","text":"<p>If the cluster will sit on a private network, you\u2019ll need a proxy for outgoing traffic, both for the install process and for regular operation. In the case of the former, the installer needs to pull containers from the external registries. In the case of the latter, the proxy is needed when application containers need access to the outside world (e.g. yum installs, external code repositories like gitlab, etc.)</p> <p>The proxy should be configured to accept connections from the IP subnet for your cluster. A simple proxy to use for this purpose is squid</p>"},{"location":"blog/2020/08/31/implementing_an_automated_installation_solution_for_okd_on_vsphere_with_user_provisioned_infrastructure_upi/#wrapper-script","title":"Wrapper Script","text":"<pre><code>#!/bin/bash\n\nmasters_count=3\nworkers_count=2\ntemplate_url=\"https://builds.coreos.fedoraproject.org/prod/streams/testing/builds/33.20210314.2.0/x86_64/fedora-coreos-33.20210314.2.0-vmware.x86_64.ova\"\ntemplate_name=\"fedora-coreos-33.20210201.2.1-vmware.x86_64\"     \nlibrary=\"Linux ISOs\"\ncluster_name=\"mycluster\"\ncluster_folder=\"/MyVSPHERE/vm/Linux/OKD/mycluster\"\nnetwork_name=\"VM Network\"\ninstall_folder=`pwd`\n\n# Import the template\n./oct.sh --import-template --library \"${library}\" --template-url \"${template_url}\"\n\n# Install the desired OKD tools\noct.sh --install-tools --release 4.6\n\n# Launch the prerun to generate and modify the ignition files\noct.sh --prerun --auto-secret\n\n# Deploy the nodes for the cluster with the appropriate ignition data\noct.sh --build --template-name \"${template_name}\" --library \"${library}\" --cluster-name \"${cluster_name}\" --cluster-folder \"${cluster_folder}\" --network-name \"${network_name}\" --installation-folder \"${install_folder}\" --master-node-count ${masters_count} --worker-node-count ${workers_count} \n\n# Turn on the cluster nodes\noct.sh --cluster-power on --cluster-name \"${cluster_name}\"  --master-node-count ${masters_count} --worker-node-count ${workers_count}\n\n# Run the OpenShift installer \nbin/openshift-install --dir=$(pwd) wait-for bootstrap-complete  --log-level=info\n</code></pre>"},{"location":"blog/2020/08/31/implementing_an_automated_installation_solution_for_okd_on_vsphere_with_user_provisioned_infrastructure_upi/#future-updates","title":"Future Updates","text":"<ul> <li>Generating the install-config template</li> <li>Pull directly from FCOS release feed</li> </ul>"},{"location":"blog/2020/08/31/aws_ipi_default_deployment/","title":"AWS IPI Default Deployment","text":"<p>This describes the resources used by OpenShift after performing an installation using the default options for the installer.</p>"},{"location":"blog/2020/08/31/aws_ipi_default_deployment/#infrastructure","title":"Infrastructure","text":""},{"location":"blog/2020/08/31/aws_ipi_default_deployment/#compute","title":"Compute","text":"<ul> <li>3 control plane nodes<ul> <li>instance type <code>m4.xlarge</code>, or <code>m5.xlarge</code> if previous not available in the region</li> </ul> </li> <li>3 compute nodes<ul> <li>instance type <code>m4.large</code>, or <code>m5.large</code> if previous not available in the region</li> </ul> </li> </ul>"},{"location":"blog/2020/08/31/aws_ipi_default_deployment/#networking","title":"Networking","text":"<ul> <li>1 virtual private cloud</li> <li>1 public subnet per availability zone in the region</li> <li>1 private subnet per availability zone in the region</li> <li>1 NAT gateway per availability zone</li> <li>1 elastic IP address per NAT gateway</li> <li>3 elastic load balancers<ul> <li>1 external network load balancer for the master API server</li> <li>1 internal network load balancer for the master API server</li> <li>1 classic load balancer for the router</li> </ul> </li> <li>21 elastic network interfaces, plus 1 interface per availability zone</li> <li>1 virtual private cloud gateway</li> <li>10 distinct security groups</li> </ul>"},{"location":"blog/2020/08/31/aws_ipi_default_deployment/#deployment","title":"Deployment","text":"<p>See the OKD documentation to proceed with deployment</p>"},{"location":"blog/2020/08/31/azure_ipi_default_deployment/","title":"Azure IPI Default Deployment","text":"<p>This describes the resources used by OpenShift after performing an installation using the default options for the installer.</p>"},{"location":"blog/2020/08/31/azure_ipi_default_deployment/#infrastructure","title":"Infrastructure","text":""},{"location":"blog/2020/08/31/azure_ipi_default_deployment/#compute","title":"Compute","text":"<ul> <li>3 control plane nodes<ul> <li>instance type <code>Standard_D8s_v3</code></li> </ul> </li> <li>3 compute nodes<ul> <li>instance type <code>Standard_D4s_v3</code></li> </ul> </li> </ul>"},{"location":"blog/2020/08/31/azure_ipi_default_deployment/#networking","title":"Networking","text":"<ul> <li>1 virtual network (VNet) containing 2 subnets</li> <li>6 network interfaces -3 network load balancers<ul> <li>1 public for compute node access</li> <li>1 private for control plane access</li> <li>1 public for control plane access</li> </ul> </li> <li>2 public IP addresses<ul> <li>1 for the public compute load balancer</li> <li>1 for the public control plane load balancer</li> </ul> </li> <li>7 private IP addresses<ul> <li>1 per control plane node</li> <li>1 per compute node</li> <li>1 for the private control plane load balancer</li> </ul> </li> <li>2 network security groups<ul> <li>1 for control plane allowing traffic on port 6443 from anywhere</li> <li>1 for compute allowing traffic on ports 80 and 443 from the internet</li> </ul> </li> </ul>"},{"location":"blog/2020/08/31/azure_ipi_default_deployment/#deployment","title":"Deployment","text":"<p>See the OKD documentation to proceed with deployment</p>"},{"location":"blog/2020/08/31/gcp_ipi_default_deployment/","title":"GCP IPI Default Deployment","text":"<p>This describes the resources used by OpenShift after performing an installation using the default options for the installer.</p>"},{"location":"blog/2020/08/31/gcp_ipi_default_deployment/#infrastructure","title":"Infrastructure","text":""},{"location":"blog/2020/08/31/gcp_ipi_default_deployment/#compute","title":"Compute","text":"<ul> <li>3 control plane nodes<ul> <li>instance type <code>n1-standard-4</code></li> </ul> </li> <li>3 compute nodes<ul> <li>instance type <code>n1-standard-2</code></li> </ul> </li> <li>1 image</li> </ul>"},{"location":"blog/2020/08/31/gcp_ipi_default_deployment/#networking","title":"Networking","text":"<ul> <li>2 networks</li> <li>2 subnetworks</li> <li>3 static IP addresses</li> <li>1 router</li> <li>2 routes</li> <li>3 target pools</li> <li>10 firewall rules</li> <li>2 forwarding rules</li> <li>3 in-use global IP addresses</li> <li>3 health checks</li> </ul>"},{"location":"blog/2020/08/31/gcp_ipi_default_deployment/#platform","title":"Platform","text":"<ul> <li>5 IAM service accounts</li> </ul>"},{"location":"blog/2020/08/31/gcp_ipi_default_deployment/#deployment","title":"Deployment","text":"<p>See the OKD documentation to proceed with deployment</p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/","title":"Create a Single Node OKD (SNO) Cluster with Assisted Installer","text":"<p>This guide outlines how to run the assisted installer locally then use it to deploy a single node OKD cluster.</p> <p>Warning</p> <p>This guide won't produce a working system as there are issues to be resolved</p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#reference-material","title":"Reference Material","text":"<p>Information from the following sources was used to create this guide:</p> <ul> <li>OpenShift Container Platform Single Node Docs</li> <li>Assisted Installer guide from Vadim</li> <li>Assisted Installer github repo instructions</li> </ul>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#preparation","title":"Preparation","text":""},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#compute-resources","title":"Compute resources","text":"<p>A single Node OKD cluster takes fewer resource than the full cluster deployment, but you still need sufficient CPU and memory resources to run.  You can run on a bare metal system or in a virtual environment.  The minimum resources required are:</p> <ul> <li>vCPU : 8 cores</li> <li>Memory : 16 GB</li> <li>Storage (ideally fast storage, such as SSD) : 120GB</li> </ul> <p>These are the absolute minimum resources needed, depending on the workload(s) you want to run in the cluster you may need additional CPU, memory and storage.</p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#network","title":"Network","text":"<p>Before starting with the assisted installer you need to setup your local network by allocating an IP address for the cluster and ensure DNS resolution is configured and working.  You may also want to configure DHCP to allocate the IP address to a specific MAC address.</p> <p>Info</p> <p>This guide assumes you have a basic working knowledge of networking, including DNS name resolution and DHCP.</p> <p>Todo</p> <p>Do we need a network primer for home users wanting to setup OKD that don't have networking experience?  If there is a good one available online we can link to it or create our own on this site.</p> <p>You should have the following information before you start.  I provide example values, but you need to substitute these for the values for your local environment</p> Item Description Sample value Machine Network The local network 192.168.0.0/24 Default gateway The default gateway for your network 192.168.0.1 DNS Server(s) Comma separated list of DNS servers (must be able to resolve cluster IP address) 192.168.0.2,192.168.0.3 Cluster IP address The IP address allocated to the OKD cluster 192.168.0.59 Base domain The domain in use on your local network lab.home Cluster name The name of the cluster.  This will form part of the URLs to access cluster okd-sno"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#dns","title":"DNS","text":"<p>You must have a DNS server that can resolve the following Fully Qualified Domain Names (FQDN) to the Cluster IP address (192.168.0.59):</p> <ul> <li><code>api.&lt;Cluster Name&gt;.&lt;base domain&gt;</code> - the Kubernetes API.  (api.okd-sno.lab.home)</li> <li><code>api-int.&lt;Cluster Name&gt;.&lt;base domain&gt;</code> - the Internal API.  (api-int.okd-sno.lab.home)</li> <li><code>*.apps.&lt;Cluster Name&gt;.&lt;base domain&gt;</code> - Ingress route.  (*.apps.okd-sno.lab.home)</li> </ul> <p>The last item is a wild card resolution which should resolve all entries ending in <code>apps.&lt;Cluster Name&gt;.&lt;base domain&gt;</code>, so console-openshift-console.apps.okd-sno.lab.home should resolve to the Cluster IP address, 192.168.0.59.</p> <p>Reverse lookup should also be working, so 192.168.0.59 should resolve to the host api.okd-sno.lab.home.</p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#dhcp","title":"DHCP","text":"<p>When your single node cluster runs it is important that it uses the assigned IP address.  You can configure this in the Assisted Installer to setup a static IP address in the cluster configuration or get your DHCP server to assign the IP address to a specific MAC address. </p> <p>You can choose the preferred solution in your network.</p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#running-the-assisted-installer","title":"Running the Assisted Installer","text":"<p>For OpenShift Container Platform RedHat hosts the assisted installer, so you can simply use their hosted version, but for OKD you need to run the installer yourself.  This guide will use podman to run the Assisted Installer locally.</p> <p>The guide uses the most basic setup of the Assisted Installer, but the documentation in the git repository provides additional information to enable secure communication (https) and enable persistent storage.</p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#installing-podman","title":"Installing Podman","text":"<p>You need to have podman available on your machine (where the assisted installer will run).  This is typically your laptop or workstation - not the target system where the OKD single node cluster will run.  Follow the instructions on podman.io to install podman if you don't already have it installed.  You should also ensure you have an up to date version of podman installed.</p> <p>Warning</p> <p>If you ae using podman machine (MacOS and Windows native users) you can't use the <code>podman play kube --configmap</code> option as mentioned in the Assisted Installer git repository, as the --configmap option is not available.  You need to concatenate your config and deployments yaml files into a single yaml file using the <code>---</code> document separator.  Linux users and Windows users running under the Windows Subsystem for Linux have access to the --configmap option.</p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#create-the-configuration-file","title":"Create the configuration file","text":"<p>Before creating the configuration file you need to know the IP of the host running podman that will host the Assisted Installer.  </p> <p>As the OKD cluster boots it will need to communicate with the Assisted Installer, so needs to know the IP address of the assisted installer.  It is important that the OKD cluster host machine is able to communicate with the machine hosting the Assisted Installer and the machine hosting the Assisted Installer is able to run the service and listen for incoming network traffic. (no network firewalls or filters that block this traffic).</p> <p>For this example I will use IP 192.168.0.141 for the system running podman and hosting the Assisted Installer.</p> <p>You need to create the configuration file to run the Assisted Installer in podman.  The base files are available in the assisted installer git repo, but I have modified them and updated them to offer both FCOS (Fedora Core OS) and SCOS (CentOS Stream Core OS) options.</p> <p>Create the file (sno.yaml) - this is the combined file for use with podman machine (will also work with Linux).  You need to change all instances of 192.168.0.141 to the IP address of your system running podman and hosting the Assisted Installer:</p> sno.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config\ndata:\n  ASSISTED_SERVICE_HOST: 192.168.0.141:8090\n  ASSISTED_SERVICE_SCHEME: http\n  AUTH_TYPE: none\n  DB_HOST: 127.0.0.1\n  DB_NAME: installer\n  DB_PASS: admin\n  DB_PORT: \"5432\"\n  DB_USER: admin\n  DEPLOY_TARGET: onprem\n  DISK_ENCRYPTION_SUPPORT: \"true\"\n  DUMMY_IGNITION: \"false\"\n  ENABLE_SINGLE_NODE_DNSMASQ: \"true\"\n  HW_VALIDATOR_REQUIREMENTS: '[{\"version\":\"default\",\"master\":{\"cpu_cores\":4,\"ram_mib\":16384,\"disk_size_gb\":100,\"installation_disk_speed_threshold_ms\":10,\"network_latency_threshold_ms\":100,\"packet_loss_percentage\":0},\"worker\":{\"cpu_cores\":2,\"ram_mib\":8192,\"disk_size_gb\":100,\"installation_disk_speed_threshold_ms\":10,\"network_latency_threshold_ms\":1000,\"packet_loss_percentage\":10},\"sno\":{\"cpu_cores\":8,\"ram_mib\":16384,\"disk_size_gb\":100,\"installation_disk_speed_threshold_ms\":10},\"edge-worker\":{\"cpu_cores\":2,\"ram_mib\":8192,\"disk_size_gb\":15,\"installation_disk_speed_threshold_ms\":10}}]'\n  IMAGE_SERVICE_BASE_URL: http://192.168.0.141:8888\n  IPV6_SUPPORT: \"true\"\n  ISO_IMAGE_TYPE: \"full-iso\"\n  LISTEN_PORT: \"8888\"\n  NTP_DEFAULT_SERVER: \"\"\n  POSTGRESQL_DATABASE: installer\n  POSTGRESQL_PASSWORD: admin\n  POSTGRESQL_USER: admin\n  PUBLIC_CONTAINER_REGISTRIES: 'quay.io'\n  SERVICE_BASE_URL: http://192.168.0.141:8090\n  STORAGE: filesystem\n  OS_IMAGES: '[{\"openshift_version\":\"4.12\",\"cpu_architecture\":\"x86_64\",\"url\":\"https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/37.20221127.3.0/x86_64/fedora-coreos-37.20221127.3.0-live.x86_64.iso\",\"version\":\"37.20221127.3.0\"},{\"openshift_version\":\"4.12-scos\",\"cpu_architecture\":\"x86_64\",\"url\":\"https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/37.20221127.3.0/x86_64/fedora-coreos-37.20221127.3.0-live.x86_64.iso\",\"version\":\"37.20221127.3.0\"}]'\n  RELEASE_IMAGES: '[{\"openshift_version\":\"4.12\",\"cpu_architecture\":\"x86_64\",\"cpu_architectures\":[\"x86_64\"],\"url\":\"quay.io/openshift/okd:4.12.0-0.okd-2023-04-01-051724\",\"version\":\"4.12.0-0.okd-2023-04-01-051724\",\"default\":true},{\"openshift_version\":\"4.12-scos\",\"cpu_architecture\":\"x86_64\",\"cpu_architectures\":[\"x86_64\"],\"url\":\"quay.io/okd/scos-release:4.12.0-0.okd-scos-2023-03-23-213604\",\"version\":\"4.12.0-0.okd-scos-2023-03-23-213604\",\"default\":false}]'\n  ENABLE_UPGRADE_AGENT: \"false\"\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: assisted-installer\n  name: assisted-installer\nspec:\n  containers:\n  - args:\n    - run-postgresql\n    image: quay.io/centos7/postgresql-12-centos7:latest\n    name: db\n    envFrom:\n    - configMapRef:\n        name: config\n  - image: quay.io/edge-infrastructure/assisted-installer-ui:latest\n    name: ui\n    ports:\n    - hostPort: 8080\n    envFrom:\n    - configMapRef:\n        name: config\n  - image: quay.io/edge-infrastructure/assisted-image-service:latest\n    name: image-service\n    ports:\n    - hostPort: 8888\n    envFrom:\n    - configMapRef:\n        name: config\n  - image: quay.io/edge-infrastructure/assisted-service:latest\n    name: service\n    ports:\n    - hostPort: 8090\n    envFrom:\n    - configMapRef:\n        name: config\n  restartPolicy: Never\n</code></pre> <p>You may want to modify this configuration to add https communication and persistent storage using information in the Assisted Installer git repo.</p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#run-the-assisted-installer","title":"Run the Assisted Installer","text":"<p>Once you have your configuration file saved to disk, open a command line, change to the directory containing your sno.yaml file then run the following command:</p> <p>Info</p> <p>On a Mac and native Windows system you need to ensure podman machine is running on your system before running the podman play command:</p> <pre><code>podman machine init\npodman machine start\n</code></pre> <pre><code>podman play kube sno.yaml\n</code></pre> <p>To stop a running Assisted Installer instance run (without the persistence option configured all cluster data within the Assisted Installer will be lost, so make sure you have all credentials downloaded to your local system):</p> <pre><code>podman play kube --down sno.yaml\n</code></pre> <p>Once the Assisted installer is running you can access it on port 8080 (http) on the system hosting podman, <code>http://192.168.0.141:8080</code> (substitute your IP address) or if accessing from the machine hosting the service <code>http://localhost:8080</code></p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#create-a-cluster","title":"Create a cluster","text":"<p>When you have the Assisted Installer running locally you can use it to deploy a cluster.  For a single node cluster follow the steps:</p> <ol> <li>On the Assisted Installer Web UI page click Create Cluster</li> <li> <p>On the Cluster details page enter:</p> <ul> <li>the cluster name (okd-sno)</li> <li>the base domain (lab.home)</li> <li>use the drop down to select the version of OKD you want to install (FCOS or SCOS)</li> <li>x86_64 is the only valid architecture at the time of writing this guide</li> <li>click the Install single node OpenShift (SNO) option</li> <li>enter <code>{\"auths\":{\"fake\":{\"auth\":\"aWQ6cGFzcwo=\"}}}</code> as the pull secret</li> <li>If you configured your DHCP server to serve the correct IP to the targe system MAC address leave the Hosts network configuration set to DHCP only.  If you want to set the target system IP address as part of the install then select Static IP, bridges and bonds</li> <li>leave the encryption option off</li> <li>select Next</li> </ul> <p>If you selected static IP address you will get additional options to define the Network-wide configuration and Host specific configurations</p> <ul> <li>on the Network-wide configuration page enter the network details</li> <li>on the Host specific configurations page enter the interface MAC address on the target host for the cluster and the custer IP address then press Next</li> </ul> </li> <li> <p>On the Operators page leave everything as default and press Next</p> </li> <li> <p>On the Host discovery page click the Add Host button and complete the dialog the appears</p> <ul> <li>Set the Provisioning type to Minimal image file - Provision with virtual media</li> <li>set the SSH public key     !!!Todo         Do we need to explain how to create this?</li> <li>leave the rest of the settings as unchecked unless you need to configure a proxy then select the Generate Discovery ISO then download the ISO by pressing Download Discovery ISO.  Once downloaded you can close the popup dialog, where you should see waiting for hosts...</li> </ul> </li> <li> <p>You should boot your target OKD host using the downloaded ISO file.  During the install the target system will reboot a couple of times, so it is important that the first boot uses the ISO but subsequent boots will use the internal hard disk.</p> <p>Warning</p> <p>All internal storage on the target system will be wiped and used for the cluster</p> <ol> <li>Once the target system has booted from the ISO it will contact the Assisted Installer and then appear on the Assisted Installer Host discovery screen.  After the target system appears and the status moves from Discovering to Ready  On the you can press the next button</li> <li>On the Storage page you can configure the storage to use on the target system.  The default should work, but you may want to modify if your target system contains multiple disks.  Once the storage settings are correct press next</li> <li>On the Networking page you should be able to leave things at the default values.  You may need to wait a short time while the host is initializing ,  When the status changes to Ready then press next</li> <li>On the Review and create page you may need to wait for the preflight checks to complete.  When they are ready you can press Install cluster to start the cluster install.</li> </ol> </li> </ol> <p>You should be able to leave the system to complete.  The target system will reboot twice and then the cluster will be installed and configured.  The Assisted installer screen will show the progress.</p> <p>As the cluster is being installed you will be able to download the kubeconfig file for the cluster.  It is important to download this before stopping the Assisted Installer as by default the Assisted Installer storage does not persist across a shutdown.</p> <p>Once the cluster setup completes you will see the cluster console access details, including the password for the kubeadmin account.  Again, you need to capture this information before stopping the Assisted Installer as the information will be lost if you have not enabled persistence.</p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#issues-to-be-resolved","title":"Issues to be resolved","text":"<p>Currently the generated clusters are not installed correctly, so some work needs to be done to correct the setup instructions or find issues with the Assisted Installer or OKD release files.</p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#scos-issue","title":"SCOS issue","text":"<p>The SCOS installation fails at step 2/7 with error (this doesn't happen with the FCOS image):</p> <p>Host okd-sno: updated status from installing-in-progress to error (Failed - failed executing nsenter [--target 1 --cgroup --mount --ipc --pid -- podman run --net host --pid=host --volume /:/rootfs:rw --volume /usr/bin/rpm-ostree:/usr/bin/rpm-ostree --privileged --entrypoint /usr/bin/machine-config-daemon quay.io/openshift/okd-content@sha256:7986774bbd06f4355567ae05b9b737b437d22dbbc3e0793c343bc7ee2de1ab54 start --node-name localhost --root-mount /rootfs --once-from /opt/install-dir/bootstrap.ign --skip-reboot], Error exit status 255, LastOutput \"Error while ensuring access to kublet config.json pull secrets: symlink /var/lib/kubelet/config.json /run/ostree/auth.json: file exists\")</p>"},{"location":"blog/2023/04/13/create_a_single_node_okd_sno_cluster_with_assisted_installer/#fcos-issue","title":"FCOS issue","text":"<p>The FCOS configuration completes the install but goes from 80% complete with status of Installed and Control Plane Initialization in Finalizing stage to Failed.</p> <p>The cluster operators are not all available</p> NAME VERSION AVAILABLE PROGRESSING DEGRADED SINCE MESSAGE authentication 4.12.0-0.okd-2023-04-01-051724 False True True 71m OAuthServerDeploymentAvailable: no oauth-openshift.openshift-authentication pods available on any node.... baremetal 4.12.0-0.okd-2023-04-01-051724 True False False 61m cloud-controller-manager 4.12.0-0.okd-2023-04-01-051724 True False False 63m cloud-credential 4.12.0-0.okd-2023-04-01-051724 True False False 70m cluster-autoscaler 4.12.0-0.okd-2023-04-01-051724 True False False 60m config-operator 4.12.0-0.okd-2023-04-01-051724 True False False 71m console control-plane-machine-set 4.12.0-0.okd-2023-04-01-051724 True False False 63m csi-snapshot-controller 4.12.0-0.okd-2023-04-01-051724 True False False 70m dns 4.12.0-0.okd-2023-04-01-051724 True False False 67m etcd 4.12.0-0.okd-2023-04-01-051724 True False False 63m image-registry 4.12.0-0.okd-2023-04-01-051724 False True False 55m Available: The registry is removed... ingress 4.12.0-0.okd-2023-04-01-051724 True True True 60m The \"default\" ingress controller reports Degraded=True: DegradedConditions: One or more other status conditions indicate a degraded state: CanaryChecksSucceeding=False (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing) insights 4.12.0-0.okd-2023-04-01-051724 True False False 61m kube-apiserver 4.12.0-0.okd-2023-04-01-051724 True False False 57m kube-controller-manager 4.12.0-0.okd-2023-04-01-051724 True False True 57m GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp 172.30.59.74:9091: connect: connection refused kube-scheduler 4.12.0-0.okd-2023-04-01-051724 True False False 58m kube-storage-version-migrator 4.12.0-0.okd-2023-04-01-051724 True False False 70m machine-api 4.12.0-0.okd-2023-04-01-051724 True False False 60m machine-approver 4.12.0-0.okd-2023-04-01-051724 True False False 63m machine-config 4.12.0-0.okd-2023-04-01-051724 True False False 69m marketplace 4.12.0-0.okd-2023-04-01-051724 True False False 70m monitoring False True True 44m reconciling PrometheusAdapter Deployment failed: updating Deployment object failed: waiting for DeploymentRollout of openshift-monitoring/prometheus-adapter: current generation 2, observed generation 1, waiting for Alertmanager object changes failed: waiting for Alertmanager openshift-monitoring/main: expected 1 replicas, got 0 updated replicas, reconciling Thanos Querier Deployment failed: updating Deployment object failed: waiting for DeploymentRollout of openshift-monitoring/thanos-querier: current generation 1, observed generation 0 network 4.12.0-0.okd-2023-04-01-051724 True False False 71m node-tuning 4.12.0-0.okd-2023-04-01-051724 True False False 59m openshift-apiserver 4.12.0-0.okd-2023-04-01-051724 True False False 57m openshift-controller-manager False True False 71m Available: no pods available on any node. openshift-samples 4.12.0-0.okd-2023-04-01-051724 True False False 56m operator-lifecycle-manager 4.12.0-0.okd-2023-04-01-051724 True False False 61m operator-lifecycle-manager-catalog 4.12.0-0.okd-2023-04-01-051724 True False False 61m operator-lifecycle-manager-packageserver 4.12.0-0.okd-2023-04-01-051724 True False False 57m service-ca 4.12.0-0.okd-2023-04-01-051724 True False False 71m storage 4.12.0-0.okd-2023-04-01-051724 True False False 60m <p>running <code>oc adm must-gather</code> produced the following terminal output:</p> <pre><code>[must-gather      ] OUT Using must-gather plug-in image: quay.io/openshift/okd-content@sha256:5b649183c0c550cdfd9f164a70c46f1e23b9e5a7e5af05fc6836bdd5280fbd79\nWhen opening a support case, bugzilla, or issue please include the following summary data along with any other requested information:\nClusterID: 655c76ee-b76c-4072-8fba-c136dcd753f7\nClusterVersion: Installing \"4.12.0-0.okd-2023-04-01-051724\" for 2 hours: Unable to apply 4.12.0-0.okd-2023-04-01-051724: some cluster operators are not available\nClusterOperators:\n    clusteroperator/authentication is not available (OAuthServerDeploymentAvailable: no oauth-openshift.openshift-authentication pods available on any node.\nOAuthServerRouteEndpointAccessibleControllerAvailable: Get \"https://oauth-openshift.apps.okd-sno.lab.home/healthz\": EOF\nOAuthServerServiceEndpointAccessibleControllerAvailable: Get \"https://172.30.32.70:443/healthz\": dial tcp 172.30.32.70:443: connect: connection refused\nOAuthServerServiceEndpointsEndpointAccessibleControllerAvailable: endpoints \"oauth-openshift\" not found) because IngressStateEndpointsDegraded: No subsets found for the endpoints of oauth-server\nOAuthServerDeploymentDegraded: 1 of 1 requested instances are unavailable for oauth-openshift.openshift-authentication (no pods found with labels \"app=oauth-openshift,oauth-openshift-anti-affinity=true\")\nOAuthServerRouteEndpointAccessibleControllerDegraded: Get \"https://oauth-openshift.apps.okd-sno.lab.home/healthz\": EOF\nOAuthServerServiceEndpointAccessibleControllerDegraded: Get \"https://172.30.32.70:443/healthz\": dial tcp 172.30.32.70:443: connect: connection refused\nOAuthServerServiceEndpointsEndpointAccessibleControllerDegraded: oauth service endpoints are not ready\n    clusteroperator/console is not available (&lt;missing&gt;) because &lt;missing&gt;\n    clusteroperator/image-registry is not available (Available: The registry is removed\nNodeCADaemonAvailable: The daemon set node-ca does not have available replicas\nImagePrunerAvailable: Pruner CronJob has been created) because Degraded: The registry is removed\n    clusteroperator/ingress is degraded because The \"default\" ingress controller reports Degraded=True: DegradedConditions: One or more other status conditions indicate a degraded state: DeploymentReplicasAllAvailable=False (DeploymentReplicasNotAvailable: 0/1 of replicas are available), CanaryChecksSucceeding=False (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing)\n    clusteroperator/kube-controller-manager is degraded because GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp 172.30.59.74:9091: connect: connection refused\n    clusteroperator/monitoring is not available (reconciling PrometheusAdapter Deployment failed: updating Deployment object failed: waiting for DeploymentRollout of openshift-monitoring/prometheus-adapter: current generation 2, observed generation 1, waiting for Alertmanager object changes failed: waiting for Alertmanager openshift-monitoring/main: expected 1 replicas, got 0 updated replicas, reconciling Thanos Querier Deployment failed: updating Deployment object failed: waiting for DeploymentRollout of openshift-monitoring/thanos-querier: current generation 1, observed generation 0) because reconciling PrometheusAdapter Deployment failed: updating Deployment object failed: waiting for DeploymentRollout of openshift-monitoring/prometheus-adapter: current generation 2, observed generation 1, waiting for Alertmanager object changes failed: waiting for Alertmanager openshift-monitoring/main: expected 1 replicas, got 0 updated replicas, reconciling Thanos Querier Deployment failed: updating Deployment object failed: waiting for DeploymentRollout of openshift-monitoring/thanos-querier: current generation 1, observed generation 0\n    clusteroperator/openshift-controller-manager is not available (Available: no pods available on any node.) because All is well\n\n\n[must-gather      ] OUT namespace/openshift-must-gather-rn52t created\n[must-gather      ] OUT clusterrolebinding.rbac.authorization.k8s.io/must-gather-9twf4 created\n[must-gather      ] OUT namespace/openshift-must-gather-rn52t deleted\n[must-gather      ] OUT clusterrolebinding.rbac.authorization.k8s.io/must-gather-9twf4 deleted\n\n\nError running must-gather collection:\n    pods \"must-gather-\" is forbidden: error looking up service account openshift-must-gather-rn52t/default: serviceaccount \"default\" not found\n\nFalling back to `oc adm inspect clusteroperators.v1.config.openshift.io` to collect basic cluster information.\nGathering data for ns/openshift-config...\nGathering data for ns/openshift-config-managed...\nGathering data for ns/openshift-authentication...\nGathering data for ns/openshift-authentication-operator...\nGathering data for ns/openshift-ingress...\nGathering data for ns/openshift-oauth-apiserver...\nGathering data for ns/openshift-machine-api...\nGathering data for ns/openshift-cloud-controller-manager-operator...\nGathering data for ns/openshift-cloud-controller-manager...\nGathering data for ns/openshift-cloud-credential-operator...\nGathering data for ns/openshift-config-operator...\nGathering data for ns/openshift-cluster-storage-operator...\nGathering data for ns/openshift-dns-operator...\nGathering data for ns/openshift-dns...\nGathering data for ns/openshift-etcd-operator...\nGathering data for ns/openshift-etcd...\nGathering data for ns/openshift-image-registry...\nGathering data for ns/openshift-ingress-operator...\nGathering data for ns/openshift-ingress-canary...\nGathering data for ns/openshift-insights...\nGathering data for ns/openshift-kube-apiserver-operator...\nGathering data for ns/openshift-kube-apiserver...\nGathering data for ns/openshift-kube-controller-manager...\nGathering data for ns/openshift-kube-controller-manager-operator...\nGathering data for ns/kube-system...\nGathering data for ns/openshift-kube-scheduler...\nGathering data for ns/openshift-kube-scheduler-operator...\nGathering data for ns/openshift-kube-storage-version-migrator...\nGathering data for ns/openshift-kube-storage-version-migrator-operator...\nGathering data for ns/openshift-cluster-machine-approver...\nGathering data for ns/openshift-machine-config-operator...\nGathering data for ns/openshift-kni-infra...\nGathering data for ns/openshift-openstack-infra...\nGathering data for ns/openshift-ovirt-infra...\nGathering data for ns/openshift-vsphere-infra...\nGathering data for ns/openshift-nutanix-infra...\nGathering data for ns/openshift-marketplace...\nGathering data for ns/openshift-monitoring...\nGathering data for ns/openshift-user-workload-monitoring...\nGathering data for ns/openshift-multus...\nGathering data for ns/openshift-ovn-kubernetes...\nGathering data for ns/openshift-host-network...\nGathering data for ns/openshift-network-diagnostics...\nGathering data for ns/openshift-network-operator...\nGathering data for ns/openshift-cloud-network-config-controller...\nGathering data for ns/openshift-cluster-node-tuning-operator...\nGathering data for ns/openshift-apiserver-operator...\nGathering data for ns/openshift-apiserver...\nGathering data for ns/openshift-controller-manager-operator...\nGathering data for ns/openshift-controller-manager...\nGathering data for ns/openshift-route-controller-manager...\nGathering data for ns/openshift-cluster-samples-operator...\nGathering data for ns/openshift-operator-lifecycle-manager...\nGathering data for ns/openshift-service-ca-operator...\nGathering data for ns/openshift-service-ca...\nGathering data for ns/openshift-cluster-csi-drivers...\nWrote inspect data to must-gather.local.5575179556041727039/inspect.local.62720609973566482.\nerror running backup collection: errors occurred while gathering data:\n    [skipping gathering clusterroles.rbac.authorization.k8s.io/system:registry due to error: clusterroles.rbac.authorization.k8s.io \"system:registry\" not found, skipping gathering clusterrolebindings.rbac.authorization.k8s.io/registry-registry-role due to error: clusterrolebindings.rbac.authorization.k8s.io \"registry-registry-role\" not found, skipping gathering podnetworkconnectivitychecks.controlplane.operator.openshift.io due to error: the server doesn't have a resource type \"podnetworkconnectivitychecks\", skipping gathering endpoints/host-etcd-2 due to error: endpoints \"host-etcd-2\" not found, skipping gathering sharedconfigmaps.sharedresource.openshift.io due to error: the server doesn't have a resource type \"sharedconfigmaps\", skipping gathering sharedsecrets.sharedresource.openshift.io due to error: the server doesn't have a resource type \"sharedsecrets\"]\n\n\nReprinting Cluster State:\nWhen opening a support case, bugzilla, or issue please include the following summary data along with any other requested information:\nClusterID: 655c76ee-b76c-4072-8fba-c136dcd753f7\nClusterVersion: Installing \"4.12.0-0.okd-2023-04-01-051724\" for 2 hours: Unable to apply 4.12.0-0.okd-2023-04-01-051724: some cluster operators are not available\nClusterOperators:\n    clusteroperator/authentication is not available (OAuthServerDeploymentAvailable: no oauth-openshift.openshift-authentication pods available on any node.\nOAuthServerRouteEndpointAccessibleControllerAvailable: Get \"https://oauth-openshift.apps.okd-sno.lab.home/healthz\": EOF\nOAuthServerServiceEndpointAccessibleControllerAvailable: Get \"https://172.30.32.70:443/healthz\": dial tcp 172.30.32.70:443: connect: connection refused\nOAuthServerServiceEndpointsEndpointAccessibleControllerAvailable: endpoints \"oauth-openshift\" not found) because IngressStateEndpointsDegraded: No subsets found for the endpoints of oauth-server\nOAuthServerDeploymentDegraded: 1 of 1 requested instances are unavailable for oauth-openshift.openshift-authentication (no pods found with labels \"app=oauth-openshift,oauth-openshift-anti-affinity=true\")\nOAuthServerRouteEndpointAccessibleControllerDegraded: Get \"https://oauth-openshift.apps.okd-sno.lab.home/healthz\": EOF\nOAuthServerServiceEndpointAccessibleControllerDegraded: Get \"https://172.30.32.70:443/healthz\": dial tcp 172.30.32.70:443: connect: connection refused\nOAuthServerServiceEndpointsEndpointAccessibleControllerDegraded: oauth service endpoints are not ready\n    clusteroperator/console is not available (&lt;missing&gt;) because &lt;missing&gt;\n    clusteroperator/image-registry is not available (Available: The registry is removed\nNodeCADaemonAvailable: The daemon set node-ca does not have available replicas\nImagePrunerAvailable: Pruner CronJob has been created) because Degraded: The registry is removed\n    clusteroperator/ingress is degraded because The \"default\" ingress controller reports Degraded=True: DegradedConditions: One or more other status conditions indicate a degraded state: DeploymentReplicasAllAvailable=False (DeploymentReplicasNotAvailable: 0/1 of replicas are available), CanaryChecksSucceeding=False (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing)\n    clusteroperator/kube-controller-manager is degraded because GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp 172.30.59.74:9091: connect: connection refused\n    clusteroperator/monitoring is not available (reconciling PrometheusAdapter Deployment failed: updating Deployment object failed: waiting for DeploymentRollout of openshift-monitoring/prometheus-adapter: current generation 2, observed generation 1, waiting for Alertmanager object changes failed: waiting for Alertmanager openshift-monitoring/main: expected 1 replicas, got 0 updated replicas, reconciling Thanos Querier Deployment failed: updating Deployment object failed: waiting for DeploymentRollout of openshift-monitoring/thanos-querier: current generation 1, observed generation 0) because reconciling PrometheusAdapter Deployment failed: updating Deployment object failed: waiting for DeploymentRollout of openshift-monitoring/prometheus-adapter: current generation 2, observed generation 1, waiting for Alertmanager object changes failed: waiting for Alertmanager openshift-monitoring/main: expected 1 replicas, got 0 updated replicas, reconciling Thanos Querier Deployment failed: updating Deployment object failed: waiting for DeploymentRollout of openshift-monitoring/thanos-querier: current generation 1, observed generation 0\n    clusteroperator/openshift-controller-manager is not available (Available: no pods available on any node.) because All is well\n\n\nError from server (Forbidden): pods \"must-gather-\" is forbidden: error looking up service account openshift-must-gather-rn52t/default: serviceaccount \"default\" not found\n</code></pre> <p>and the generated bundle can be found here</p>"},{"location":"blog/2020/08/31/single_node_okd_installation/","title":"Single Node OKD Installation","text":"<p>This document outlines how to deploy a single node OKD cluster using virt.</p>"},{"location":"blog/2020/08/31/single_node_okd_installation/#requirements","title":"Requirements","text":"<ul> <li>Host with a minimal CentOS Stream, Fedora, or CentOS-8 installed (do not create a /home filesystem)</li> <li>Monitor, mouse, and keyboard attached to the host</li> <li>Static IP for the host</li> <li>The following packages installed: virt, wget, git, net-tools, bind, bind-utils, bash-completion, rsync, libguestfs-tools, virt-install, epel-release, libvirt-devel, httpd-tools, snf, nginx</li> </ul>"},{"location":"blog/2020/08/31/single_node_okd_installation/#procedure","title":"Procedure","text":"<p>For the complete procedure, please see Building an OKD4 single node cluster with minimal resources</p>"},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/","title":"Sri's Overkill Homelab Setup","text":"<p>This document lays out the resources used to create my completely-overkill homelab. This cluster provides all the compute and storage I think I'll need for the foreseeable future, and the CPU, RAM, and storage can all be scaled vertically independently of each other. Not that I think I'll need to do that for a while.</p> <p>More detail into the deployment and my homelab's Terraform configuration can be found here.</p>"},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/#hardware","title":"Hardware","text":"<ul> <li> <p>3 hyper-converged hypervisors</p> <ul> <li>Ryzen 5 3600</li> <li>64 GiB RAM</li> <li>3x 4TiB HDD</li> <li>2x 500GiB SSD in RAID1</li> <li>1x 256GiB NVME for boot disk</li> </ul> </li> <li> <p>1 NUC I had laying around gathering dust</p> <ul> <li>Intel Core i3-5010U</li> <li>16 GiB RAM</li> <li>500GiB SSD</li> </ul> </li> </ul>"},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/#main-cluster","title":"Main cluster","text":"<p>My hypervisors each host an identical workload. The total size of this cluster is 3 control plane nodes, and 9 worker nodes. So it splits very nicely three ways. Each hypervisor hosts 1 control plane VM and 3 worker VMs.</p> <ul> <li> <p>3 control plane nodes</p> <ul> <li>4x CPU</li> <li>10 GiB RAM</li> <li>50 GiB disk</li> </ul> </li> <li> <p>9 worker nodes</p> <ul> <li>8x CPU</li> <li>16 GiB RAM</li> <li>50 GiB root disk</li> <li>4 TiB HDD for workload use</li> </ul> </li> <li> <p>1 bootstrap node (temporary, taken down after initial setup is complete)</p> <ul> <li>4 vCPU</li> <li>8 GiB RAM</li> <li>120 GiB root disk</li> </ul> </li> </ul>"},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/#supporting-infrastructure","title":"Supporting infrastructure","text":""},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/#networking","title":"Networking","text":"<p>OKD, and especially baremetal UPI OKD, requires a very specific network setup. You will most likely need something more flexible than your ISP's router to get everything fully configured. The documentation is very clear on the various DNS records and DHCP static allocations you will need to make, so I won't go into them here.</p> <p>However, there are a couple extra things that you may want to set for best results. In particular, I make sure that I have PTR records set up for all my cluster nodes. This is extremely important as the nodes need a correct PTR record set up for them to auto-discover their hostname. Clusters typically do not set themselves up properly if there are hostname collisions!</p>"},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/#api-load-balancer","title":"API load balancer","text":"<p>I run a separate smaller VM on the NUC as a single-purpose load balancer appliance, running HAProxy.</p> <ul> <li>1 load balancer VM<ul> <li>2x vCPU</li> <li>256MiB RAM</li> <li>10GiB disk</li> </ul> </li> </ul> <p>The HAProxy config is straightforward. I adapted mine from the example config file created by the ocp4-helpernode playbook.</p>"},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/#deployment","title":"Deployment","text":"<p>I create the VMs on the hypervisors using Terraform. The Terraform Libvirt provider is very, very cool. It's also used by <code>openshift-install</code> for its Libvirt-based deployments, so it supports everything needed to deploy OKD nodes. Most importantly, I can use Terraform to supply the VMs with their Ignition configs, which means I don't have to worry about passing kernel args manually or setting up a PXE server to get things going like the official OKD docs would have you do. Terraform also makes it easy to tear down the cluster and reset in case something goes wrong.</p>"},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/#post-bootstrap-one-time-setup","title":"Post-Bootstrap One-Time Setup","text":""},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/#storage-with-rook-and-ceph","title":"Storage with Rook and Ceph","text":"<p>I deploy a Ceph cluster into OKD using Rook. The Rook configuration deploys OSDs on top of the 4TiB HDDs assigned to each worker. I deploy an erasure-coded CephFS pool (6+2) for RWX workloads and a 3x replica block pool for RWO workloads.</p>"},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/#monitoring-and-alerting","title":"Monitoring and Alerting","text":"<p>OKD comes with a very comprehensive monitoring and alerting suite, and it would be a shame not to take advantage of it. I set up an Alertmanager webhook to send any alerts to a small program I wrote that posts the alerts to Discord.</p> <p>I also deploy a Prometheus + Grafana set up into the cluster that collects metrics from the various hypervisors and supporting infrastructure VMs. I use Grafana's built-in Discord alerting mechanism to post those alerts.</p>"},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/#loadbalancer-with-metallb","title":"LoadBalancer with MetalLB","text":"<p>MetalLB is a piece of fantastic software that allows on-prem or otherwise non-public-cloud Kubernetes clusters to enjoy the luxury of <code>LoadBalancer</code> type services. It's dead simple to set up and makes you feel you're in a real datacenter. I deploy several workloads that don't use standard HTTP and so can't be deployed behind a <code>Route</code>. Without MetalLB, I wouldn't be able to deploy these workloads on OKD at all but with it, I can!</p>"},{"location":"blog/2020/08/31/sris_overkill_homelab_setup/#software-i-run","title":"Software I Run","text":"<p>I maintain an ansible playbook that handles deploying my workloads into the cluster. I prefer Ansible over other tools like Helm because it has more robust capabilities to store secrets, I find its templating capabilities more flexible and powerful than Helm's (especially when it comes to inlining config files into config maps or creating templated Dockerfiles for BuildConfigs), and because I am already familiar with Ansible and know how it works.</p> <ul> <li>paperless-ng - A document organizer that uses machine learning to automatically classify and organize</li> <li>bitwarden_rs - Password manager</li> <li>Jellyfin - Media management</li> <li>Samba - I joined a StatefulSet to my AD domain and it serves an authenticated SMB share</li> <li>Netbox - Infrastructure management tool</li> <li>Quassel - IRC bouncer</li> <li>Ukulele - Bot that plays music into Discord channels</li> <li>RPM and deb package repos for internal packages</li> </ul>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/","title":"Single Node UPI OKD Installation","text":"<p>This document outlines how to deploy a single node (the real hard way) using UPI OKD cluster on bare metal or virtual machines.</p>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#overview","title":"Overview","text":"<p>User provisioned infrastructure (UPI) of OKD 4.x Single Node cluster on bare metal or virtual machines</p> <p>N.B. Installer provisioned infrastructure (IPI) - this is the preferred method as it is much simpler, it automatically provisions and maintains the install for you, however it is targeted towards cloud and on prem services i.e aws, gcp, azure, also for openstack, IBM, and vSphere. </p> <p>If your install falls in these supported options then use IPI, if not this means that you will more than likely have to fallback on the UPI install method.</p> <p>At the end of this document I have supplied a link to my repository. It includes some useful scripts and an example install-config.yaml</p>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#requirements","title":"Requirements","text":"<p>The base installation should have 7 VM's (for a full production setup) but for our home lab SNO  we will use 2 VM's (one for bootstrap and one for the master/worker node) with the following specs :</p> <ul> <li> <p>Master/Worker Node/s</p> <ul> <li>CPU: 4 core</li> <li>RAM: 32G </li> <li>HDD: 50GB </li> </ul> </li> <li> <p>Bootstrap Node</p> <ul> <li>CPU: 4 core</li> <li>RAM: 8G</li> <li>HDD: 50G</li> </ul> </li> </ul> <p>N.B. - firewall services are disabled for this installation process</p>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#architecture-this-refers-to-a-full-high-availability-cluster","title":"Architecture (this refers to a full high availability cluster)","text":"<p>The diagram below shows an install for high availability scalable solution. For our single node install we only need a bootstrap node and a master/worker node (2 bare metal servers or 2 VM's)</p> <p></p>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#software","title":"Software","text":"<p>For the UPI SNO I made use of FCOS (Fedora CoreOS)</p> <p>FCOS</p> <ul> <li>For OKD  https://getfedora.org/en/coreos/download?tab=metal_virtualized&amp;stream=stable&amp;arch=x86_64<ul> <li>Download the ISO image</li> <li>Download the raw.tar.gz</li> </ul> </li> </ul> <p>OC Client &amp; Installer</p> <ul> <li>From https://github.com/openshift/okd/releases<ul> <li>Download Openshift installer</li> <li>Download Openshift client (cli)</li> </ul> </li> </ul>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#procedure","title":"Procedure","text":"<p>The following is a manual process of installing and configuring the infrastructure needed. </p> <ul> <li>HAProxy</li> <li>DNS (dnsmasq)</li> <li>NFS</li> <li>Config for ocp install etc</li> </ul>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#provision-vms-optional-skip-this-step-if-you-using-bare-metal-servers","title":"Provision VM's (Optional) - Skip this step if you using bare metal servers","text":"<p>The use of VM's is optional, each node could be a bare metal server. As I did not have several servers at my disposal I used a NUC (ryzen9 with 32G of RAM) and created 2 VM's (bootstrap and master/worker)</p> <p>I used cockpit (fedora) to validate the network and vm setup (from the scripts). Use the virtualization software that you prefer. For the okd-svc machine I used the bare metal server and installed fedora 37 (this hosted my 2 VM's)</p> <p>The bootstrap server can be shutdown once the master/worker has been fully setup</p> <p>Install virtualization</p> <pre><code>sudo dnf install @virtualization\n</code></pre>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#setup-ips-and-mac-addresses","title":"Setup IP's and MAC addresses","text":"<p>Refer to the \u201cArchitecture Diagram\u201d above to setup each VM</p> <p>Obviously the IP addresses will change according to you preferred setup (i.e 192.168.122.x) I have listed all servers, as it will be fairly easy to change the single node cluster to a fully fledged HA cluster, by changing the install-config.yaml</p> <p>As a useful example this is what I setup</p> <ul> <li>Gateway/Helper : okd-svc 192.168.122.1</li> <li>Bootstrap : okd-bootstrap 192.168.122.253</li> <li>Control Plane 1 : okd-cp-1 192.168.122.2</li> <li>Control Plane 2 : okd-cp-2 192.168.122.3</li> <li>Control Plane 3: okd-cp-3 192.168.122.4</li> <li>Worker 1 : okd-w-1 192.168.122.5</li> <li>Worker 2: okd-w-2  192.168.122.6</li> <li>Worker 3: okd-w-3  192.168.122.7</li> </ul> <p>Hard code MAC addresses (I created a text file to include in the VM network setting)</p> <pre><code>MAC: 52:54:00:3f:de:37, IP: 192.168.122.253\nMAC: 52:54:00:f5:9d:d4, IP: 192.168.122.2\nMAC: 52:54:00:70:b9:af, IP: 192.168.122.3\nMAC: 52:54:00:fd:6a:ca, IP: 192.168.122.4\nMAC: 52:54:00:bc:56:ff, IP: 192.168.122.5\nMAC: 52:54:00:4f:06:97, IP: 192.168.122.6\n</code></pre>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#install-configure-dependency-software","title":"Install &amp; Configure Dependency Software","text":""},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#install-configure-apache-web-server","title":"Install &amp; configure Apache Web Server","text":"<pre><code>dnf install httpd -y\n</code></pre> <p>Change default listen port to 8080 in httpd.conf</p> <pre><code>sed -i 's/Listen 80/Listen 0.0.0.0:8080/' /etc/httpd/conf/httpd.conf\n</code></pre> <p>Enable and start the service</p> <pre><code> systemctl enable httpd\n systemctl start httpd\n systemctl status httpd\n</code></pre> <p>Making a GET request to localhost on port 8080 should now return the default Apache webpage</p> <pre><code>curl localhost:8080\n</code></pre>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#install-haproxy-and-update-the-haproxycfg-as-follows","title":"Install HAProxy and update the haproxy.cfg as follows","text":"<pre><code>dnf install haproxy -y\n</code></pre> <p>Copy HAProxy config</p> <pre><code>cp ~/openshift-vm-install/haproxy.cfg /etc/haproxy/haproxy.cfg\n</code></pre> <p>Update Config</p> <pre><code># Global settings\n#---------------------------------------------------------------------\nglobal\n    maxconn     20000\n    log         /dev/log local0 info\n    chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    user        haproxy\n    group       haproxy\n    daemon\n\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    log                     global\n    mode                    http\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option redispatch\n    option forwardfor       except 127.0.0.0/8\n    retries                 3\n    maxconn                 20000\n    timeout http-request    10000ms\n    timeout http-keep-alive 10000ms\n    timeout check           10000ms\n    timeout connect         40000ms\n    timeout client          300000ms\n    timeout server          300000ms\n    timeout queue           50000ms\n\n# Enable HAProxy stats\nlisten stats\n    bind :9000\n    stats uri /stats\n    stats refresh 10000ms\n\n# Kube API Server\nfrontend k8s_api_frontend\n    bind :6443\n    default_backend k8s_api_backend\n    mode tcp\n\nbackend k8s_api_backend\n    mode tcp\n    balance source\n    server      bootstrap 192.168.122.253:6443 check\n    server      okd-cp-1 192.168.122.2:6443 check\n    server      okd-cp-2 192.168.122.3:6443 check\n    server      okd-cp-3 192.168.122.4:6443 check\n\n# OCP Machine Config Server\nfrontend ocp_machine_config_server_frontend\n    mode tcp\n    bind :22623\n    default_backend ocp_machine_config_server_backend\n\nbackend ocp_machine_config_server_backend\n    mode tcp\n    balance source\n    server      bootstrap 192.168.122.253:22623 check\n    server      okd-cp-1 192.168.122.2:22623 check\n    server      okd-cp-2 192.168.122.3:22623 check\n    server      okd-cp-3 192.168.122.4:22623 check\n\n# OCP Ingress - layer 4 tcp mode for each. Ingress Controller will handle layer 7.\nfrontend ocp_http_ingress_frontend\n    bind :80\n    default_backend ocp_http_ingress_backend\n    mode tcp\n\nbackend ocp_http_ingress_backend\n    balance source\n    mode tcp\n    server      okd-cp-1 192.168.122.2:80 check\n    server      okd-cp-2 192.168.122.3:80 check\n    server      okd-cp-3 192.168.122.4:80 check\n    server      okd-w-1 192.168.122.5:80 check\n    server      okd-w-2 192.168.122.6:80 check\n\nfrontend ocp_https_ingress_frontend\n    bind *:443\n    default_backend ocp_https_ingress_backend\n    mode tcp\n\nbackend ocp_https_ingress_backend\n    mode tcp\n    balance source\n    server      okd-cp-1 192.168.122.2:443 check\n    server      okd-cp-2 192.168.122.3:443 check\n    server      okd-cp-3 192.168.122.4:443 check\n    server      okd-w-1 192.168.122.5:443 check\n    server      okd-w-2 192.168.122.6:443 check\n</code></pre> <p>Start the HAProxy service</p> <pre><code>sudo systemctl start haproxy\n</code></pre> <p>Install dnsmasq and set the dnsmasq.conf file as follows</p> <pre><code># Configuration file for dnsmasq.\n\nport=53\n\n# The following two options make you a better netizen, since they\n# tell dnsmasq to filter out queries which the public DNS cannot\n# answer, and which load the servers (especially the root servers)\n# unnecessarily. If you have a dial-on-demand link they also stop\n# these requests from bringing up the link unnecessarily.\n\n# Never forward plain names (without a dot or domain part)\n#domain-needed\n# Never forward addresses in the non-routed address spaces.\nbogus-priv\n\nno-poll\n\nuser=dnsmasq\ngroup=dnsmasq\n\nbind-interfaces\n\nno-hosts\n# Include all files in /etc/dnsmasq.d except RPM backup files\nconf-dir=/etc/dnsmasq.d,.rpmnew,.rpmsave,.rpmorig\n\n# If a DHCP client claims that its name is \"wpad\", ignore that.\n# This fixes a security hole. see CERT Vulnerability VU#598349\n#dhcp-name-match=set:wpad-ignore,wpad\n#dhcp-ignore-names=tag:wpad-ignore\n\n\ninterface=eno1\ndomain=okd.lan\n\nexpand-hosts\n\naddress=/bootstrap.lab.okd.lan/192.168.122.253\nhost-record=bootstrap.lab.okd.lan,192.168.122.253\n\naddress=/okd-cp-1.lab.okd.lan/192.168.122.2\nhost-record=okd-cp-1.lab.okd.lan,192.168.122.2\n\naddress=/okd-cp-2.lab.okd.lan/192.168.122.3\nhost-record=okd-cp-2.lab.okd.lan,192.168.122.3\n\naddress=/okd-cp-3.lab.okd.lan/192.168.122.4\nhost-record=okd-cp-3.lab.okd.lan,192.168.122.4\n\naddress=/okd-w-1.lab.okd.lan/192.168.122.5\nhost-record=okd-w-1.lab.okd.lan,192.168.122.5\n\naddress=/okd-w-2.lab.okd.lan/192.168.122.6\nhost-record=okd-w-2.lab.okd.lan,192.168.122.6\n\naddress=/okd-w-3.lab.okd.lan/192.168.122.7\nhost-record=okd-w-3.lab.okd.lan,192.168.122.7\n\naddress=/api.lab.okd.lan/192.168.122.1\nhost-record=api.lab.okd.lan,192.168.122.1\naddress=/api-int.lab.okd.lan/192.168.122.1\nhost-record=api-int.lab.okd.lan,192.168.122.1\n\naddress=/etcd-0.lab.okd.lan/192.168.122.2\naddress=/etcd-1.lab.okd.lan/192.168.122.3\naddress=/etcd-2.lab.okd.lan/192.168.122.4\naddress=/.apps.lab.okd.lan/192.168.122.1\n\nsrv-host=_etcd-server-ssl._tcp,etcd-0.lab.okd.lan,2380\nsrv-host=_etcd-server-ssl._tcp,etcd-1.lab.okd.lan,2380\nsrv-host=_etcd-server-ssl._tcp,etcd-2.lab.okd.lan,2380\n\naddress=/oauth-openshift.apps.lab.okd.lan/192.168.122.1\naddress=/console-openshift-console.apps.lab.okd.lan/192.168.122.1\n</code></pre> <p>Start the dnsmasq service </p> <pre><code>sudo /usr/sbin/dnsmasq --conf-file=/etc/dnsmasq.conf\n</code></pre> <p>Test that your DNS setup is working correctly</p> <p>N.B.  It's important to verify that dns works, I found that for example if api-int.lab.okd.lan didn\u2019t resolve (also with reverse lookup) I had problems with bootstrap failing.</p> <pre><code># test &amp; results\n$ dig +noall +answer @192.168.122.1 api.lab.okd.lan\napi.lab.okd.lan.    0    IN    A    192.168.122.1\n\n$ dig +noall +answer @192.168.122.1 api-int.lab.okd.lan\napi-int.lab.okd.lan.    0    IN    A    192.168.122.1\n\n$ dig +noall +answer @192.168.122.1 random.apps.lab.okd.lan\nrandom.apps.lab.okd.lan. 0    IN    A    192.168.122.1\n\n$ dig +noall +answer @192.168.122.1 console-openshift-console.apps.lab.okd.lan\nconsole-openshift-console.apps.lab.okd.lan. 0 IN A 192.168.122.1\n\n$ dig +noall +answer @192.168.122.1 okd-bootstrap.lab.okd.lan\nokd-bootstrap.lab.okd.lan. 0    IN    A    192.168.122.253\n\n$ dig +noall +answer @192.168.122.1 okd-cp1.lab.okd.lan\nokd-cp1.lab.okd.lan.    0    IN    A    192.168.122.2\n\n$ dig +noall +answer @192.168.122.1 okd-cp2.lab.okd.lan\nokd-cp2.lab.okd.lan.    0    IN    A    192.168.122.3\n\n\n$ dig +noall +answer @192.168.122.1 okd-cp3.lab.okd.lan\nokd-cp3.lab.okd.lan.    0    IN    A    192.168.122.4\n\n$ dig +noall +answer @192.168.122.1 -x 192.168.122.1\n1.122.168.192.in-addr.arpa. 0    IN    PTR    okd-svc.okd-dev.\n\n$ dig +noall +answer @192.168.122.1 -x 192.168.122.2\n2.122.168.192.in-addr.arpa. 0    IN    PTR    okd-cp1.lab.okd.lan.\n\n$ dig +noall +answer @192.168.122.1 -x 192.168.122.3\n3.122.168.192.in-addr.arpa. 0    IN    PTR    okd-cp2.lab.okd.lan.\n\n$ dig +noall +answer @192.168.122.1 -x 192.168.122.4\n4.122.168.192.in-addr.arpa. 0    IN    PTR    okd-cp3.lab.okd.lan.\n\n$ dig +noall +answer @192.168.122.1 -x 192.168.122.5\n5.122.168.192.in-addr.arpa. 0    IN    PTR    okd-w1.lab.okd.lan.\n\n$ dig +noall +answer @192.168.122.1 -x 192.168.122.6\n6.122.168.192.in-addr.arpa. 0    IN    PTR    okd-w2.lab.okd.lan.\n\n$ dig +noall +answer @192.168.122.1 -x 192.168.122.7\n7.122.168.192.in-addr.arpa. 0    IN    PTR    okd-w3.lab.okd.lan.\n</code></pre> <p>Install and configure NFS for the OKD Registry. It is a requirement to provide storage for the Registry, emptyDir can be specified if necessary.</p> <pre><code>sudo dnf install nfs-utils -y\n</code></pre> <p>Create the share</p> <pre><code>mkdir -p /shares/registry\nchown -R nobody:nobody /shares/registry\nchmod -R 777 /shares/registry\n</code></pre> <p>Export the share, this allows any service in the 192.168.122.xxx range to access NFS</p> <pre><code>echo \"/shares/registry  192.168.122.0/24(rw,sync,root_squash,no_subtree_check,no_wdelay)\" &gt; /etc/exports\n\nexportfs -rv\n</code></pre> <p>Enable and start the NFS related services</p> <pre><code>sudo systemctl enable nfs-server rpcbind\nsudo systemctl start nfs-server rpcbind nfs-mountd\n</code></pre> <p>Create an install directory</p> <pre><code> mkdir ~/okd-install\n</code></pre> <p>Copy the install-config.yaml included in the cloned repository (see link at end of the document) to the install directory</p> <pre><code>cp ~/openshift-vm-install/install-config.yaml ~/okd-install\n</code></pre> <p>Where install-config.yaml is as follows</p> <pre><code>apiVersion: v1\nbaseDomain: okd.lan\ncompute:\n  - hyperthreading: Enabled\n    name: worker\n    replicas: 0 # Must be set to 0 for User Provisioned Installation as worker nodes will be manually deployed.\ncontrolPlane:\n  hyperthreading: Enabled\n  name: master\n  replicas: 3\nmetadata:\n  name: lab # Cluster name\nnetworking:\n  clusterNetwork:\n    - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  networkType: OpenShiftSDN\n  serviceNetwork:\n    - 172.30.0.0/16\nplatform:\n  none: {}\nfips: false\npullSecret: 'add your pull secret here'\nsshKey: 'add your ssh public key here'\n</code></pre> <p>Update the install-config.yaml with your own pull-secret and ssh key.</p> <pre><code>vim ~/okd-install/install-config.yaml\n</code></pre> <ul> <li>Line 23 should contain the contents of your pull-secret.txt</li> <li>Line 24 should contain the contents of your '~/.ssh/id_rsa.pub'  as an (example) </li> </ul> <p>If needed create public/private key pair using openssh</p> <p>Generate Kubernetes manifest files</p> <pre><code>~/openshift-install create manifests --dir ~/okd-install\n</code></pre> <p>A warning is shown about making the control plane nodes schedulable. </p> <p>For the SNO it's mandatory to run workloads on the <code>Control Plane</code> nodes. </p> <p>If you don't want to you (incase you move to the full HA install)  you can disable this with:</p> <pre><code>`sed -i 's/mastersSchedulable: true/mastersSchedulable: false/' ~/okd-install/manifests/cluster-scheduler-02-config.yml`.\n</code></pre> <p>Make any other custom changes you like to the core Kubernetes manifest files.</p> <p>Generate the Ignition config and Kubernetes auth files</p> <pre><code>~/openshift-install create ignition-configs --dir ~/okd-install\n</code></pre> <p>Create a hosting directory to serve the configuration files for the OKD booting process</p> <pre><code>mkdir /var/www/html/okd4\n</code></pre> <p>Copy all generated install files to the new web server directory</p> <pre><code>cp -R ~/okd-install/* /var/www/html/okd4\n</code></pre> <p>Move the Core OS image to the web server directory (later you need to type this path multiple times so it is a good idea to shorten the name)</p> <pre><code>mv ~/fhcos-X.X.X-x86_64-metal.x86_64.raw.gz /var/www/html/okd4/fhcos\n</code></pre> <p>Change ownership and permissions of the web server directory</p> <pre><code>chcon -R -t httpd_sys_content_t /var/www/html/okd4/\nchown -R apache: /var/www/html/okd4/\nchmod 755 /var/www/html/okd4/\n</code></pre> <p>Confirm you can see all files added to the <code>/var/www/html/ocp4/</code>  through Apache</p> <pre><code>curl localhost:8080/okd4/\n</code></pre> <p>Start VMS/Bare metal servers</p> <p>Execute for each VM type the appropriate coreos-installer command</p> <p>Change the \u2013ignition-url for each type i.e</p> <p>N.B. For our  SNO install we are only going to use bootstrap and master ignition files (ignore worker.ign)</p> <p>Bootstrap Node <pre><code>--ignition-url https://192.168.122.1:8080/okd4/bootstrap.ign\n</code></pre></p> <p>Master Node <pre><code>--ignition-url https://192.168.122.1:8080/okd4/master.ign\n</code></pre></p> <p>Worker Node <pre><code>--ignition-url https://192.168.122.1:8080/okd4/worker.ign\n</code></pre></p> <p>A typical cli for CoreOS (using master.ign would look like this)</p> <pre><code>$ sudo coreos-installer install /dev/sda --ignition-url http://192.168.122.1:8080/okd4/master.ign  --image-url http://192.168.122.1:8080/okd4/fhcos  --insecure-ignition -\u2013insecure \n</code></pre> <p>NB Note if using Fedora CoreOS the device would need to change i.e /dev/vda</p> <p>Once the VM's are running with the relevant ignition files</p> <p>Issue the following commands</p> <p>This will install  and wait for the bootstrap service to complete</p> <pre><code>openshift-install --dir ~/$INSTALL_DIR wait-for bootsrap-complete --log-level=debug\n</code></pre> <p>Once the bootstrap has installed then issue this command</p> <pre><code>openshift-install --dir ~/$INSTALL_DIR wait-for install-complete --log-level=debug\n</code></pre> <p>This will take about 40 minutes (or longer) after a successful install you will need to approve certificates and setup the persistent volume for the internal registry</p>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#post-install","title":"Post Install","text":"<p>At this point you can shutdown the bootstrap server</p> <p>Approve certification signing request</p> <pre><code># Export the KUBECONFIG environment variable (to gain access to the cluster)\nexport KUBECONFIG=$INSTALL_DIR/auth/kubeconfig\n\n# View CSRs\noc get csr\n# Approve all pending CSRs\noc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs oc adm certificate approve\n# Wait for kubelet-serving CSRs and approve them too with the same command\noc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs oc adm certificate approve\n</code></pre> <p>Configure Registry</p> <pre><code>oc edit configs.imageregistry.operator.openshift.io\n\n# update the yaml\nmanagementState: Managed\n\nstorage:\n  pvc:\n    claim: # leave the claim blank\n\n# save the changes and execute the following commands\n\n# check for \u2018pending\u2019 state\noc get pvc -n openshift-image-registry\n\noc create -f registry-pv.yaml\n# After a short wait the 'image-registry-storage' pvc should now be bound\noc get pvc -n openshift-image-registry\n</code></pre> <p>Remote Access</p> <p>As haproxy has been set up as a load balancer for the cluster,  add the following to your /etc/hosts file.  Obviously the IP address will change according to where you setup your haproxy </p> <pre><code>192.168.8.122 okd-svc api.lab.okd.lan api-int.lab.okd.lan console-openshift-console.apps.lab.okd.lan oauth-openshift.apps.lab.okd.lan downloads-openshift-console.apps.lab.okd.lan alertmanager-main-openshift-monitoring.apps.lab.okd.lan grafana-openshift-monitoring.apps.lab.okd.lan prometheus-k8s-openshift-monitoring.apps.lab.okd.lan thanos-querier-openshift-monitoring.apps.lab.okd.lan\n</code></pre> <p>Helper Script</p> <p>I have included a WIP script to help with setting up the virtual network, machines and utilities to configure the OKD install,  apply haproxy config, apply dns config, setup NFS  and firewall setup. </p> <p>Dependencies</p> <ul> <li>You would need to install virt-manager, virsh etc.</li> <li>OKD command line client </li> <li>OKD installer</li> <li>HAProxy</li> <li>Apache httpd</li> <li>DNS server (dnsmasq)</li> <li>NFS (all relevant utils)</li> </ul> <p>As mentioned it\u2019s still a work in progress, but fairly helpful (imho) for now.</p> <p>A typical flow would be (once all the dependencies have been installed)</p> <pre><code>./virt-env-install.sh config # configures install-config.yaml\n./virt-env-install.sh dnsconfig\n\n# before continuing manually test your dns setup\n\n./virt-env-install.sh haproxy\n./virt-env-install.sh firewall # can be ignored as firewalld has been disabled\n./virt-env-install.sh network\n./virt-env-install.sh manifests\n./virt-env-install.sh ignition\n./virt-env-install.sh copy\n./virt-env-install.sh vm bootstrap ok (repeat this for each vm needed)\n./virt-env-install.sh vm cp-1 ok \n./virt-env-install.sh okd-install bootstrap\n./virt-env-install.sh okd-install install\n</code></pre> <p>N.B. If there are any discrepancies or improvements please make note. PR's are most welcome !!!</p> <p>Screenshot of final OKD install </p> <p></p>"},{"location":"blog/2020/08/31/single_node_upi_okd_installation/#acknowledgement-links","title":"Acknowledgement &amp; Links","text":"<p>github repo https://github.com/lmzuccarelli/okd-baremetal-install</p> <p>Thanks and acknowledgement to Ryan Hay</p> <p>Reference :  https://github.com/ryanhay/ocp4-metal-install</p>"},{"location":"blog/2020/08/31/vadims_homelab/","title":"Vadim's homelab","text":"<p>This describes the resources used by OpenShift after performing an installation to make it similar to my homelab setup.</p>"},{"location":"blog/2020/08/31/vadims_homelab/#compute","title":"Compute","text":"<ol> <li> <p>Ubiquity EdgeRouter ER-X</p> <ul> <li>runs DHCP (embedded), custom DNS server via AdGuard</li> </ul> <p></p> </li> <li> <p>NAS/Bastion host</p> <ul> <li>haproxy for loadbalancer</li> <li>ceph cluster for PVs</li> <li>NFS server for shared data</li> </ul> <p></p> </li> <li> <p>control plane</p> <ul> <li>Intel i5 CPU, 16+4 GB RAM</li> <li>120 GB NVME disk</li> </ul> <p></p> </li> <li> <p>compute nodes</p> <ul> <li>Lenovo X220 laptop</li> </ul> <p></p> </li> </ol>"},{"location":"blog/2020/08/31/vadims_homelab/#router-setup","title":"Router setup","text":"<p>Once nodes have booted assign static IPs using MAC pinning.</p> <p>EdgeRouter has dnsmasq to support custom DNS entries, but I wanted to have a network-wide ad filtering and DNS-over-TLS for free, so I followed this guide to install AdGuard Home on the router.</p> <p>This gives a fancy UI for DNS rewrites and gives a useful stats about the nodes on the network.</p>"},{"location":"blog/2020/08/31/vadims_homelab/#nasbastion-setup","title":"NAS/Bastion setup","text":"<p>HAProxy setup is fairly standard - see ocp4-helpernode for idea.</p> <p>Along with (fairly standard) NFS server I also run a single node Ceph cluster, so that I could benefit from CSI / autoprovision / snapshots etc.</p>"},{"location":"blog/2020/08/31/vadims_homelab/#installation","title":"Installation","text":"<p>Currently \"single node install\" requires a dedicated throwaway bootstrap node, so I used future compute node (x220 laptop) as a bootstrap node. Once master was installed, the laptop was re-provisioned to become a compute node.</p>"},{"location":"blog/2020/08/31/vadims_homelab/#upgrading","title":"Upgrading","text":"<p>Since I use a single master install, upgrades are bit complicated. Both nodes are labelled as workers, so upgrading those is not an issue.</p> <p>Upgrading single master is tricky, so I use this script to pivot the node into expected master ignition content, which runs <code>rpm-ostree rebase &lt;new content&gt;</code>. This script needs to be cancelled before it starts installing OS extensions (NetworkManager-ovs etc.) as its necessary.</p> <p>This issue as a class would be addressed in 4.8.</p>"},{"location":"blog/2020/08/31/vadims_homelab/#useful-software","title":"Useful software","text":"<p>Grafana operator is incredibly useful to setup monitoring.</p> <p>This operator helps me to define a configuration for various datasources (i.e. Promtail+Loki) and control dashboard source code using CRs.</p> <p>SnapScheduler makes periodic snapshots of some PVs so that risky changes could be reverted.</p> <p>Tekton operator is helping me to run a few clean up jobs in cluster periodically.</p> <p>Most useful pipeline I've been using is running <code>oc adm must-gather</code> on this cluster, unpacking it and storing it in Git. This helps me keep track of changes in the cluster in a git repo - and, unlike gitops solution like ArgoCD - I can still tinker with things in the console.</p> <p>Other useful software running in my cluster:</p> <ul> <li>Gitea - git server</li> <li>HomeAssistant - controls smart home devices</li> <li>BitWarden_rs - password storage</li> <li>Minio - S3-like storage</li> <li>Nextcloud - file sync software</li> <li>Navidrome - music server</li> <li>MiniFlux - RSS reader</li> <li>Matrix Synapse - federated chat app</li> <li>Pleroma - federated microblogging app</li> <li>Wallabag - Read-It-Later app</li> </ul>"},{"location":"blog/2020/08/31/vsphere_ipi_deployment/","title":"vSphere IPI Deployment","text":"<p>This describes the resources used by OpenShift after performing an installation using the required options for the installer.</p>"},{"location":"blog/2020/08/31/vsphere_ipi_deployment/#infrastructure","title":"Infrastructure","text":""},{"location":"blog/2020/08/31/vsphere_ipi_deployment/#compute","title":"Compute","text":"<p>All vms stored within folder described above and tagged with tag created by installer.</p> <ul> <li>3 control plane vms (name format: <code>{cluster name}-{generated cluster id}-master-{0,1,2}</code>)</li> <li>4 vCPU</li> <li>16 GB RAM</li> <li> <p>120 GB storage</p> </li> <li> <p>3 worker vms (name format: <code>{cluster name}-{generated cluster id}-master-{generated worker id}</code>)</p> </li> <li>2 vCPU</li> <li>8 GB RAM</li> <li>120 GB storage</li> </ul>"},{"location":"blog/2020/08/31/vsphere_ipi_deployment/#networking","title":"Networking","text":"<p>Should be set up by user. Installer doesn't create anything there. Network name should be provided as installer argument.</p>"},{"location":"blog/2020/08/31/vsphere_ipi_deployment/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>tag category with format <code>openshift-{cluster name}-{generated cluster id}</code></li> <li>tag with format <code>{cluster name}-{generated cluster id}</code></li> <li>folder with title format <code>{cluster name}-{generated cluster id}</code></li> <li>disabled virtual machine with name <code>{cluster name}-rhcos-{generated cluster id}</code> which using as template for further scaling</li> </ul>"},{"location":"blog/2020/08/31/vsphere_ipi_deployment/#deployment","title":"Deployment","text":"<p>See the OKD documentation to proceed with deployment</p>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/","title":"Prerequisites for vSphere UPI","text":"<p>In this example I describe the setup of a DNS/DHCP server and a Load Balancer on a Raspberry PI microcomputer. The instructions most certainly will also work for other environments.</p> <p>I use Raspberry Pi OS (debian based).</p>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#ip-addresses-of-components-in-this-example","title":"IP Addresses of components in this example","text":"<ul> <li>Homelab subnet: 192.168.178.0/24</li> <li>DSL router/gateway: 192.168.178.1</li> <li>IP address of Raspberry Pi (DHCP/DNS/Load Balancer): 192.168.178.5</li> <li>local domain: homelab.net</li> <li>local cluster (name: c1) domain: c1.homelab.net</li> <li>DHCP range: 192.168.178.40 \u2026 192.168.178.199</li> <li>Static IPs for OKD\u2019s bootstrap, masters and workers</li> </ul>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#upgrade-raspberry-pi","title":"Upgrade Raspberry Pi","text":"<pre><code>sudo apt-get update\nsudo apt-get upgrade\nsudo reboot\n</code></pre>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#set-static-ip-address-on-raspberry-pi","title":"Set static IP address on Raspberry Pi","text":"<p>Add this:</p> <pre><code>interface eth0 \nstatic ip_address=192.168.178.5/24 \nstatic routers=192.168.178.1 \nstatic domain_name_servers=192.168.178.5 8.8.8.8\n</code></pre> <p>to /etc/dhcpcd.conf</p>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#dhcp","title":"DHCP","text":"<p>Ensure that no other DHCP servers are activated in the network of your homelab e.g. in your internet router.</p> <p>The DHCP server in this example is setup with DDNS (Dynamic DNS) enabled.</p>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#install","title":"Install","text":"<p><code>sudo apt-get install isc-dhcp-server</code></p>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#configure","title":"Configure","text":"<p>Enable DHCP server for IPv4 on eth0:</p> <p>/etc/default/isc-dhcp-server</p> <pre><code>INTERFACESv4=\"eth0\" \nINTERFACESv6=\"\"\n</code></pre> <p>/etc/dhcp/dhcpd.conf</p> <pre><code># dhcpd.conf\n#\n\n####################################################################################\n# Configuration for Dynamic DNS (DDNS) updates                                     #\n# Clients requesting an IP and sending their hostname for domain *.homelab.net     #\n# will be auto registered in the DNS server.                                       #\n####################################################################################\nddns-updates on;\nddns-update-style standard;\n\n# This option points to the copy rndc.key we created for bind9.\ninclude \"/etc/bind/rndc.key\";\n\nallow unknown-clients;\nuse-host-decl-names on;\ndefault-lease-time 300; # 5 minutes\nmax-lease-time 300;     # 5 minutes\n\n# homelab.net DNS zones\nzone homelab.net. {\n  primary 192.168.178.5; # This server is the primary DNS server for the zone\n  key rndc-key;       # Use the key we defined earlier for dynamic updates\n}\nzone 178.168.192.in-addr.arpa. {\n  primary 192.168.178.5; # This server is the primary reverse DNS for the zone\n  key rndc-key;       # Use the key we defined earlier for dynamic updates\n}\n\nddns-domainname \"homelab.net.\";\nddns-rev-domainname \"in-addr.arpa.\";\n####################################################################################\n\n\n####################################################################################\n# Basic configuration                                                              #\n####################################################################################\n# option definitions common to all supported networks...\ndefault-lease-time 300;\nmax-lease-time     300;\n\n# If this DHCP server is the official DHCP server for the local\n# network, the authoritative directive should be uncommented.\nauthoritative;\n\n# Parts of this section will be put in the /etc/resolv.conf of your hosts later\noption domain-name \"homelab.net\";\noption routers 192.168.178.1;\noption subnet-mask 255.255.255.0;\noption domain-name-servers 192.168.178.5;\n\nsubnet 192.168.178.0 netmask 255.255.255.0 {\n  range 192.168.178.40 192.168.178.199;\n}\n####################################################################################\n\n\n####################################################################################\n# Static IP addresses                                                              #\n# (Replace the MAC addresses here with the ones you set in vsphere for your vms)   #\n####################################################################################\ngroup {\n  host bootstrap {\n      hardware ethernet 00:1c:00:00:00:00;\n      fixed-address 192.168.178.200;\n  }\n\n  host master0 {\n      hardware ethernet 00:1c:00:00:00:10;\n      fixed-address 192.168.178.210;\n  }\n\n  host master1 {\n      hardware ethernet 00:1c:00:00:00:11;\n      fixed-address 192.168.178.211;\n  }\n\n  host master2 {\n      hardware ethernet 00:1c:00:00:00:12;\n      fixed-address 192.168.178.212;\n  }\n\n  host worker0 {\n      hardware ethernet 00:1c:00:00:00:20;\n      fixed-address 192.168.178.220;\n  }\n\n  host worker1 {\n      hardware ethernet 00:1c:00:00:00:21;\n      fixed-address 192.168.178.221;\n  }\n\n  host worker2 {\n      hardware ethernet 00:1c:00:00:00:22;\n      fixed-address 192.168.178.222;\n  }  \n}\n</code></pre>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#dns","title":"DNS","text":""},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#install_1","title":"Install","text":"<pre><code>sudo apt install bind9 dnsutils\n</code></pre>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#basic-configuration","title":"Basic configuration","text":"<p>/etc/bind/named.conf.options</p> <pre><code>include \"/etc/bind/rndc.key\";\n\nacl internals {\n    // lo adapter\n    127.0.0.1;\n\n    // CIDR for your homelab network\n    192.168.178.0/24;\n};\n\noptions {\n        directory \"/var/cache/bind\";\n\n        // If there is a firewall between you and nameservers you want\n        // to talk to, you may need to fix the firewall to allow multiple\n        // ports to talk.  See http://www.kb.cert.org/vuls/id/800113\n\n        // If your ISP provided one or more IP addresses for stable\n        // nameservers, you probably want to use them as forwarders.\n        // Uncomment the following block, and insert the addresses replacing\n        // the all-0's placeholder.\n\n        forwarders {\n          8.8.8.8;\n          8.8.4.4;\n        };\n        forward only;\n\n        //========================================================================\n        // If BIND logs error messages about the root key being expired,\n        // you will need to update your keys.  See https://www.isc.org/bind-keys\n        //========================================================================\n        dnssec-validation no;\n\n        listen-on-v6 { none; };\n        auth-nxdomain no;\n        listen-on port 53 { any; };\n\n        // Allow queries from my Homelab and also from Wireguard Clients.\n        allow-query { internals; };\n        allow-query-cache { internals; };\n        allow-update { internals; };\n        recursion yes;\n        allow-recursion { internals; };\n        allow-transfer { internals; };\n\n        dnssec-enable no;\n\n        check-names master ignore;\n        check-names slave ignore;\n        check-names response ignore;\n};\n</code></pre> <p>/etc/bind/named.conf.local</p> <pre><code>#include \"/etc/bind/rndc.key\";\n\n//\n// Do any local configuration here\n//\n\n// Consider adding the 1918 zones here, if they are not used in your\n// organization\n//include \"/etc/bind/zones.rfc1918\";\n\n# All devices that don't belong to the OKD cluster will be maintained here.\nzone \"homelab.net\" {\n   type master;\n   file \"/etc/bind/forward.homelab.net\";\n   allow-update { key rndc-key; };\n};\n\nzone \"c1.homelab.net\" {\n   type master;\n   file \"/etc/bind/forward.c1.homelab.net\";\n   allow-update { key rndc-key; };\n};\n\nzone \"178.168.192.in-addr.arpa\" {\n   type master;\n   notify no;\n   file \"/etc/bind/178.168.192.in-addr.arpa\";\n   allow-update { key rndc-key; };\n};\n</code></pre> <p>Zone file for homlab.net: /etc/bind/forward.homelab.net</p> <pre><code>;\n; BIND data file for local loopback interface\n;\n$TTL    604800\n@       IN      SOA     homelab.net. root.homelab.net. (\n                              2         ; Serial\n                         604800         ; Refresh\n                          86400         ; Retry\n                        2419200         ; Expire\n                         604800 )       ; Negative Cache TTL\n;\n@       IN      NS      homelab.net.\n@       IN      A       192.168.178.5\n@       IN      AAAA    ::1\n</code></pre> <p>The name of the next file depends on the subnet that is used:</p> <p>/etc/bind/178.168.192.in-addr.arpa</p> <pre><code>$TTL 1W\n@ IN SOA ns1.homelab.net. root.homelab.net. (\n                                2019070742 ; serial\n                                10800      ; refresh (3 hours)\n                                1800       ; retry (30 minutes)\n                                1209600    ; expire (2 weeks)\n                                604800     ; minimum (1 week)\n                                )\n                        NS      ns1.homelab.net.\n\n200                     PTR     bootstrap.c1.homelab.net.\n\n210                     PTR     master0.c1.homelab.net.\n211                     PTR     master1.c1.homelab.net.\n212                     PTR     master2.c1.homelab.net.\n\n220                     PTR     worker0.c1.homelab.net.\n221                     PTR     worker1.c1.homelab.net.\n222                     PTR     worker2.c1.homelab.net.\n\n5                       PTR     api.c1.homelab.net.\n5                       PTR     api-int.c1.homelab.net.\n</code></pre>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#dns-records-for-okd-4","title":"DNS records for OKD 4","text":"<p>Zone file for c1.homelab.net (our OKD 4 cluster will be in this domain):</p> <p>/etc/bind/forward.c1.homelab.net</p> <pre><code>;\n; BIND data file for local loopback interface\n;\n$TTL    604800\n@       IN      SOA     c1.homelab.net. root.c1.homelab.net. (\n                              2         ; Serial\n                         604800         ; Refresh\n                          86400         ; Retry\n                        2419200         ; Expire\n                         604800 )       ; Negative Cache TTL\n;\n@       IN      NS      c1.homelab.net.\n@       IN      A       192.168.178.5\n@       IN      AAAA    ::1\n\nload-balancer IN A      192.168.178.5\n\nbootstrap IN    A       192.168.178.200\n\nmaster0 IN      A       192.168.178.210\nmaster1 IN      A       192.168.178.211\nmaster2 IN      A       192.168.178.212\n\nworker0 IN      A       192.168.178.220\nworker1 IN      A       192.168.178.221\nworker2 IN      A       192.168.178.222\nworker3 IN      A       192.168.178.223\n\n*.apps.c1.homelab.net.  IN CNAME load-balancer.c1.homelab.net.\napi-int.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net.\napi.c1.homelab.net.     IN CNAME load-balancer.c1.homelab.net.\n</code></pre>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#set-file-permissions","title":"Set file permissions","text":"<p>For dynamic DNS (ddns) to work you should do this: </p> <pre><code>sudo chown -R bind:bind /etc/bind\n</code></pre>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#load-balancer","title":"Load Balancer","text":""},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#install_2","title":"Install","text":"<pre><code>sudo apt-get install haproxy\n</code></pre>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#configure_1","title":"Configure","text":"<p>/etc/haproxy/haproxy.cfg</p> <pre><code>global\n        log /dev/log    local0\n        log /dev/log    local1 notice\n        chroot /var/lib/haproxy\n        stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners\n        stats timeout 30s\n        user haproxy\n        group haproxy\n        daemon\n\n        # Default SSL material locations\n        ca-base /etc/ssl/certs\n        crt-base /etc/ssl/private\n\n        # Default ciphers to use on SSL-enabled listening sockets.\n        # For more information, see ciphers(1SSL). This list is from:\n        #  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/\n        # An alternative list with additional directives can be obtained from\n        #  https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy\n        ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS\n        ssl-default-bind-options no-sslv3\n\ndefaults\n        log     global\n        mode    http\n        option  httplog\n        option  dontlognull\n        timeout connect 20000\n        timeout client  10000\n        timeout server  10000\n        errorfile 400 /etc/haproxy/errors/400.http\n        errorfile 403 /etc/haproxy/errors/403.http\n        errorfile 408 /etc/haproxy/errors/408.http\n        errorfile 500 /etc/haproxy/errors/500.http\n        errorfile 502 /etc/haproxy/errors/502.http\n        errorfile 503 /etc/haproxy/errors/503.http\n        errorfile 504 /etc/haproxy/errors/504.http\n\n\n# You can see the stats and observe OKD's bootstrap process by opening\n# http://&lt;IP&gt;:4321/haproxy?stats\nlisten stats\n    bind :4321\n    mode            http\n    log             global\n    maxconn 10\n\n    timeout client  100s\n    timeout server  100s\n    timeout connect 100s\n    timeout queue   100s\n\n    stats enable\n    stats hide-version\n    stats refresh 30s\n    stats show-node\n    stats auth admin:password\n    stats uri  /haproxy?stats\n\n\nfrontend openshift-api-server\n    bind *:6443\n    default_backend openshift-api-server\n    mode tcp\n    option tcplog\n\nbackend openshift-api-server\n    balance source\n    mode tcp\n    server bootstrap bootstrap.c1.homelab.net:6443 check\n    server master0 master0.c1.homelab.net:6443 check\n    server master1 master1.c1.homelab.net:6443 check\n    server master2 master2.c1.homelab.net:6443 check\n\n\nfrontend machine-config-server\n    bind *:22623\n    default_backend machine-config-server\n    mode tcp\n    option tcplog\n\nbackend machine-config-server\n    balance source\n    mode tcp\n    server bootstrap bootstrap.c1.homelab.net:22623 check\n    server master0 master0.c1.homelab.net:22623 check\n    server master1 master1.c1.homelab.net:22623 check\n    server master2 master2.c1.homelab.net:22623 check\n\n\nfrontend ingress-http\n    bind *:80\n    default_backend ingress-http\n    mode tcp\n    option tcplog\n\nbackend ingress-http\n    balance source\n    mode tcp\n    server master0 master0.c1.homelab.net:80 check\n    server master1 master1.c1.homelab.net:80 check\n    server master2 master2.c1.homelab.net:80 check\n\n    server worker0 worker0.c1.homelab.net:80 check\n    server worker1 worker1.c1.homelab.net:80 check\n    server worker2 worker2.c1.homelab.net:80 check\n    server worker3 worker3.c1.homelab.net:80 check\n\n\nfrontend ingress-https\n    bind *:443\n    default_backend ingress-https\n    mode tcp\n    option tcplog\n\nbackend ingress-https\n    balance source\n    mode tcp\n\n    server master0 master0.c1.homelab.net:443 check\n    server master1 master1.c1.homelab.net:443 check\n    server master2 master2.c1.homelab.net:443 check\n\n    server worker0 worker0.c1.homelab.net:443 check\n    server worker1 worker1.c1.homelab.net:443 check\n    server worker2 worker2.c1.homelab.net:443 check\n    server worker3 worker3.c1.homelab.net:443 check\n</code></pre>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#reboot-and-check-status","title":"Reboot and check status","text":"<p>Reboot Raspberry Pi:</p> <pre><code>sudo reboot\n</code></pre> <p>Check status of DNS/DHCP server and Load Balancer:</p> <pre><code>sudo systemctl status haproxy.service \nsudo systemctl status isc-dhcp-server.service \nsudo systemctl status bind9\n</code></pre>"},{"location":"blog/2020/08/31/prerequisites_for_vsphere_upi/#proxy-if-on-a-private-network","title":"Proxy (if on a private network)","text":"<p>If the cluster will sit on a private network, you\u2019ll need a proxy for outgoing traffic, both for the install process and for regular operation. In the case of the former, the installer needs to pull containers from the external registries. In the case of the latter, the proxy is needed when application containers need access to the outside world (e.g. yum installs, external code repositories like gitlab, etc.)</p> <p>The proxy should be configured to accept connections from the IP subnet for your cluster. A simple proxy to use for this purpose is squid</p>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/","title":"OKD Virtualization on user provided infrastructure","text":"<p>This guide shows how to set up OKD Virtualization</p>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#preparing-the-hardware","title":"Preparing the hardware","text":"<p>As a first step for providing an infrastructure for OKD Virtualization, you need to prepare the hardware:</p> <ul> <li>check that the minimum hardware requirements for running OKD are satisfied</li> <li>check that the additional hardware requirements for running OKD Virtualization are also satisfied.</li> </ul>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#preparing-the-infrastructure","title":"Preparing the infrastructure","text":"<p>Once your hardware is ready and connected to the network you need to configure your services, your network and your DNS for allowing the OKD installer to deploy the software. You may also need to prepare in advance a few services you'll need during the deployment. Carefully read the Preparing the user-provisioned infrastructure section and ensure all the requirements are met.</p>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#provision-your-hosts","title":"Provision your hosts","text":"<p>For the bastion / service host you can use CentOS Stream 8. You can follow the CentOS 8 installation documentation but we recommend using the latest CentOS Stream 8 ISO.</p> <p>For the OKD nodes you\u2019ll need Fedora CoreOS. You can get it from the Get Fedora! website, choose the Bare Metal ISO.</p>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#configure-the-bastion-to-host-needed-services","title":"Configure the bastion to host needed services","text":"<p>Configure Apache to serve on port 8080/8443 as the http/https port will be used by the haproxy service. Apache will be needed to provide ignition configuration for OKD nodes.</p> <pre><code>dnf install -y httpd\nsed -i 's/Listen 80/Listen 8080/' /etc/httpd/conf/httpd.conf\nsed -i 's/Listen 443/Listen 8443/' /etc/httpd/conf.d/ssl.conf\nsetsebool -P httpd_read_user_content 1\nsystemctl enable --now httpd.service\nfirewall-cmd --permanent --add-port=8080/tcp\nfirewall-cmd --permanent --add-port=8443/tcp\nfirewall-cmd --reload\n# Verify it\u2019s up:\ncurl localhost:8080\n</code></pre> <p>Configure haproxy:</p> <pre><code>dnf install haproxy -y\nfirewall-cmd --permanent --add-port=6443/tcp\nfirewall-cmd --permanent --add-port=22623/tcp\nfirewall-cmd --permanent --add-service=http\nfirewall-cmd --permanent --add-service=https\nfirewall-cmd --reload\nsetsebool -P haproxy_connect_any 1\nsystemctl enable --now haproxy.service\n</code></pre>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#installing-okd","title":"Installing OKD","text":"<p>OKD current stable-4 branch is delivering OKD 4.8. If you're using an older version we recommend to update to ODK 4.8.</p> <p>At this point you should have all OKD nodes ready to be installed with Fedora CoreOS and the bastion with all the needed services. Check that all nodes and the bastion have the correct ip addresses and fqdn and that they are resolvable via DNS.</p> <p>As we are going to use the baremetal UPI installation you\u2019ll need to create a <code>install-config.yaml</code> following the example for installing bare metal</p> <p>Remember to configure your proxy settings if you have a proxy</p>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#apply-the-workarounds","title":"Apply the workarounds","text":"<ul> <li>qemu-ga is hitting selinux denials https://bugzilla.redhat.com/show_bug.cgi?id=1927639</li> </ul> <p>You can workaround this by adding a custom policy: <pre><code>echo '(allow virt_qemu_ga_t container_var_lib_t (dir (search)))' &gt;local_virtqemu_ga.cil\nsemodule -i local_virtqemu_ga.cil\n</code></pre></p> <ul> <li>iptables is hitting selinux denials https://bugzilla.redhat.com/show_bug.cgi?id=2008097</li> </ul> <p>You can workaround this by adding a custom policy:</p> <pre><code>echo '(allow iptables_t cgroup_t (dir (ioctl)))' &gt;local_iptables.cil\nsemodule -i local_iptables.cil\n</code></pre> <ul> <li>rpcbind</li> </ul> <pre><code>echo '(allow rpcbind_t unreserved_port_t (udp_socket (name_bind)))' &gt;local_rpcbind.cil\nsemodule -i local_rpcbind.cil\n</code></pre> <ul> <li>master nodes are failing the first boot with access denied to <code>[::1]:53</code> https://github.com/okd-project/okd/issues/897</li> </ul> <p>While the master node is booting edit the grub config adding to kernel command line <code>console=null</code>.</p> <ul> <li>worker nodes may fail on openvswitch</li> </ul> <pre><code>echo '(allow openvswitch_t init_var_run_t (capability (fsetid)))' &gt;local_openvswitch.cil\nsemodule -i local_openvswitch.cil\n</code></pre>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#installing-hco-and-kubevirt","title":"Installing HCO and KubeVirt","text":"<p>Once the OKD console is up, connect to it. Go to Operators -&gt; OperatorHub, look for <code>KubeVirt HyperConverged Cluster Operator</code> and install it.</p> <p>Click on the Create Hyperconverged button, all the defaults should be fine.</p>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#providing-storage","title":"Providing storage","text":"<p>Shared storage is not mandatory for OKD Virtualization, but without a doubt it provides many advantages over a configuration based on local storage which is considered a suboptimal configuration.</p> <p>Between the advantages enabled by shared storage it is worth mentioning: - Live migration of Virtual Machines   - Founding pillar for HA   - Enables seamless cluster upgrades without the need to shut down and restart all the VMs on each upgrade - Centralized storage management enabling elastic scalability - Centralized backup</p>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#shared-storage","title":"Shared storage","text":"<p>TBD: rook.io deployment</p>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#local-storage","title":"Local storage","text":"<p>You can configure local storage for your virtual machines by using the OKD Virtualization hostpath provisioner feature.</p> <p>When you install OKD Virtualization, the hostpath provisioner Operator is automatically installed. To use it, you must: - Configure SELinux on your worker nodes via a Machine Config object. - Create a HostPathProvisioner custom resource. - Create a StorageClass object for the hostpath provisioner.</p>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#configuring-selinux-for-the-hostpath-provisioner-on-okd-worker-nodes","title":"Configuring SELinux for the hostpath provisioner on OKD worker nodes","text":"<p>You can configure SELinux for your OKD Worker nodes using a MachineConfig.</p>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#creating-a-custom-resource-cr-for-the-hostpathprovisioner-operator","title":"Creating a custom resource (CR) for the HostPathProvisioner operator","text":"<ol> <li> <p>Create the HostPathProvisioner custom resource file. For example:</p> <pre><code>$ touch hostpathprovisioner_cr.yaml\n</code></pre> </li> <li> <p>Edit that file. For example:</p> <pre><code>apiVersion: hostpathprovisioner.kubevirt.io/v1beta1\nkind: HostPathProvisioner\nmetadata:\n  name: hostpath-provisioner\nspec:\n  imagePullPolicy: IfNotPresent\n  pathConfig:\n    path: \"/var/hpvolumes\" # The path of the directory on the node\n    useNamingPrefix: false # Use the name of the PVC bound to the created PV as part of the directory name.\n</code></pre> </li> <li> <p>Create the CR in the <code>kubevirt-hyperconverged</code> namespace:</p> <pre><code>$ oc create -n kubevirt-hyperconverged -f hostpathprovisioner_cr.yaml\n</code></pre> </li> </ol>"},{"location":"blog/2020/08/31/okd_virtualization_on_user_provided_infrastructure/#creating-a-storageclass-for-the-hostpathprovisioner-operator","title":"Creating a StorageClass for the HostPathProvisioner operator","text":"<ol> <li> <p>Create the YAML file for the storage class. For example:</p> <pre><code>$ touch hppstorageclass.yaml\n</code></pre> </li> <li> <p>Edit that file. For example:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: hostpath-provisioner\nprovisioner: kubevirt.io/hostpath-provisioner\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> </li> <li> <p>Creating the Storage Class object:</p> <pre><code>$ oc create -f hppstorageclass.yaml\n</code></pre> </li> </ol>"},{"location":"okd_tech_docs/","title":"OKD Technical Documentation","text":"<p>Warning</p> <p>This section is under construction</p> <p>This section of the documentation is for developers that want to customize OKD.</p> <p>The section will cover:</p> <ul> <li>How is OKD delivered</li> <li>How to build an OKD operator</li> <li>How to deploy a self-build version of an OKD operator to an existing cluster</li> <li>How to create a customized OKD installer containing your self-built operator</li> </ul> <p>The above section will allow you to work on fixes and enhancements to core OKD operators and be able to run them locally.</p> <p>In addition to the above this section will also look at the Red Hat build and test setup, looking at how OpenShift and OKD operators are built and tested and how releases are created.</p>"},{"location":"okd_tech_docs/#okd-releases","title":"OKD Releases","text":"<p>OKD is a Kubernetes based platform that delivers a fully managed platform from the core operating system to the Kubernetes platform and the services running on it.  All aspects of OKD are managed by a collection of operators.</p> <p>OKD shares most of the same source code as Red Hat OpenShift.  One of the primary differences is that OKD uses Fedora CoreOS where OpenShift uses Red Hat Enterprise Linux CoreOS as the base platform for cluster nodes.</p> <p>An OKD release is a strictly defined set of software.  A release is defined by a release payload, which contains an operator (Cluster Version Operator), a list of manifests to apply and a reference file.  You can get information about a release using the oc command line utility, <code>oc adm release info &lt;release name&gt;</code>.</p> <p>You can find the latest available release here.</p> <p>You can get the current version of your cluster using the <code>oc get clusterversion</code> command, or from the Cluster Settings page in the Administration section of the OKD web console.</p> <p>For the OKD 4.10 release named 4.10.0-0.okd-2022-03-07-131213 the command would be <code>oc adm release info 4.10.0-0.okd-2022-03-07-131213</code></p> <p>you can add additional command line options to get more specific information about a release:</p> <ul> <li><code>--commit-urls</code> shows the source code that makes up the release</li> <li><code>--commits</code> allows you to specify 2 releases and see the differences between the releases</li> <li><code>--pullspecs</code> show the exact container images that will be used by a release</li> </ul>"},{"location":"okd_tech_docs/modifying_okd/","title":"Making changes to OKD","text":"<p>Warning</p> <p>This section is under construction</p> <p>The source code for OKD is available on github.  OKD is made up of many components bundled into a release.  You can find the exact commit for each component included in a release using the <code>oc adm release info</code> command with the <code>--commit-urls</code> option, as outlined in the overview section.</p> <p>To make a change to OKD you need to:</p> <ol> <li>Identify the component(s) that needs to be changed</li> <li>Clone/fork the git repository (you can choose to fork the exact commit used to create the image referenced by the OKD release or a newer version of the source)</li> <li>Make the change</li> <li>Build the image and push to a container registry that the OKD cluster will be able to access</li> <li>Run the modified container on a cluster</li> </ol>"},{"location":"okd_tech_docs/modifying_okd/#building-images","title":"Building images","text":"<p>Most component repositories contain a Dockerfile, so building the image is as simple as <code>podman build</code> or <code>docker build</code> depending on your container tool of choice.</p> <p>Some component repositories contain a Makefile, so building the image can be done using the Makefile, typically with <code>make build</code></p> <p>First thing to do is to replace the <code>FROM</code> images in <code>Dockerfile.rhel7</code>.  You may want to just copy it to <code>Dockerfile</code> and then make the changes. </p> <p><pre><code>    FROM registry.ci.openshift.org/openshift/release:golang-1.17 AS builder\n</code></pre> and <pre><code>    FROM registry.ci.openshift.org/origin/4.10:base\n</code></pre></p> <p>Note</p> <p>The original and replacement image may change as golang version and release requirements change.</p> <p>Question</p> <p>Is there a way to find the correct base image for an OKD release?</p> <p>The original images are unavailable to the public. There is an effort to update the Dockerfiles with publicly available images.</p>"},{"location":"okd_tech_docs/modifying_okd/#example-scenario","title":"Example Scenario","text":"<ul> <li>Modify console-operator to have a link to the community site okd.io instead of docs.okd.io</li> <li>add to pre-existing cluster</li> <li>build a custom release to include the modified console-operator, then install a new cluster will custom release</li> </ul> <p>To complete the scenario the following steps need to be performed:</p> <ol> <li>Fork the console-operator repository</li> <li>Clone the new fork locally: <code>git clone https://github.com/&lt;username&gt;/console-operator.git</code></li> <li>create new branch from master (or main):  <code>git switch -c &lt;branch name&gt;</code></li> <li>Make needed modifications.  Commit/squash as needed.  Maintainers like to see 1 commit rather than several.</li> <li>Create the image: <code>podman build -f &lt;Dockerfile file&gt; -t &lt;target repo&gt;/&lt;username&gt;/console-operator:4.11-&lt;some additional identifier&gt;</code></li> <li>Push image to external repository: <code>podman push  &lt;target repo&gt;/&lt;username&gt;/console-operator:4.11-&lt;some additional identifier&gt;</code> </li> <li> <p>Create new release to test with.  This requires the <code>oc</code> command to be available.  I use the following script (make_payload.sh).  It can be modified as needed, such as adding the correct container registry and username:</p> <pre><code>server=https://api.ci.openshift.org\n\nfrom_release=registry.ci.openshift.org/origin/release:4.11.0-0.okd-2022-04-12-000907\nrelease_name=4.11.0-0.jef-2022-04-12-0\nto_image=quay.io/fortinj66/origin-release:v4.11-console-operator\n\noc adm release new --from-release ${from_release} \\\n                --name ${release_name} \\\n                --to-image ${to_image} \\\n                console-operator=&lt;target repo&gt;/&lt;username&gt;/console-operator:4.11-&lt;some additional identifier&gt;\n</code></pre> <p><code>from_release</code>, <code>release_name</code>, <code>to_image</code> will need to be updated as needed  </p> </li> <li> <p>Pull installer for cluster release: <code>oc adm release extract --tools &lt;to_image from above&gt;</code>  (Make sure image is publicly available)</p> </li> </ol> <p>Warning</p> <p>When working with some Go lang projects you may need to be on Go lang v1.17 or better, as some projects use language features not supported before v1.17, even though some of the project README.md files may specify V1.15, these README files are out of date</p> <p>If it is not clear how to build a component you can look in the release repository at <code>https://github.com/openshift/release/tree/master/ci-operator/config/openshift/&lt;operator repo name&gt;</code>, this is used by the Red Hat build system to build components so can be used to determine how to build a component.</p> <p>You should also check the repo README.md file or any documentation, typically in a doc folder, as there may be some repo specific details</p> <p>Question</p> <p>Are there any special repos unique to OKD that need specific mention here, such as machine config?</p>"},{"location":"okd_tech_docs/modifying_okd/#running-the-modified-image-on-a-cluster","title":"Running the modified image on a cluster","text":"<p>An OKD release contain a specific set of images and there are operators that ensure that only the correct set of images are running a cluster, so you need to do some specific actions to be able to run your modified image on a cluster.  You can do this by:</p> <ol> <li>configuring an existing cluster to run a modified image</li> <li>create a new installer containing your image then creating a new cluster with the modified installer</li> </ol>"},{"location":"okd_tech_docs/modifying_okd/#running-on-an-existing-cluster","title":"Running on an existing cluster","text":"<p>The Cluster Version Operator watches the deployments and images related to the core OKD services to ensure that only valid images are running in the core.  This prevents you from changing any of the core images.  If you want to replace an image you need to scale the Cluster Version Operator down to 0 replicas:</p> <pre><code>oc scale --replicas=0 deployment/cluster-version-operator -n openshift-cluster-version\n</code></pre> <p>Some images, such as the Cluster Cloud Controller Manager Operator and the Machine API Operator need additional steps to be able to make changes, but these typically have a docs folder containing additional information about how to make changes to these images.</p>"},{"location":"okd_tech_docs/modifying_okd/#create-custom-release","title":"Create custom release","text":""},{"location":"okd_tech_docs/operators/","title":"Operator Hub Catalogs","text":"<p>Warning</p> <p>This section is under construction</p> <p>OKD contains many operators which deliver the base platform, however there is also additional capabilities delivered as operators available via the Operator Hub.</p> <p>The operator hub story for OKD isn't ideal currently (as at OKD 4.10) as OKD shares source with OpenShift, the commercial sibling to OKD.  OpenShift has additional operator hub catalogs provided by Red Hat, which deliver additional capabilities as part of the supported OpenShift product. These additional capabilities are not currently provided to OKD.</p> <p>OpenShift and OKD share a community catalog of operators, which are a subset of the operators available in the OperatorHub.  The operators in the community catalog should run on OKD/OpenShift and will include any additional configuration, such as security context configuration.</p> <p>However, where an operator in the community catalog has a dependency that Red Hat supports and delivers as part of the additional OpenShift operator catalog, then the community catalog operator will specify the dependency from the supported OpenShift catalog.  This results in missing dependency errors when attempting to install on OKD.</p> <p>Question</p> <ul> <li>will the proposed OKD catalog solve all the dependency issues in the community catalog?</li> <li>what is the timeline for the OKD catalog?</li> </ul> <p>Todo</p> <p>Some useful repo links - do we need to create instructions for specific operators?</p> <ul> <li>Community operator source<ul> <li>docs</li> <li>repo</li> </ul> </li> <li>OKD operators : repo</li> <li>Marketplace operator : repo</li> <li>Devworkspace operator : repo</li> <li>GitOps operator : repo</li> <li>devspaces (crw) : public repo</li> </ul>"},{"location":"okd_tech_docs/release/","title":"OKD Development Resources","text":"<p>Warning</p> <p>This section is under construction</p> <p>Question</p> <p>What is the end-to-end process to build an OKD release? Is it possible outside Red Hat CI infrastructure?</p> <ul> <li>openshift-installer source</li> </ul>"},{"location":"okd_tech_docs/troubleshoot/","title":"Troubleshooting OKD","text":"<p>Warning</p> <p>This section is under construction</p> <p>Todo</p> <p>Complete this section from comments in discussion thread</p>"},{"location":"wg_crc/overview/","title":"CRC Build Subgroup","text":"<p>Code-Ready Containers is a cut down version of OKD, designed to run on a developer's machine, which would not have sufficient resources for a full installation of OKD.</p> <p>The working group was established after a live session where Red Hat's Charro Gruver walked through the build process for OKD CRC</p> <p>The build process is currently manual, so the working group was established to automate the process and investigate options for creating a continuous integration setup to build and test OKD CRC.</p>"},{"location":"wg_docs/content/","title":"Content guidelines","text":""},{"location":"wg_docs/content/#site-content-maintainability","title":"Site content maintainability","text":"<p>The site has adopted Markdown as the standard way to create content for the site.  Previously the site used an HTML based framework, which resulted in content not being frequently updated as there was a steep learning curve.</p> <p>All content on the site should be created using Markdown.  To ensure content is maintainable going forward only markdown features outlined below should be used to create site content.  If you wish to use additional components on a page then please contact the documentation working group to discuss your requirements before creating a pull request containing additional components.</p> <p>MkDocs includes the ability to create custom page templates.  This facility has been used to create a customized home page for the site.  If any other pages require a custom layout or custom features, then a page template should be used so the content can remain in Markdown.  Creation of custom page templates should be discussed with the documentation working group.</p>"},{"location":"wg_docs/content/#changing-content","title":"Changing content","text":"<p>MkDocs supports standard Markdown syntax and a set Markdown extensions provided by plugins.  The exact Markdown syntax supported is based on the python implementation.</p> <p>MkDocs is configured using the mkdocs.yml file in the root of the git repository.</p> <p>The mkdoc.yml file defines the top level navigation for the site.  The level of indentation is configurable (this requires the theme to support this feature) with Markdown headings, levels 2 (<code>##</code>) and 3 (<code>###</code>) being used for the in-page navigation on the right of the page.</p>"},{"location":"wg_docs/content/#standard-markdown-features","title":"Standard Markdown features","text":"<p>The following markdown syntax is used within the documentation</p> Syntax Result <code># Title</code> heading - you can create up to 6 levels of headings by adding additional <code>#</code> characters, so <code>###</code> is a level 3 heading <code>**text**</code> will display the word <code>text</code> in bold <code>*text*</code> will display the word <code>text</code> in italic <code>`code`</code> inline code block <code>```shell ... ```</code> multi-line (Fenced) code block <code>1. list item</code> ordered list <code>- unordered list item</code> unordered list <code>---</code> horizontal break <p>HTML can be embedded in Markdown, but embedded HTML should not be used in the documentation.  All content should use Markdown with the permitted extensions.</p>"},{"location":"wg_docs/content/#indentation","title":"Indentation","text":"<p>MkDocs uses 4 spaces for tabs, so when indenting code ensure you are working with tabs set to 4 spaces rather than 2, which is commonly used.</p> <p>When using some features of Markdown indentation is used to identify blocks.</p> <pre><code>1. Ubiquity EdgeRouter ER-X\n    - runs DHCP (embedded), custom DNS server via AdGuard\n\n    ![pic](./img/erx.jpg){width=80%}\n</code></pre> <p>In the code block above you will see the unordered list item is indented, so it aligns with the content of the ordered list (rather than aligning with the number of the ordered list).  The image is also indented so it too aligns with the ordered list text.</p> <p>Many of the Markdown elements can be nested and indentation is used to define the nesting relationship.  If you look down on this page at the Information boxes section, the example shows an example of nesting elements and the Markdown tab shows how indentation is being used to identify the nesting relationships.</p>"},{"location":"wg_docs/content/#links-within-mkdocs-generated-content","title":"Links within MkDocs generated content","text":"<p>MkDocs will warn of any internal broken links, so it is important that links within the documentation are recognized as internal links.</p> <ul> <li>a link starting with a protocol name, such as http or https, is an external link</li> <li>a link starting with <code>/</code> is an external link.  This is because MkDocs generated content can be embedded into another web application, so links can point outside of the MkDocs generated site but hosted on the same website</li> <li>a link starting with a file name (including the .md extension) or relative directory (../directory/filename.md) is an internal link and will be verified by MkDocs</li> </ul> <p>Information</p> <p>Internal links should be to the Markdown file (with .md extension).  When the site is generated the filename will be automatically converted to the correct URL</p> <p>As part of the build process a linkchecker application will check the generated html site for any broken links.  You can run this linkchecker locally using the instructions.  If any links in the documentation should be excluded from the link checker, such as links to localhost, then they should be added as a regex to the linkcheckerrc file, located in the root folder of the project - see linkchecker documentation for additional information</p>"},{"location":"wg_docs/content/#markdown-extensions-used-in-okdio","title":"Markdown Extensions used in OKD.io","text":"<p>There are a number of Markdown extensions being used to create the site.  See the mkdocs.yml file to see which extensions are configured.  The documentation for the extensions can be found here</p>"},{"location":"wg_docs/content/#link-configuration","title":"Link configuration","text":"<p>Links on the page or embedded images can be annotated to control the links and also the appearance of the links:</p>"},{"location":"wg_docs/content/#image","title":"Image","text":"<p>Images are embedded in a page using the standard Markdown syntax <code>![description](URL)</code>, but the image can be formatted with Attribute Lists.  This is most commonly used to scale an image or center an image, e.g.</p> <pre><code>![GitHub repo url](images/github-repo-url.png){style=\"width: 80%\" .center }\n</code></pre>"},{"location":"wg_docs/content/#external-links","title":"External Links","text":"<p>External links can also use attribute lists to control behaviors, such as open in new tab or add a css class attribute to the generated HTML, such as external in the example below:</p> <pre><code>[MkDocs](http://mkdocs.org){: target=\"_blank\" .external }\n</code></pre> <p>Info</p> <p>You can embed an image as the description of a link to create clickable images that launch to another site: <code>[![Image description](Image URL)](target URL \"hover text\"){: target=_blank}</code></p>"},{"location":"wg_docs/content/#youtube-videos","title":"YouTube videos","text":"<p>It is not possible to embed a YouTube video and have it play in place using pure markdown.  You can use HTML within the markdown file to embed a video:</p> <pre><code>&lt;iframe width=\"100%\" height=\"500\" src=\"https://www.youtube.com/watch?v=qh1zYW7BLxE&amp;t=431s\" title=\"Building an OKD 4 Home Lab with special guest Craig Robinson\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"wg_docs/content/#tabs","title":"Tabs","text":"<p>Content can be organized into a set of horizontal tabs.</p> <pre><code>=== \"Tab 1\"\n    Hello\n\n=== \"Tab 2\"\n    World\n</code></pre> <p>produces :</p> Tab 1Tab 2 <p>Hello</p> <p>World</p>"},{"location":"wg_docs/content/#information-boxes","title":"Information boxes","text":"<p>The Admonition extension allows you to add themed information boxes using the <code>!!!</code> and <code>???</code> syntax:</p> <pre><code>!!! note\n    This is a note\n</code></pre> <p>produces:</p> <p>Note</p> <p>This is a note</p> <p>and</p> <pre><code>??? note\n    This is a collapsible note\n\n    You can add a `+` character to force the box to be initially open `???+`\n</code></pre> <p>produces a collapsible box:</p> Note <p>This is a collapsible note</p> <p>You can add a <code>+</code> character to force the box to be initially open <code>???+</code></p> <p>You can override the title of the box by providing a title after the Admonition type.</p> <p>Example</p> <p>You can also nest different components as required</p> notecollapsible notecustom title noteMarkdown <p>Note</p> <p>This is a note</p> Note <p>This is a note</p> <p>Sample Title</p> <p>This is a note</p> <pre><code>!!!Example\n    You can also nest different components as required\n\n    === \"note\"\n        !!!note\n            This is a note\n\n    === \"collapsible note\"\n        ???+note\n            This is a note\n\n    === \"custom title note\"\n        !!!note \"Sample Title\"\n            This is a note\n</code></pre>"},{"location":"wg_docs/content/#supported-admonition-classes","title":"Supported Admonition Classes","text":"<p>The Admonitions supported by the Material theme are :</p> <p>Note</p> <p>This is a note</p> <p>Abstract</p> <p>This is an abstract</p> <p>Info</p> <p>This is an info</p> <p>Tip</p> <p>This is a tip</p> <p>Success</p> <p>This is a success</p> <p>Question</p> <p>This is a question</p> <p>Warning</p> <p>This is a warning</p> <p>Failure</p> <p>This is a failure</p> <p>Danger</p> <p>This is a danger</p> <p>Bug</p> <p>This is a bug</p> <p>Example</p> <p>This is an example</p> <p>Quote</p> <p>This is a quote</p>"},{"location":"wg_docs/content/#code-blocks","title":"Code blocks","text":"<p>Code blocks allow you to insert code or blocks of text in line or as a block.</p> <p>To use inline you simply enclose the text using a single back quote ` character. So a command can be included using `oc get pods` and will create <code>oc get pods</code></p> <p>When you want to include a block of code you use a fence, which is 3 back quote character at the start and end of the block.  After the opening quotes you should also specify the content type contained in the block.</p> <pre><code>```shell\noc get pods\n```\n</code></pre> <p>which will produce:</p> <pre><code>oc get pods\n</code></pre> <p>Notice that the block automatically gets the copy to clipboard link to allow easy copy and paste.</p> <p>Every code block needs to identify the content.  Where there is no content type, then text should be used to identify the content as plain text.  Some of the common content types are shown in the table below.  However, a full link of supported content types can be found here, where the short name in the documentation should be used.</p> type Content shell Shell script content powershell Windows Power Shell content bat Windows batch file (.bat or .cmd files) json JSON content yaml YAML content markdown or md Markdown content java Java programming language javascript or js JavaScript programming language typescript or ts TypeScript programming language text Plain text content"},{"location":"wg_docs/content/#advanced-highlighting-of-code-blocks","title":"Advanced highlighting of code blocks","text":"<p>There are some additional features available due to the highlight plugin installed in MkDocs.  Full details can be found in the MkDocs Materials documentation.</p>"},{"location":"wg_docs/content/#line-numbers","title":"Line numbers","text":"<p>You can add line numbers to a code block with the linenums directive.  You must specify the starting line number, 1 in the example below:</p> <pre><code>``` javascript linenums=\"1\"\n&lt;script&gt;\ndocument.getElementById(\"demo\").innerHTML = \"My First JavaScript\";\n&lt;/script&gt;\n```\n</code></pre> <p>creates</p> <pre><code>&lt;script&gt;\ndocument.getElementById(\"demo\").innerHTML = \"My First JavaScript\";\n&lt;/script&gt;\n</code></pre> <p>Info</p> <p>The line numbers do not get included when the copy to clipboard link is selected</p>"},{"location":"wg_docs/content/#spell-checking","title":"Spell checking","text":"<p>This project uses cSpell to check spelling within the markdown.  The configuration included in the project automatically excludes content in a code block, enclosed in triple back quotes ```.</p> <p>The configuration file also specifies that US English is the language used in the documentation, so only US English spellings should be used for words where alternate international English spellings exist.</p> <p>You can add words to be considered valid either within a markdown document or within the cspell configuration file, cspell.json, in the root folder of the documentation repository.</p> <p>Words defined within a page only apply to that page, but words added to the configuration file apply to the entire project.</p>"},{"location":"wg_docs/content/#adding-local-words","title":"Adding local words","text":"<p>You can add a list of words to be considered valid for spell checking purposes as a comment in a Markdown file.</p> <p>The comment has a specific format to be picked up by the cSpell tool:</p> <p><code>&lt;!--- cSpell:ignore linkchecker linkcheckerrc mkdocs mkdoc --&gt;</code></p> <p>here the words linkchecker, linkcheckerrc, mkdocs and mkdoc are specified as words to be accepted by the spell checker within the file containing the comment.</p>"},{"location":"wg_docs/content/#adding-global-words","title":"Adding global words","text":"<p>The cSpell configuration file cspell.json contains a list of words that should always be considered valid when spell checking.  The list of words applies to all files being checked.</p>"},{"location":"wg_docs/doc-env/","title":"Setup environment","text":""},{"location":"wg_docs/doc-env/#setting-up-a-documentation-environment","title":"Setting up a documentation environment","text":"<p>To work on documentation and be able to view the rendered web site you need to create an environment, which comprises of:</p> <ul> <li>MkDocs with the Materials theme</li> <li>CSpell</li> <li>linkchecker</li> <li>Node.js</li> <li>Python 3</li> </ul> <p>You can create the environment by :</p> <ul> <li>running the tooling within a container runtime, so you don't need to do any local installs</li> <li>if you have an Eclipse Che installation on an OKD cluster then you can make the changes using only a browser, with all the tooling running inside Che on the OKD cluster.</li> <li>installing the components on your local system</li> </ul> Tooling within a containerEditing on clusterLocal mkdocs and python tooling installation <p>You can use a container to run MkDocs so no local installation is required.  You need a container runtime on your system.</p> <p>The recommended options are:</p> <ul> <li>If running on Windows or MacOS you can run Docker Desktop</li> <li>if running on Windows pr MacOS you can run Podman or Podman Desktop</li> <li>if running on Linux you can run Podman</li> </ul> <p>On Windows and MacOS you need to ensure the Docker or Podman system is running</p> <p>Info</p> <p>On MacOS you need to ensure the directory you are working in is available within the podman virtual machine. You can specify additional directories to mount when issuing the <code>podman machine init</code> command.  An example command to initialize the podman machine on MacOS could be:</p> <pre><code>podman machine init --cpus 6 --disk-size 150 -m 8096 --now -v /Users:/Users -v /private:/private -v /var/folders:/var/folders\n</code></pre> <p>This specifies the CPU, disk and memory resource to give the podman machine and also the directories to mount into the virtual machine.  This ensures that all user home directories are available within the podman machine.</p> <p>If you have a node.js environment installed that includes the npm command then you can make use of the run scripts provided in the project to run the docker or podman commands</p> <p>The following commands all assume you are working in the root directory of your local git clone of your forked copy of the okd.io git repo.  (your working directory should contain mkdocs.yml and package.json files)</p> <p>Warning</p> <p>If you are using Linux with SELinux enabled, then you need to configure your system to allow the local directory containing the cloned git repo to be mounted inside a container.  The following commands will configure SELinux to allow this:</p> <p>(change the path to the location of your okd.io directory)</p> <pre><code>sudo semanage fcontext -a -t container_file_t '/home/brian/Documents/projects/okd.io(/.*)?'\nsudo restorecon -Rv /home/brian/Documents/projects/okd.io\n</code></pre> <p>There is a community operator available in the OperatorHub on OKD to install Eclipse Che, the upstream project for Red Hat CodeReady Workspaces.</p> <p>You can use Che to modify site content through your browser, with your OKD cluster hosting the workspace and developer environment.</p> <p>You need to have access to an OKD cluster and have the Che operator installed and an Che instance deployed and running.</p> <p>In your OKD console, you should have an applications link in the top toolbar.  Open the Applications menu (3x3 grid icon) and select Che.  This will open the Che application - Google Chrome is the supported browser and will give the best user experience.</p> <p>In the Che console side menu, select to Create Workspace, then in the Import from Git section add the URL of your fork of the okd.io git repository (should be similar to <code>https://github.com/&lt;user or org name&gt;/okd.io.git</code>) then press Create &amp; Open to start the workspace.</p> <p>After a short while the workspace will open (the cluster has to download and start a number of containers, so the first run may take a few minutes depending on your cluster network access).  When the workspace is displayed you may have to wait a few seconds for the workspace to initialize and clone your git repo into the workspace.  You may also get asked if you trust the author of the git repository, answer yes to this question.  Your environment should then be ready to start work.</p> <p>The web based developer environment uses the same code base as Microsoft Visual Studio Code, so provides a similar user experience, but within your browser.</p> <p>You can install MkDocs and associated plugins on your development system and run the tools locally:</p> <ul> <li>Install Python 3 on your system</li> <li>Install Node.js on your system</li> <li>Clone your fork of the okd.io repository</li> <li>cd into the local repo directory (./okd.io)</li> <li>Install the required python packages `pip install -r requirements.txt'</li> <li>Install the spell checker using command <code>npm ci</code>.  If you want to use the cspell command on the command line, then you need to install it globally <code>npm -g i cspell</code></li> </ul> <p>Note</p> <p>sudo command may be needed to install globally, depending on your system configuration</p> <p>You now have all the tools installed to be able to create the static HTML site from the markdown documents.  The documentation for MkDocs provides full instructions for using MkDocs, but the important commands are:</p> <ul> <li><code>mkdocs build</code> will build the static site.  This must be run in the root directory of the repo, where mkdocs.yml is located.  The static site will be created/updated in the public directory</li> <li><code>mkdocs serve</code> will build the static site and launch a test server on <code>http://localhost:8000</code>.  Every time a document is modified the website will automatically be updated and any browser open will be refreshed to the latest.</li> <li>To check links in the built site (<code>mkdocs build</code> must be run first), use the linkchecker, with command <code>linkchecker -f linkcheckerrc --check-extern public</code>.  This command should be run in the root folder of the project, containing the linkcheckerrc file.</li> <li>To check spelling <code>cspell docs/**/*.md</code> should be run in the root folder of the project, containing the cspell.json file.</li> </ul> <p>There is also a convenience script <code>./build.sh</code> in the root of the repository that will check spelling, build the site then run the link checker.</p> <p>You should verify there are no spelling mistakes, by finding the last line of the CSpell output: </p> <pre><code>CSpell: Files checked: 31, Issues found: 0 in 0 files\n</code></pre> <p>Similarly, the link checker creates a summary after checking the site:</p> <pre><code>That's it. 662 links in 695 URLs checked. 0 warnings found. 0 errors found\n</code></pre> <p>Any issues reported should be fixed before submitting a pull request to add your changes to the okd.io site.</p>"},{"location":"wg_docs/doc-env/#creating-the-container","title":"Creating the container","text":"<p>To create the container image on your local system choose the appropriate command from the list:</p> <ul> <li> <p>if you are using the docker command on Linux, Mac OS or Windows:</p> <pre><code>docker build -t mkdocs-build -f ./dev/Dockerfile .\n</code></pre> </li> <li> <p>if you are using the podman command on Linux:</p> <pre><code>podman build -t mkdocs-build -f ./dev/Dockerfile .\n</code></pre> </li> <li> <p>if you have npm and use Docker on Linux, Mac OS or Windows:</p> <pre><code>npm run docker-build-image\n</code></pre> </li> <li> <p>if you have npm and use Podman on Linux:</p> <pre><code>npm run podman-build-image\n</code></pre> </li> </ul> <p>This will build a local container image named mkdocs-build</p>"},{"location":"wg_docs/doc-env/#live-editing-of-the-content","title":"Live editing of the content","text":"<p>To change the content of the web site you can use your preferred editing application.  To see the changes you can run a live local copy of okd.io that will automatically update as you save local changes.</p> <p>Ensure you have the local container image, built in the previous step, available on your system then choose the appropriate command from the list:</p> <ul> <li> <p>if you are using the docker command on Linux or Mac OS:</p> <pre><code>docker run -it --rm --name mkdocs-serve -p 8000:8000 -v `pwd`:/site mkdocs-build\n</code></pre> </li> <li> <p>if you are using the podman command on Linux:</p> <pre><code>podman run -it --rm --name mkdocs-serve -p 8000:8000 -v `pwd`:/site mkdocs-build\n</code></pre> </li> <li> <p>if you are on Windows using the docker command in Powershell:</p> <pre><code>docker run -it --rm --name mkdocs-build -p 8000:8000 -v \"$(pwd):/site\" mkdocs-build\n</code></pre> </li> <li> <p>if you are on Windows using the docker command in CMD prompt:</p> <pre><code>docker run -it --rm --name mkdocs-build -p 8000:8000 -v %cd%:/site mkdocs-build\n</code></pre> </li> <li> <p>if you have npm and use Docker on Linux or Mac OS:</p> <pre><code>npm run docker-serve\n</code></pre> </li> <li> <p>if you have npm on Windows:</p> <pre><code>npm run win-docker-serve\n</code></pre> </li> <li> <p>if you have npm and use Podman on Linux:</p> <pre><code>npm run podman-serve\n</code></pre> </li> </ul> <p>You can now open a browser to localhost:8000.  You should see the okd.io web site in the browser.  As you change files on your local system the web pages will automatically update.</p> <p>When you have completed editing the site use Ctrl-c (hold down the control key then press c) to quit the site.</p>"},{"location":"wg_docs/doc-env/#build-and-validate-the-site","title":"Build and validate the site","text":"<p>Before you submit any changes to the site in a pull request please check there are no spelling mistakes or broken links, by running the build script and checking the output.</p> <p>The build script will create or update the static web site in the public directory - this is what will be created and published as the live site if you submit a pull request with your modifications.</p> <ul> <li> <p>if you are using the docker command on Linux or Mac OS:</p> <pre><code>docker run -it --rm --name mkdocs-build -p -v `pwd`:/site --entrypoint /site/build.sh mkdocs-build\n</code></pre> </li> <li> <p>if you are using the podman command on Linux:</p> <pre><code>podman run -it --rm --name mkdocs-build -p -v `pwd`:/site --entrypoint /site/build.sh mkdocs-build\n</code></pre> </li> <li> <p>if you are on Windows using the docker command in Powershell:</p> <pre><code>docker run -it --rm --name mkdocs-build -v \"$(pwd):/site\" --entrypoint /site/build.sh mkdocs-build\n</code></pre> </li> <li> <p>if you are on Windows using the docker command in CMD prompt:</p> <pre><code>docker run -it --rm --name mkdocs-build -v %cd%:/site --entrypoint /site/build.sh mkdocs-build\n</code></pre> </li> <li> <p>if you have npm and use Docker on Linux or Mac OS:</p> <pre><code>npm run docker-build\n</code></pre> </li> <li> <p>if you have npm on Windows:</p> <pre><code>npm run win-docker-build\n</code></pre> </li> <li> <p>if you have npm and use Podman on Linux:</p> <pre><code>npm run podman-build\n</code></pre> </li> </ul> <p>You should verify there are no spelling mistakes, by finding the last line of the CSpell output: </p> <pre><code>CSpell: Files checked: 31, Issues found: 0 in 0 files\n</code></pre> <p>Further down in the console output wil be the summary of the link checker:</p> <pre><code>That's it. 662 links in 695 URLs checked. 0 warnings found. 0 errors found\n</code></pre> <p>Any issues reported should be fixed before submitting a pull request to add your changes to the okd.io site.</p>"},{"location":"wg_docs/doc-env/#live-editing-of-the-content_1","title":"Live editing of the content","text":"<p>To change the content of the web site you can use the in browser editor provided by Che.  To see the changes you can run a live local copy of okd.io that will automatically update as you save local changes.</p> <p>On the right side of the workspace window you should see 3 icons, hovering over them should reveal they are the Outline, Endpoints and Workspace.  Clicking into the workspace, you should see a User Runtimes section with the option to open a new terminal, then 2 commands (Live edit and Build) and finally a link to launch MkDocs web site (initially this link will not work)</p> <p>To allow you to see your changes in a live site (where any change you save will automatically be updated on the site) click on the 1. Live edit link.  This will launch a new terminal window where the mkdocs serve command will run, which provides a local live site.  However, as you are running the development site on a cluster, the Che runtime automatically makes this site available to you.  The MkDocs link now points to the site, but you will be asked if you want to open the site in a new tab or in Preview.</p> <p>Preview will add a 4th icon to the side toolbar and open the web site in the side panel. You can drag the side of the window to resize the browser view to allow you to edit on the left and view the results on the right of your browser window.</p> <p>If you have multiple monitors you may want to select to open the website in a new Tab or use the MkDocs link, then drag the browser tab on to a different monitor.</p> <p>By default, the Che environment auto-saves any file modification after half a second of no activity.  You can alter this in the preferences section.  When ever a file is saved the live site will update in the browser. </p> <p>When you finished editing simply close the terminal window running the Live edit script.  This will stop the web server running the preview site.</p>"},{"location":"wg_docs/doc-env/#build-and-validate-the-site_1","title":"Build and validate the site","text":"<p>The build script will create or update the static web site in the public directory - this is what will be created and published as the live site if you submit a pull request with your modifications.</p> <p>To run the build script simply click the 2. Build link in the Workspace panel.</p> <p>You should verify there are no spelling mistakes, by finding the last line of the CSpell output: </p> <pre><code>CSpell: Files checked: 31, Issues found: 0 in 0 files\n</code></pre> <p>Further down in the console output wil be the summary of the link checker:</p> <pre><code>That's it. 662 links in 695 URLs checked. 0 warnings found. 0 errors found\n</code></pre> <p>Any issues reported should be fixed before submitting a pull request to add your changes to the okd.io site.</p>"},{"location":"wg_docs/okd-io/","title":"Contributing to okd.io","text":"<p>The source for okd.io is in a github repository.</p> <p>The site is created using MkDocs. which takes Markdown documents and turns them into a static website that can be accessed from a filesystem or served from a web server.</p> <p>To update or add new content to the site you need to</p> <ul> <li>fork the repository in your own GitHub account<ul> <li>it important to leave the repo name as okd.io in your GitHub account if you want to use Che/CodeReady Containers to modify the content</li> </ul> </li> <li>make the changes in your local repository</li> <li>create a pull request to deliver the updates to the primary repository.</li> </ul> <p>The site is created using MkDocs with the Materials theme theme.</p>"},{"location":"wg_docs/okd-io/#updating-the-site","title":"Updating the site","text":"<p>To make changes to the site.  Create a pull request to deliver the changes in your fork of the repo to the main branch of the okd.io repo.  Before creating a pull request you should run the build script and verify there are no spelling mistakes or broken links.  Details on how to do this can be found at the end of the instructions for setting up a documentation environment</p> <p>Github automation is used to generate the site then publish to GitHub pages, which serves the site.  If your changes contain spelling issues or broken links, then the automation will fail and the GitHub pages site will not be updated, so please do a local test using the build.sh script before creating the pull request.</p>"},{"location":"wg_docs/overview/","title":"Documentation Subgroup","text":"<p>The Documentation working group is responsible for improving the OKD documentation.  Both the community documentation (this site) and the product documentation.</p>"},{"location":"wg_docs/overview/#joining-the-group","title":"Joining the group","text":"<p>The Documentation Subgroup is open to all.  You don't need to be invited to join, just attend on of the bi-weekly video calls:</p> <ul> <li>Calendar link : OKD working group calendar</li> <li>Agenda link : Documentation working group agenda and meeting nodes</li> </ul>"},{"location":"wg_docs/overview/#product-documentation","title":"Product Documentation","text":"<p>The OKD product documentation is maintained in the same git repository as Red Hat OpenShift product documentation, as they are sibling projects and largely share the same source code.</p> <p>The process for making changes to the documentation is outlined in the documentation section</p>"},{"location":"wg_docs/overview/#community-documentation","title":"Community Documentation","text":"<p>This site is the community documentation.  It is hosted on github and uses a static site generator to convert the Markdown documents in the git repo into this website.</p> <p>Details of how to modify the site content is contained on the page Modifying OKD.io.</p>"},{"location":"wg_virt/community/","title":"Get involved!","text":"<p>The OKD Virtualization SIG is a group of people just like you who are aiming to promote the adoption of the virtualization components on OKD.</p>"},{"location":"wg_virt/community/#social-media","title":"Social Media","text":"<p>Reddit :   r/OKD Virtualization</p> <p>YouTube :   OKD Workgroup meeting</p> <p>Twitter :   Follow @OKD_Virt_SIG</p>"},{"location":"wg_virt/community/#getting-started-as-a-user-future-contributor","title":"Getting started as a user (future contributor!)","text":"<p>Before getting started, please read OKD community etiquette guidelines.</p> <p>Feel free to dive into OKD documentation following the installation guide for setting up your initial OKD deployment on your bare metal datacenter. Once it's up, please follow the OKD documentation regarding Virtualization installation.</p> <p>If you find difficulties during the process let us know! Please report issues in our GitHub tracker.</p> <p>TODO: we may switch to okd organization once it will be ready</p>"},{"location":"wg_virt/community/#getting-started-as-contributor","title":"Getting started as contributor","text":"<p>The OKD Virtualization SIG is a group of multidisciplinary individuals who are contributing code, writing documentation, reporting bugs, contributing UX and design expertise, and engaging with the community.</p> <p>Before getting started, we recommend that you:</p> <ul> <li>Join the OKD Workgroup Google Group and send a message presenting yourself and saying how you would like to contribute.</li> <li>Join the biweekly OKD Workgroup meetings (agenda, previous meetings recordings)</li> <li>Please read OKD community etiquette guidelines. (Quick summary: Be nice!)</li> </ul> <p>The OKD Virtualization SIG is a community project, and we welcome contributions from everyone! If you'd like to write code, report bugs, contribute designs, or enhance the documentation, we would love your help!</p>"},{"location":"wg_virt/community/#testing","title":"Testing","text":"<p>We're always eager to have new contributors to join improving the OKD Virtualization quality, no matter your experience level. Please try to deploy and use OKD Virtualization and report issues in our GitHub tracker.</p> <p>TODO: we may switch to okd organization once it will be ready</p>"},{"location":"wg_virt/community/#documentation","title":"Documentation","text":"<p>OKD Virtualization documentation is mostly included in GitHub openshift-docs repository and we are working for getting it published on OKD documentation website</p> <p>Some additional documentation may be available within this SubGroup space.</p>"},{"location":"wg_virt/community/#supporters-sponsors-and-providers","title":"Supporters, Sponsors, and Providers","text":"<p>OKD Virtualization SIG is still in its early days.</p> <p>If you are using, supporting or providing services with OKD Virtualization we would like to share your story here!</p>"},{"location":"wg_virt/overview/","title":"OKD Virtualization Subgroup","text":"<p>The Goal of the OKD Virtualization Subgroup is to provide an integrated solution for classical virtualization users based on OKD, HCO and KubeVirt, including a graphical user interface and deployed using bare metal suited method.</p> <p>Meet our community!</p>"},{"location":"wg_virt/overview/#projects","title":"Projects","text":"<p>The OKD Virtualization Subgroup is monitoring and integrating the following projects in a user consumable virtualization solution:</p> <ul> <li>OKD - as the platform</li> <li>KubeVirt - as the virtualization plugin</li> <li>HyperConverged Cluster Operator (HCO) - for supporting tools</li> <li>Rook? (Or whatever the upstream operator is called) for feature rich data storage</li> <li>Medik8s and NHC for high-availability</li> <li>Konveyor for migration from other platforms</li> <li>Faros deploy on small footprint, bare-metal clusters</li> </ul>"},{"location":"wg_virt/overview/#deployment","title":"Deployment","text":"<ul> <li>UPI first</li> <li>Assisted Installer? for easy provisioning of OKD on bare-metal nodes</li> </ul>"},{"location":"wg_virt/overview/#mailing-list-slack","title":"Mailing List &amp; Slack","text":"<p>OKD Workgroup Google Group: https://groups.google.com/forum/#!forum/okd-wg</p> <p>Slack Channel: #openshift-users</p>"},{"location":"wg_virt/overview/#todo","title":"TODO","text":"<ul> <li> <p>some social activity like a blog post and more upstream documentation</p> </li> <li> <p>improve reliability of testing and CI on OKD</p> </li> </ul>"},{"location":"wg_virt/overview/#sig-membership","title":"SIG Membership","text":"<ul> <li>Fabian Deutsch (Red Hat)</li> <li>Sandro Bonazzola (Red Hat)</li> <li>Simone Tiraboschi (Red Hat)</li> <li>Michal Skrivanek (Red Hat)</li> </ul>"},{"location":"wg_virt/overview/#resources-for-the-sig","title":"Resources for the SIG","text":""},{"location":"wg_virt/overview/#automation-in-place","title":"Automation in place:","text":"<p>HCO main branch gets tested against OKD 4.9: https://github.com/openshift/release/blob/master/ci-operator/config/kubevirt/hyperconverged-cluster-operator/kubevirt-hyperconverged-cluster-operator-main__okd.yaml</p> <p>HCO precondition job: https://prow.ci.openshift.org/job-history/gs/origin-ci-test/pr-logs/directory/pull-ci-kubevirt-hyperconverged-cluster-operator-main-okd-hco-e2e-image-index-gcp</p> <p>KubeVirt is uploaded to operatorhub and on community-operators: https://github.com/redhat-openshift-ecosystem/community-operators-prod/tree/main/operators/community-kubevirt-hyperconverged</p>"},{"location":"working-group/minutes/minutes/","title":"OKD Working Group Meeting Minutes","text":"<ul> <li>04-12-2022</li> <li>05-24-2022</li> </ul>"},{"location":"working-group/minutes/2022/WG-Meeting-Minutes-04-12-2022/","title":"OKD Working Group Meeting Notes","text":""},{"location":"working-group/minutes/2022/WG-Meeting-Minutes-04-12-2022/#april-12-2022","title":"April 12, 2022","text":""},{"location":"working-group/minutes/2022/WG-Meeting-Minutes-04-12-2022/#attendees","title":"Attendees:","text":"<ul> <li>Jaime Magiera  - University of Michigan</li> <li>Brian Innes - IBM</li> <li>Larry Brigman - CommScope</li> <li>Michael elmiko McCune - Red Hat</li> <li>Timoth\u00e9e Ravier - Red Hat</li> <li>Mohammad Reza Ostadi</li> <li>Daniel Axelrod - Datto</li> <li>Sri Ramanujam</li> <li>Bruce Link - BCIT</li> <li>Neal Gompa - Datto</li> <li>Christian Glombek - Red Hat</li> </ul>"},{"location":"working-group/minutes/2022/WG-Meeting-Minutes-04-12-2022/#agenda","title":"Agenda","text":"<ul> <li>Agenda Review</li> <li>OKD Release updates (Christian)<ul> <li>https://github.com/systemd/systemd/pull/22868 (rejected, new try WIP)</li> <li>https://github.com/openshift/installer/pull/5788 (Installer fix for bootstrap api-int issue)</li> </ul> </li> <li>FCOS updates (Timoth\u00e9e)<ul> <li>Fedora 36 test day/week:<ul> <li>https://github.com/coreos/fedora-coreos-tracker/issues/1147</li> <li>https://github.com/coreos/fedora-coreos-tracker/issues/1123</li> <li>https://github.com/coreos/fedora-coreos-tracker/issues/1101</li> </ul> </li> <li>Removing libvarlink-utils from FCOS:<ul> <li>https://github.com/coreos/fedora-coreos-tracker/issues/1130</li> </ul> </li> <li>Update hardware version in VMware OVA after 6.5/6.7 EOL date:<ul> <li>https://github.com/coreos/fedora-coreos-tracker/issues/1141</li> <li>https://docs.fedoraproject.org/en-US/fedora-coreos/provisioning-vmware/#_modifying_ovf_metadata</li> </ul> </li> <li>VirtualBox images coming soon!<ul> <li>https://github.com/coreos/fedora-coreos-tracker/issues/1008</li> </ul> </li> </ul> </li> <li>Docs updates (Brian, Jaime)<ul> <li>Systems and Contributors guides<ul> <li>https://binnes.github.io/okd-io/okd_tech_docs/</li> </ul> </li> <li>Meeting minutes going into the website</li> <li>Website styling<ul> <li>http://luminouscoder.com/okd.io/public</li> </ul> </li> <li>Automated testing<ul> <li>Example: https://github.com/JaimeMagiera/oct</li> </ul> </li> <li>Troubleshooting<ul> <li>https://github.com/kxr/o-must-gather</li> <li>https://github.com/elmiko/okd-camgi</li> <li>https://github.com/elmiko/camgi.rs (rust rewrite)</li> <li>https://github.com/rvanderp3/release-devenv</li> </ul> </li> </ul> </li> <li>Issues<ul> <li>Ceph/Rook issues with FCOS 35 release in OKD 4.10 (https://github.com/openshift/okd/issues/1160)</li> <li>NetworkPolicy - deny-all policy does not correctly restrict traffic to pod when using nodeports </li> </ul> </li> <li>Discussions</li> <li>New Business<ul> <li>CRC</li> <li>Survey</li> <li>Vote for subcommittee co-chairs</li> </ul> </li> <li>Tasks<ul> <li>Jaime - get 4.10 and 4.11 update info (ongoing)</li> <li>Charro Gruver - gather OKD download stats from dl.fedoraproject.org or delegate</li> <li>Daniel - Write up guides format into README in guides directory and then file tickets against people who have agreed to update them</li> <li>Ceate troubleshooting dicsussion thread (Brian)</li> </ul> </li> </ul>"},{"location":"working-group/minutes/2022/WG-Meeting-Minutes-05-24-2022/","title":"OKD Working Group Meeting Notes","text":""},{"location":"working-group/minutes/2022/WG-Meeting-Minutes-05-24-2022/#may-24-2022","title":"May 24, 2022","text":""},{"location":"working-group/minutes/2022/WG-Meeting-Minutes-05-24-2022/#attendees","title":"Attendees:","text":"<ul> <li>Jaime Magiera (University of Michigan)</li> <li>Bruce Link (BCIT)</li> <li>Timoth\u00e9e Ravier (Red Hat)</li> <li>Neal Gompa (Datto)</li> <li>Diane Mueller (Red Hat)</li> <li>Michael elmiko McCune (Red Hat)</li> <li>Jack Henschel (CERN)</li> <li>Erik Berg (UiO)</li> </ul>"},{"location":"working-group/minutes/2022/WG-Meeting-Minutes-05-24-2022/#agenda","title":"Agenda","text":"<ul> <li>Agenda Review</li> <li>OKD Release updates (Christian)<ul> <li>Question: Operator count? There was a change, we don't know the details</li> </ul> </li> <li>FCOS updates (Timoth\u00e9e)<ul> <li>Ignition config accessible to unprivileged software on VMware</li> <li>Format change for Nutanix artifact</li> <li>coreos autoinstall creates huge number of xfs allocation groups<ul> <li>https://docs.okd.io/latest/installing/installing_bare_metal/installing-bare-metal.html#installation-user-infra-machines-advanced_disk_installing-bare-metal</li> </ul> </li> </ul> </li> <li>Docs updates (Brian, Jaime)</li> <li>Issues<ul> <li>Plan for repository transition is being formulated. Please provide feedback.</li> <li>Survey<ul> <li>Dhriti forwarded link to survey</li> <li>Need feedback</li> </ul> </li> <li>Rook/Ceph status<ul> <li>Fix applied upstream, waiting for it to make its way down to us</li> <li>John's Bugzilla https://bugzilla.redhat.com/show_bug.cgi?id=2063929 still open</li> </ul> </li> </ul> </li> <li>Discussions<ul> <li>Discussion 1231</li> </ul> </li> <li>Project Updates<ul> <li>CRC<ul> <li>New name</li> <li>see https://www.youtube.com/watch?v=M_kjqMD6JlU at 9:51 in they talk about rebranding code ready workspaces to open shift dev spaces, and at 12:47 they talk about rebranding CEC to Openshift local.  (This is dated April 8, 2022)</li> </ul> </li> <li>Operate First/Mass Open Cloud <ul> <li>Community managed CI/CD for OKD conversation(s) on the Boston University Cloud (MassOpenCloud)</li> </ul> </li> <li>CERN OKD on OpenStack <ul> <li>invitation to speak to working group on their DIY OKD build process (June 7) &amp; in Dublin at Gathering (June 23?)</li> <li>Jack Henschel (CERN)</li> </ul> </li> </ul> </li> <li>Tasks<ul> <li>Jaime - get 4.10 and 4.11 update info (ongoing)</li> <li>Charro Gruver - gather OKD download stats from dl.fedoraproject.org or delegate</li> <li>Daniel - Write up guides format into README in guides directory and then file tickets against people who have agreed to update them</li> <li>~~Create troubleshooting dicsussion thread (Brian)~~ - https://github.com/openshift/okd/discussions/1198 </li> <li>Find out about serial output for installs<ul> <li>manually editing <code>/var/lib/containers/storage/volumes/ironic/_data/html/*.conf</code> on the bootstrap node</li> <li>https://docs.fedoraproject.org/en-US/fedora-coreos/emergency-shell/</li> <li>https://docs.openshift.com/container-platform/4.10/installing/install_config/installing-customizing.html#installation-special-config-kargs_installing-customizing</li> <li>or directly with <code>kernelArguments</code> for first boot: https://github.com/coreos/ignition/blob/main/docs/configuration-v3_3.md</li> </ul> </li> <li>find out about bare metal IPI install provding RHCOS nodes</li> <li>Find out about DNS changes to support mx (Jaime and Diane)</li> </ul> </li> </ul>"},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/archive/2021/","title":"2021","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/page/2/","title":"Posts","text":""},{"location":"blog/page/3/","title":"Posts","text":""},{"location":"blog/archive/2020/page/2/","title":"2020","text":""}]}